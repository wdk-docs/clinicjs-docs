{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\/\\s\\-\\.]+"},"docs":[{"location":"reviews/","text":"\u8bc4\u8bba \u00b6 {{ blog_content reviews }}","title":"\u8bc4\u8bba"},{"location":"reviews/#_1","text":"{{ blog_content reviews }}","title":"\u8bc4\u8bba"},{"location":"tags/","text":"{{ tag_content }}","title":"Tags"},{"location":"about/","text":"\u5173\u4e8e \u00b6 \u6211\u4eec\u9879\u76ee\u7684\u9996\u8981\u76ee\u6807\u662f\u521b\u5efa\u5177\u6709\u60ca\u4eba\u7528\u6237\u4f53\u9a8c\u7684\u5f00\u53d1\u4eba\u5458\u5de5\u5177\u3002 \u8fd9\u610f\u5473\u7740\u8fd9\u4e9b\u5de5\u5177\u4e0d\u5e94\u8be5\u9700\u8981\u5927\u578b\u57fa\u7840\u8bbe\u65bd\u6765\u8fd0\u884c\uff0c\u5e76\u4e14\u5e94\u8be5\u53ef\u4ee5\u5728\u5f00\u53d1\u4eba\u5458\u7684\u7b14\u8bb0\u672c\u7535\u8111\u4e0a\u4f7f\u7528\u3002 \u5b83\u4eec\u5e94\u8be5\u751f\u6210\u53ef\u79fb\u690d\u7684\u8f93\u51fa\uff0c\u53ef\u4ee5\u5728\u5f00\u53d1\u4eba\u5458\u4e4b\u95f4\u5171\u4eab\uff0c\u5e76\u4e14\u5b83\u4eec\u5e94\u8be5\u5177\u6709\u4e00\u79cd\u6e10\u8fdb\u7684\u4fe1\u606f\u663e\u793a\u65b9\u6cd5\uff0c\u53ea\u6709\u5728\u9700\u8981\u65f6\u624d\u63d0\u4f9b\u8be6\u7ec6\u4fe1\u606f\u3002 \u53c2\u4e0e \u00b6 \u5982\u679c\u60a8\u60f3\u53c2\u4e0e\u9879\u76ee\u7684\u4efb\u4f55\u65b9\u9762\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u4ee3\u7801\uff0c\u8bf7\u9996\u5148\u9605\u8bfb\u6211\u4eec\u7684 \u884c\u4e3a\u51c6\u5219 \u548c \u8d21\u732e\u8005\u6307\u5357 \u3002 \u7136\u540e\u968f\u65f6\u8054\u7cfb\u6211\u4eec\u5728GitHub\u6216\u901a\u8fc7\u901a\u5e38\u7684\u793e\u4ea4\u6e20\u9053\u3002","title":"\u5173\u4e8e"},{"location":"about/#_1","text":"\u6211\u4eec\u9879\u76ee\u7684\u9996\u8981\u76ee\u6807\u662f\u521b\u5efa\u5177\u6709\u60ca\u4eba\u7528\u6237\u4f53\u9a8c\u7684\u5f00\u53d1\u4eba\u5458\u5de5\u5177\u3002 \u8fd9\u610f\u5473\u7740\u8fd9\u4e9b\u5de5\u5177\u4e0d\u5e94\u8be5\u9700\u8981\u5927\u578b\u57fa\u7840\u8bbe\u65bd\u6765\u8fd0\u884c\uff0c\u5e76\u4e14\u5e94\u8be5\u53ef\u4ee5\u5728\u5f00\u53d1\u4eba\u5458\u7684\u7b14\u8bb0\u672c\u7535\u8111\u4e0a\u4f7f\u7528\u3002 \u5b83\u4eec\u5e94\u8be5\u751f\u6210\u53ef\u79fb\u690d\u7684\u8f93\u51fa\uff0c\u53ef\u4ee5\u5728\u5f00\u53d1\u4eba\u5458\u4e4b\u95f4\u5171\u4eab\uff0c\u5e76\u4e14\u5b83\u4eec\u5e94\u8be5\u5177\u6709\u4e00\u79cd\u6e10\u8fdb\u7684\u4fe1\u606f\u663e\u793a\u65b9\u6cd5\uff0c\u53ea\u6709\u5728\u9700\u8981\u65f6\u624d\u63d0\u4f9b\u8be6\u7ec6\u4fe1\u606f\u3002","title":"\u5173\u4e8e"},{"location":"about/#_2","text":"\u5982\u679c\u60a8\u60f3\u53c2\u4e0e\u9879\u76ee\u7684\u4efb\u4f55\u65b9\u9762\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u4ee3\u7801\uff0c\u8bf7\u9996\u5148\u9605\u8bfb\u6211\u4eec\u7684 \u884c\u4e3a\u51c6\u5219 \u548c \u8d21\u732e\u8005\u6307\u5357 \u3002 \u7136\u540e\u968f\u65f6\u8054\u7cfb\u6211\u4eec\u5728GitHub\u6216\u901a\u8fc7\u901a\u5e38\u7684\u793e\u4ea4\u6e20\u9053\u3002","title":"\u53c2\u4e0e"},{"location":"blog/","text":"\u535a\u5ba2 \u00b6 {{ blog_content }}","title":"\u65e5\u5fd7"},{"location":"blog/#_1","text":"{{ blog_content }}","title":"\u535a\u5ba2"},{"location":"blog/all-the-latest-coverage-and-news-on-clinic-js/","text":"We have been very remiss in keeping the Clinic.js blog up to date. Rest assured that is going to change for 2020. If you are curious about how Clinic.js might be able to help you pinpoint performance bottlenecks, here\u2019s a glimpse at how it has helped others and what they have to say about what makes it unique. NearForm Research new team members \u00b6 The most important thing to tell you is that NearForm Research, a new group inside NearForm, is now responsible for Clinic.js. It's still the same Node.js experts but now we have even more team members. You can expect to see a lot of new functionality in Clinic.js over the next several months. Our newly expanded NearForm Research team finally all together in one place! \ud83d\ude80 #ideasflowing #opensource #nodejs @jasnell @addaleax @goto_bus_stop pic.twitter.com/Gub6LELzrF \u2014 NearForm Research (@NF__Research) January 29, 2020 Getting involved \u00b6 We love hearing from Clinic.js users and how it helped to solve performance issues for them. If you are interested in sharing your thoughts on the future of Clinic.js, please get in touch. Some great mentions of Clinic.js \u00b6 ADP Lifion \u00b6 Clinic.js is one of those tools that has become invaluable for many Node.js developers. We were particularly pleased to see this post from Ali Yousuf in ADP Lifion Engineering on the Promise.allpocalypse - The performance implications of misunderstanding Node.js promises. Opensource.com \u00b6 This was quickly followed by Hiren Dhadhuk over on opensource.com including Clinic.js in their 9 favorite open source tools for Node.js developers Sematext \u00b6 And earlier in 2019, Adnan Rahi\u0107 included Clinic.js in their Node.js Open Source Monitoring Tools list. And we loved the comment that \"It\u2019s surprisingly easy to use.\" TensorFlow.js in Clinic.js Doctor \u00b6 The TensorFlow.js team have very kindly highlighted its use in Clinic.js Doctor at multiple events including TensorFlow Dev and Node+JS Interactive. Clinic.js Helping Users \u00b6 Here's just a selection of the lovely things people have been saying on Twitter about how Clinic.js has helped them. Michael Geers \u00b6 Was able to cut the build time of our design system in half today \ud83d\ude80. First time I used clinic.js by @NearForm . Awesome tool to visualise what's going on inside your node process. Thanks @DennisReimann #uiengine #designsystem #profiling #debugging https://t.co/88xCDuec06 pic.twitter.com/vNJDlkdK8M \u2014 Michael Geers (@naltatis) August 8, 2019 Liran Tal \u00b6 /3 Through these questions we understood the left and right borders and usage scenarios. One of the preliminary activities to developing this was doing performance testing with @NearForm 's clinic to assess the JSON processing impact on the CPU \u2014 Liran Tal \u2728 Node.js CLI Magic \u2728 (@liran_tal) February 10, 2020 Antoine Gomez \u00b6 Thanks to @nodeclinic for helping me figure out a stupid pagination memory bug in my code Before / After : pic.twitter.com/5kz7ebxad7 \u2014 Antoine Gomez (@antoineg) September 12, 2019 Dipro Chatterjee \u00b6 I recently spoke about #clinicjs and is absolutely delighted to be a user. \u2014 Dipro Chatterjee (@chatterjeedipro) December 17, 2019 Flavio Aandres \u00b6 Estan usando Node.JS para aplicaciones realmente robustas, posiblemente con Leaks de memoria, Fallas en el EventLoop y procesos demasiado costosos? Clinic.JS permite saber visualmente cuales son las funciones o metodos que bloquean el eventLoop! \ud83d\udd25\ud83d\ude4c https://t.co/7CDbA5mMwV \u2014 Flavio Aandres (@FlavioAandres) November 19, 2019 Webinars \u00b6 Want to find out more? We're planning a series of webinars to demonstrate how the Clinic.js suite of tools can help to solve Node.js performance issues. Register your interest here and we'll be in touch with some suggested dates.","title":"Clinic.js\u7684\u6240\u6709\u6700\u65b0\u62a5\u9053\u548c\u65b0\u95fb"},{"location":"blog/all-the-latest-coverage-and-news-on-clinic-js/#nearform-research-new-team-members","text":"The most important thing to tell you is that NearForm Research, a new group inside NearForm, is now responsible for Clinic.js. It's still the same Node.js experts but now we have even more team members. You can expect to see a lot of new functionality in Clinic.js over the next several months. Our newly expanded NearForm Research team finally all together in one place! \ud83d\ude80 #ideasflowing #opensource #nodejs @jasnell @addaleax @goto_bus_stop pic.twitter.com/Gub6LELzrF \u2014 NearForm Research (@NF__Research) January 29, 2020","title":"NearForm Research new team members"},{"location":"blog/all-the-latest-coverage-and-news-on-clinic-js/#getting-involved","text":"We love hearing from Clinic.js users and how it helped to solve performance issues for them. If you are interested in sharing your thoughts on the future of Clinic.js, please get in touch.","title":"Getting involved"},{"location":"blog/all-the-latest-coverage-and-news-on-clinic-js/#some-great-mentions-of-clinicjs","text":"","title":"Some great mentions of Clinic.js"},{"location":"blog/all-the-latest-coverage-and-news-on-clinic-js/#adp-lifion","text":"Clinic.js is one of those tools that has become invaluable for many Node.js developers. We were particularly pleased to see this post from Ali Yousuf in ADP Lifion Engineering on the Promise.allpocalypse - The performance implications of misunderstanding Node.js promises.","title":"ADP Lifion"},{"location":"blog/all-the-latest-coverage-and-news-on-clinic-js/#opensourcecom","text":"This was quickly followed by Hiren Dhadhuk over on opensource.com including Clinic.js in their 9 favorite open source tools for Node.js developers","title":"Opensource.com"},{"location":"blog/all-the-latest-coverage-and-news-on-clinic-js/#sematext","text":"And earlier in 2019, Adnan Rahi\u0107 included Clinic.js in their Node.js Open Source Monitoring Tools list. And we loved the comment that \"It\u2019s surprisingly easy to use.\"","title":"Sematext"},{"location":"blog/all-the-latest-coverage-and-news-on-clinic-js/#tensorflowjs-in-clinicjs-doctor","text":"The TensorFlow.js team have very kindly highlighted its use in Clinic.js Doctor at multiple events including TensorFlow Dev and Node+JS Interactive.","title":"TensorFlow.js in Clinic.js Doctor"},{"location":"blog/all-the-latest-coverage-and-news-on-clinic-js/#clinicjs-helping-users","text":"Here's just a selection of the lovely things people have been saying on Twitter about how Clinic.js has helped them.","title":"Clinic.js Helping Users"},{"location":"blog/all-the-latest-coverage-and-news-on-clinic-js/#michael-geers","text":"Was able to cut the build time of our design system in half today \ud83d\ude80. First time I used clinic.js by @NearForm . Awesome tool to visualise what's going on inside your node process. Thanks @DennisReimann #uiengine #designsystem #profiling #debugging https://t.co/88xCDuec06 pic.twitter.com/vNJDlkdK8M \u2014 Michael Geers (@naltatis) August 8, 2019","title":"Michael Geers"},{"location":"blog/all-the-latest-coverage-and-news-on-clinic-js/#liran-tal","text":"/3 Through these questions we understood the left and right borders and usage scenarios. One of the preliminary activities to developing this was doing performance testing with @NearForm 's clinic to assess the JSON processing impact on the CPU \u2014 Liran Tal \u2728 Node.js CLI Magic \u2728 (@liran_tal) February 10, 2020","title":"Liran Tal"},{"location":"blog/all-the-latest-coverage-and-news-on-clinic-js/#antoine-gomez","text":"Thanks to @nodeclinic for helping me figure out a stupid pagination memory bug in my code Before / After : pic.twitter.com/5kz7ebxad7 \u2014 Antoine Gomez (@antoineg) September 12, 2019","title":"Antoine Gomez"},{"location":"blog/all-the-latest-coverage-and-news-on-clinic-js/#dipro-chatterjee","text":"I recently spoke about #clinicjs and is absolutely delighted to be a user. \u2014 Dipro Chatterjee (@chatterjeedipro) December 17, 2019","title":"Dipro Chatterjee"},{"location":"blog/all-the-latest-coverage-and-news-on-clinic-js/#flavio-aandres","text":"Estan usando Node.JS para aplicaciones realmente robustas, posiblemente con Leaks de memoria, Fallas en el EventLoop y procesos demasiado costosos? Clinic.JS permite saber visualmente cuales son las funciones o metodos que bloquean el eventLoop! \ud83d\udd25\ud83d\ude4c https://t.co/7CDbA5mMwV \u2014 Flavio Aandres (@FlavioAandres) November 19, 2019","title":"Flavio Aandres"},{"location":"blog/all-the-latest-coverage-and-news-on-clinic-js/#webinars","text":"Want to find out more? We're planning a series of webinars to demonstrate how the Clinic.js suite of tools can help to solve Node.js performance issues. Register your interest here and we'll be in touch with some suggested dates.","title":"Webinars"},{"location":"blog/clinic-doctor-just-got-more-advanced-with-tensorflow-js/","text":"figure { margin: 1.5em 0; } figure img { margin: 0 auto; display: block; } figure figcaption { font-size: 0.9em; margin-top: 1em; } At the start of 2018 we introduced Clinic.js , an Open Source toolbox for developers to profile and improve their applications. As part of this toolbox, we launched Doctor, a very user-friendly tool, that uses data analysis to recommend the next steps the user should take to diagnose and fix performance problems in Node.js. And just last month we shipped a new tool, called Clinic.js Bubbleprof , which specializes in profiling I/O issues. While we are developing new exciting tools, we are also making a significant effort in improving the existing tools we have. An issue we have noticed during our many workshops, where we teach people to use Clinic.js, is that the way Doctor analyzed the CPU usage wasn\u2019t optimal. Until recently Doctor was using simple statistics to analyze the CPU usage, but now, with the experience we have gathered over the last half year, we have been able to develop much a more advanced Machine Learning model. This model is developed with TensorFlow.js . Because TensorFlow.js uses JavaScript, Doctor is just as easy to install as always but is now much more advanced. The Details \u00b6 An application with an I/O issue typically has a very low CPU usage, because it is spending most of its time being idle. However, what we have noticed is that even applications with I/O issues generate garbage, that the garbage collector will clean up. When the garbage collector runs, it makes the CPU usage spike. If the garbage collector runs often enough, the CPU usage spikes so much that our simple statistical model can no longer see that the CPU usage is too low. CPU usage, with GC overlay: Shows the CPU usage with different types of garbage collection overlayed. Notice, that the alignment isn\u2019t perfect and other events are interfering. In a simple world, one could just have used information about when garbage collection is happening to filter out the CPU spikes. Unfortunately, the CPU usage data is sample based while the garbage collection data is event-based, which makes it difficult to align the two data sources. From our investigation, it is also clear that other things are going one. Some possible causes could be optimization or writing the logging information itself. All are things that happen in another thread, therefore they make the CPU usage spike to above 100%. Since it isn\u2019t possible to consistently detect when another thread from V8 within the application makes the CPU usage spike, the solution was to separate the spiky data with Machine Learning. For this, we use a Hidden Markov Model (HMM) with Gaussian Emissions. The central idea in HMM is that a measurable observation, the CPU usage, is influenced by a hidden unobservable state, is V8 is running an extra thread or not. The Baum\u2013Welch algorithm , allows one to statistically determine what the most likely connection is between the hidden state and the measurable observations. The Viterbi algorithm can then infer the hidden state from the measurable observations. A trainable and inferable implementation of HMM with Gaussian Emissions didn\u2019t exist in the npm register, therefore we ended up implementing it ourselves using TensorFlow.js . The implementation is called hidden-markov-model-tf and is available on npm. It's fully Open Source and the sourcecode is available here . CPU usage filtered: Shows the CPU usage from the user\u2019s application itself, being correctly separated from the CPU usage spikes caused by V8. Using our implementation of the Hidden Markov Model, it is possible to separate the CPU usage caused by V8 from the CPU usage directly related to the user's application. With this separation in place, it is now possible to detect an I/O issue by checking if the CPU usage of the user\u2019s application is too low. Final Result: The Doctor detects an I/O issues by seeing there is a low CPU usage, even if there are spikes caused by V8. Contact \u00b6 If you would like us to do a workshop, where we teach how to profile and improve the performance of your application. Or if you need Machine Learning applied to some of your data. Then, feel free to contact us at https://www.nearform.com/contact/ .","title":"`TensorFlow.js` \u8ba9 `Clinic.js` \u533b\u751f\u66f4\u8fdb\u4e86\u4e00\u6b65"},{"location":"blog/clinic-doctor-just-got-more-advanced-with-tensorflow-js/#the-details","text":"An application with an I/O issue typically has a very low CPU usage, because it is spending most of its time being idle. However, what we have noticed is that even applications with I/O issues generate garbage, that the garbage collector will clean up. When the garbage collector runs, it makes the CPU usage spike. If the garbage collector runs often enough, the CPU usage spikes so much that our simple statistical model can no longer see that the CPU usage is too low. CPU usage, with GC overlay: Shows the CPU usage with different types of garbage collection overlayed. Notice, that the alignment isn\u2019t perfect and other events are interfering. In a simple world, one could just have used information about when garbage collection is happening to filter out the CPU spikes. Unfortunately, the CPU usage data is sample based while the garbage collection data is event-based, which makes it difficult to align the two data sources. From our investigation, it is also clear that other things are going one. Some possible causes could be optimization or writing the logging information itself. All are things that happen in another thread, therefore they make the CPU usage spike to above 100%. Since it isn\u2019t possible to consistently detect when another thread from V8 within the application makes the CPU usage spike, the solution was to separate the spiky data with Machine Learning. For this, we use a Hidden Markov Model (HMM) with Gaussian Emissions. The central idea in HMM is that a measurable observation, the CPU usage, is influenced by a hidden unobservable state, is V8 is running an extra thread or not. The Baum\u2013Welch algorithm , allows one to statistically determine what the most likely connection is between the hidden state and the measurable observations. The Viterbi algorithm can then infer the hidden state from the measurable observations. A trainable and inferable implementation of HMM with Gaussian Emissions didn\u2019t exist in the npm register, therefore we ended up implementing it ourselves using TensorFlow.js . The implementation is called hidden-markov-model-tf and is available on npm. It's fully Open Source and the sourcecode is available here . CPU usage filtered: Shows the CPU usage from the user\u2019s application itself, being correctly separated from the CPU usage spikes caused by V8. Using our implementation of the Hidden Markov Model, it is possible to separate the CPU usage caused by V8 from the CPU usage directly related to the user's application. With this separation in place, it is now possible to detect an I/O issue by checking if the CPU usage of the user\u2019s application is too low. Final Result: The Doctor detects an I/O issues by seeing there is a low CPU usage, even if there are spikes caused by V8.","title":"The Details"},{"location":"blog/clinic-doctor-just-got-more-advanced-with-tensorflow-js/#contact","text":"If you would like us to do a workshop, where we teach how to profile and improve the performance of your application. Or if you need Machine Learning applied to some of your data. Then, feel free to contact us at https://www.nearform.com/contact/ .","title":"Contact"},{"location":"blog/introducing-bubbleprof/","text":"Since the beginning of Node, profiling asynchronous operations has been a big hurdle. After months of internal research we have finally found a solution which greatly simplifies the process of finding async bottlenecks in Node applications. Today, we are glad to announce the first public version of Clinic.js Bubbleprof. Bubbleprof allows developers to capture all async activity with minimum effort. It then uses a novel \"bubble\" approach to visualize this activity. Everything you need is in one HTML file. You can find it included in the Clinic.js bundle: 1 npm install -g clinic Try it out by cloning our basic Bubbleprof examples from Github: 1 git clone git://github.com/clinicjs/node-clinic-bubbleprof-examples Start simple and run the basic-latency example, which visualizes how a latency of 1s affects your server: 1 clinic bubbleprof --on-port \u2018autocannon localhost: $PORT \u2019 -- node node-clinic-bubbleprof-examples/basic-latency Notice the long lines of latency? That\u2019s a quick way of finding something to improve in the app. Traditional Async Profiling \u00b6 Node works best in heavily-I/O-related contexts, and often acts as a mediator between many data streams and interfaces. Due to JavaScript\u2019s evented nature, most I/O is performed asynchronously \u2013 especially when our Node process is a networked application. If we can measure asynchronous activity in a decoupled way, we can find out where an application is waiting. Finding out where an application is waiting means finding out where to optimise our asynchronous flow. The end result being that our application becomes faster, and our users happier. Tracking and profiling asynchronous is a tricky issue. Consider the following simple example: 1 2 3 4 5 6 7 8 setTimeout ( crash , 1000 ) // maybe crash after 1s setTimeout ( crash , 1000 ) // call it again function crash () { if ( Math . random () < 0.5 ) { throw new Error ( \u2018 an async error \u2019 ) } } Running the above program will (probably) crash our Node.js process with a stack like this: 1 2 3 4 5 6 7 8 9 set-timeout-crash.js:5 throw new Error('an async error') ^ Error: an async error at Timeout.crash [as _onTimeout] (set-timeout-crash.js:5:9) at ontimeout (timers.js:466:11) at tryOnTimeout (timers.js:304:5) at Timer.listOnTimeout (timers.js:267:5) Figuring out which one of the setTimeout s triggered the crash is a surprisingly hard problem. The stack trace only contains the last synchronous part of our application. If we want to not just figure out what crashed the program but also profile how many times the async function is called and from where we would run into similar problems. This is not a new problem in Node.js, but in fact one that\u2019s been around since the very beginning. Lots of solutions have been attempted for making the situation better in regards to capturing longer stack traces and measure runtime of functions. Most older solutions did things like rewrite all your code that does something async to capture a stack trace before and then combine them back to produce a \u201cfull\u201d trace. This requires plenty of \u201cmonkey patching\u201d and is known for being brittle and have a substantial negative performance effect - it would often de-optimise your code because of too much reflection. Enter node core instrumentation \u00b6 Luckily the Node.js core collaborators have been trying hard to make the diagnostic situation better for everyone by adding more instrumentation to the Node.js core codebase. In fact, all async code (excluding native modules) is actually triggered from Node.js core which means that the situation has to improve there for userland to get any benefit. Some of the new instrumentation that has landed recently is the new async_hooks core module. async_hooks is an interface to a new set of instrumentation in core that instruments any async operation. This means that we now have a way to hook into when something async happens and also a way to find out what the parent async operation was. This is a massive improvement in the profiling landscape as it means we can now get a ton of valuable timing data out of Node.js core without having to resort to a bunch of hacks. If we add some async_hooks magic to our previous example, then we can figure out which of the setTimeout s made our program crash: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 const hooks = require ( 'async_hooks' ) const stacks = [] hooks . createHook ({ // save the stack when we do something async init : ( id , type , trigger ) => ( stacks [ id ] = new Error ( 'error' ). stack ) }) . enable () setTimeout ( crash , 1000 ) // maybe crash after 1s setTimeout ( crash , 1000 ) // call it again function crash () { if ( Math . random () < 0.5 ) { // lookup the previous stack console . log ( stacks [ hooks . executionAsyncId ()]) throw new Error ( 'an async error' ) } } Running it will show something similar to this: 1 2 3 4 5 6 7 8 9 at AsyncHook.init \u2026 (set-timeout-crash-hook.js:11:1) ... Error: an async error at Timeout.crash [as _onTimeout] (set-timeout-crash-hook.js:17:11) at ontimeout (timers.js:466:11) at tryOnTimeout (timers.js:304:5) at Timer.listOnTimeout (timers.js:267:5) Which tells us that this time it was the timeout at line 11 (first one) that triggered the crash. In addition to letting us capture the async context async_hooks gives us valuable insights into latency stats, throughput, the type of resource, and more. Reducing complexity \u00b6 async_hooks and other new core instrumentation give us a new waterhose of profiling data in Node.js. This is essential for doing any kind of profiling but also poses an interesting challenge. How do we digest this data in a way that helps us diagnose bottlenecks and/or find errors in our application? This, of course, is a non-trivial problem and depends on the use-case we are trying to support. This is why we\u2019ve started building Bubbleprof, a new command in the Clinic.js toolchain. Bubbleprof uses a series of heuristics and a novel \u201cbubble\u201d based UI to help all kinds of users - from beginners to advanced - to determine where asynchronous time is spent in their application. Through a series of design workshops we\u2019ve come up with a couple of heuristics we think help group async data together in a way that makes it easy to get an overview over your application. There are 3 interesting groupings of code in your Node.js application: \"User code\", ie. code you are writing as part of your app. \"Module code\", code running in modules you have installed from npm \"Node core code\", code running in Node.js core. When code crosses the boundary between one of these groupings, we consider it important information that can help us group asynchronous operations together. Bubbleprof tries to collect and aggregate all async operations (using async_hooks ) and then group them into bubbles based on this heuristic. Visualizing async flow \u00b6 Now that we have a manageable number of groupings - instead of hundreds or thousands of distinct async operations - we can draw a diagram that acts like a map of an application's async flow, showing where time is spent. For example, in this profile based on ACMEair (a dummy flight data application designed for use in benchmarking), we immediately see a flow centered around the database module MongoDB: Live example Bubbles and links \u00b6 These \"bubbles\" represent the time spent in synchronous and asynchronous operations within this grouping. The largest bubble in the view represents calls from the mongodb-core , which calls other functions elsewhere in the mongodb code. It dwarves every other bubble, which compared to it are tiny dots, and much of the application branches off from it. Clearly, optimising how this application uses mongodb has a lot of potential to reduce overhead and trigger other async operations earlier. The dominating green colouring in the inner ring tells us that most of this time is spent on network-related activities, while the small sliver of purple indicates some time spent in scheduling (ticks, timeouts and promises). Rolling the mouse over the labels for these colours gives us information on what they mean and highlights the appropriate parts of the diagram. There is also a very long line to a tiny bubble labelled nextTick + http.server . A thin white stripe tells us that this is code from our own application. Connecting lines like this one represent asynchronous delays from operations that were called in one grouping (here, the mongodb module), but execute code in another (here, some of our application's own code which involves an http server and nextTick wrappers). This suggests another area for us to focus on: our own code, which might be a quick win. Live example Drilling down to code \u00b6 We can click into these links and bubbles to see what is going on inside them. Clicking on the line shows something very simple: startServer is responsible for almost all of the delay in this grouping. Clicking again brings up the stack trace; and also - because context here is important - the complete async trace of the chain of events from our application's initiation to this point. Live example Because these traces can be very long, we group adjacent frames from the same section of code, automatically expanding those from the user's own application. In this example, startServer at line 192 of our app.js called the express module, which invoked a number of operations in node core, with createServerHandle being the last one. This was called from the mongodb client. Looking at the area chart on the right, we see that this was pending for most of the runtime of our application. Is this a slow server we want to optimise, or a healthy service constantly awaiting requests? We can check our application code and find out. We can click the upwards-pointing arrow labelled \"Mongo...\" to navigate up into that big, mongodb-core bubble. The view inside it also very simple - dominated by Connection.connect , called from within node_modules/mongodb-core . Live example Navigating async complexity \u00b6 So what about the parts of our mongodb integration that are fast? If we click the small topmost grouping, it expands out into another more complex layout. These tiny, extremely short async operations would have been too small to be useful information in the previous view, so Bubbleprof has collapsed them together. Live example Exploring this view, we see activity mostly relating to database connection pools and sockets. Everything looks fine in terms of speed, and we can use these maps of how processes execute in series and parallel to better understand the module's flow, diagnose timing bugs, spot operations in series that could be more efficient in parrallel, and identify unnecessary steps that can be optimised away. This is just a simple example. You can use Bubbleprof with benchmarking tools like Autocannon or Wrk to generate robust performance profiles, and see the changes in your application's throughput and flow as you try things out. We have a simple step-by-step walkthrough guide to help you get started. Get Started \u00b6 We'd love for you to install and try out Bubbleprof to see how it can help get to the bottom of your Node.js performance issues. Head over to the walkthrough and you'll be up and running in minutes. We'd also love to get your feedback on Bubbleprof/Doctor/Flame and the outputs they generate. In this early phase of release, we're eager to have it used on as many real-world problems as possible. We welcome everyone who is interested in becoming part of our community. If you'd like to get involved in any aspect, not just code, first read our Code of Conduct and Contributor guide . Then feel free to reach out to us on GitHub or via the usual social channels.","title":"\u4ecb\u7ecdBubbleprof \u2014\u2014 \u4e00\u79cd\u65b0\u9896\u7684Node.js\u5f02\u6b65\u5206\u6790\u65b9\u6cd5"},{"location":"blog/introducing-bubbleprof/#traditional-async-profiling","text":"Node works best in heavily-I/O-related contexts, and often acts as a mediator between many data streams and interfaces. Due to JavaScript\u2019s evented nature, most I/O is performed asynchronously \u2013 especially when our Node process is a networked application. If we can measure asynchronous activity in a decoupled way, we can find out where an application is waiting. Finding out where an application is waiting means finding out where to optimise our asynchronous flow. The end result being that our application becomes faster, and our users happier. Tracking and profiling asynchronous is a tricky issue. Consider the following simple example: 1 2 3 4 5 6 7 8 setTimeout ( crash , 1000 ) // maybe crash after 1s setTimeout ( crash , 1000 ) // call it again function crash () { if ( Math . random () < 0.5 ) { throw new Error ( \u2018 an async error \u2019 ) } } Running the above program will (probably) crash our Node.js process with a stack like this: 1 2 3 4 5 6 7 8 9 set-timeout-crash.js:5 throw new Error('an async error') ^ Error: an async error at Timeout.crash [as _onTimeout] (set-timeout-crash.js:5:9) at ontimeout (timers.js:466:11) at tryOnTimeout (timers.js:304:5) at Timer.listOnTimeout (timers.js:267:5) Figuring out which one of the setTimeout s triggered the crash is a surprisingly hard problem. The stack trace only contains the last synchronous part of our application. If we want to not just figure out what crashed the program but also profile how many times the async function is called and from where we would run into similar problems. This is not a new problem in Node.js, but in fact one that\u2019s been around since the very beginning. Lots of solutions have been attempted for making the situation better in regards to capturing longer stack traces and measure runtime of functions. Most older solutions did things like rewrite all your code that does something async to capture a stack trace before and then combine them back to produce a \u201cfull\u201d trace. This requires plenty of \u201cmonkey patching\u201d and is known for being brittle and have a substantial negative performance effect - it would often de-optimise your code because of too much reflection.","title":"Traditional Async Profiling"},{"location":"blog/introducing-bubbleprof/#enter-node-core-instrumentation","text":"Luckily the Node.js core collaborators have been trying hard to make the diagnostic situation better for everyone by adding more instrumentation to the Node.js core codebase. In fact, all async code (excluding native modules) is actually triggered from Node.js core which means that the situation has to improve there for userland to get any benefit. Some of the new instrumentation that has landed recently is the new async_hooks core module. async_hooks is an interface to a new set of instrumentation in core that instruments any async operation. This means that we now have a way to hook into when something async happens and also a way to find out what the parent async operation was. This is a massive improvement in the profiling landscape as it means we can now get a ton of valuable timing data out of Node.js core without having to resort to a bunch of hacks. If we add some async_hooks magic to our previous example, then we can figure out which of the setTimeout s made our program crash: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 const hooks = require ( 'async_hooks' ) const stacks = [] hooks . createHook ({ // save the stack when we do something async init : ( id , type , trigger ) => ( stacks [ id ] = new Error ( 'error' ). stack ) }) . enable () setTimeout ( crash , 1000 ) // maybe crash after 1s setTimeout ( crash , 1000 ) // call it again function crash () { if ( Math . random () < 0.5 ) { // lookup the previous stack console . log ( stacks [ hooks . executionAsyncId ()]) throw new Error ( 'an async error' ) } } Running it will show something similar to this: 1 2 3 4 5 6 7 8 9 at AsyncHook.init \u2026 (set-timeout-crash-hook.js:11:1) ... Error: an async error at Timeout.crash [as _onTimeout] (set-timeout-crash-hook.js:17:11) at ontimeout (timers.js:466:11) at tryOnTimeout (timers.js:304:5) at Timer.listOnTimeout (timers.js:267:5) Which tells us that this time it was the timeout at line 11 (first one) that triggered the crash. In addition to letting us capture the async context async_hooks gives us valuable insights into latency stats, throughput, the type of resource, and more.","title":"Enter node core instrumentation"},{"location":"blog/introducing-bubbleprof/#reducing-complexity","text":"async_hooks and other new core instrumentation give us a new waterhose of profiling data in Node.js. This is essential for doing any kind of profiling but also poses an interesting challenge. How do we digest this data in a way that helps us diagnose bottlenecks and/or find errors in our application? This, of course, is a non-trivial problem and depends on the use-case we are trying to support. This is why we\u2019ve started building Bubbleprof, a new command in the Clinic.js toolchain. Bubbleprof uses a series of heuristics and a novel \u201cbubble\u201d based UI to help all kinds of users - from beginners to advanced - to determine where asynchronous time is spent in their application. Through a series of design workshops we\u2019ve come up with a couple of heuristics we think help group async data together in a way that makes it easy to get an overview over your application. There are 3 interesting groupings of code in your Node.js application: \"User code\", ie. code you are writing as part of your app. \"Module code\", code running in modules you have installed from npm \"Node core code\", code running in Node.js core. When code crosses the boundary between one of these groupings, we consider it important information that can help us group asynchronous operations together. Bubbleprof tries to collect and aggregate all async operations (using async_hooks ) and then group them into bubbles based on this heuristic.","title":"Reducing complexity"},{"location":"blog/introducing-bubbleprof/#visualizing-async-flow","text":"Now that we have a manageable number of groupings - instead of hundreds or thousands of distinct async operations - we can draw a diagram that acts like a map of an application's async flow, showing where time is spent. For example, in this profile based on ACMEair (a dummy flight data application designed for use in benchmarking), we immediately see a flow centered around the database module MongoDB: Live example","title":"Visualizing async flow"},{"location":"blog/introducing-bubbleprof/#bubbles-and-links","text":"These \"bubbles\" represent the time spent in synchronous and asynchronous operations within this grouping. The largest bubble in the view represents calls from the mongodb-core , which calls other functions elsewhere in the mongodb code. It dwarves every other bubble, which compared to it are tiny dots, and much of the application branches off from it. Clearly, optimising how this application uses mongodb has a lot of potential to reduce overhead and trigger other async operations earlier. The dominating green colouring in the inner ring tells us that most of this time is spent on network-related activities, while the small sliver of purple indicates some time spent in scheduling (ticks, timeouts and promises). Rolling the mouse over the labels for these colours gives us information on what they mean and highlights the appropriate parts of the diagram. There is also a very long line to a tiny bubble labelled nextTick + http.server . A thin white stripe tells us that this is code from our own application. Connecting lines like this one represent asynchronous delays from operations that were called in one grouping (here, the mongodb module), but execute code in another (here, some of our application's own code which involves an http server and nextTick wrappers). This suggests another area for us to focus on: our own code, which might be a quick win. Live example","title":"Bubbles and links"},{"location":"blog/introducing-bubbleprof/#drilling-down-to-code","text":"We can click into these links and bubbles to see what is going on inside them. Clicking on the line shows something very simple: startServer is responsible for almost all of the delay in this grouping. Clicking again brings up the stack trace; and also - because context here is important - the complete async trace of the chain of events from our application's initiation to this point. Live example Because these traces can be very long, we group adjacent frames from the same section of code, automatically expanding those from the user's own application. In this example, startServer at line 192 of our app.js called the express module, which invoked a number of operations in node core, with createServerHandle being the last one. This was called from the mongodb client. Looking at the area chart on the right, we see that this was pending for most of the runtime of our application. Is this a slow server we want to optimise, or a healthy service constantly awaiting requests? We can check our application code and find out. We can click the upwards-pointing arrow labelled \"Mongo...\" to navigate up into that big, mongodb-core bubble. The view inside it also very simple - dominated by Connection.connect , called from within node_modules/mongodb-core . Live example","title":"Drilling down to code"},{"location":"blog/introducing-bubbleprof/#navigating-async-complexity","text":"So what about the parts of our mongodb integration that are fast? If we click the small topmost grouping, it expands out into another more complex layout. These tiny, extremely short async operations would have been too small to be useful information in the previous view, so Bubbleprof has collapsed them together. Live example Exploring this view, we see activity mostly relating to database connection pools and sockets. Everything looks fine in terms of speed, and we can use these maps of how processes execute in series and parallel to better understand the module's flow, diagnose timing bugs, spot operations in series that could be more efficient in parrallel, and identify unnecessary steps that can be optimised away. This is just a simple example. You can use Bubbleprof with benchmarking tools like Autocannon or Wrk to generate robust performance profiles, and see the changes in your application's throughput and flow as you try things out. We have a simple step-by-step walkthrough guide to help you get started.","title":"Navigating async complexity"},{"location":"blog/introducing-bubbleprof/#get-started","text":"We'd love for you to install and try out Bubbleprof to see how it can help get to the bottom of your Node.js performance issues. Head over to the walkthrough and you'll be up and running in minutes. We'd also love to get your feedback on Bubbleprof/Doctor/Flame and the outputs they generate. In this early phase of release, we're eager to have it used on as many real-world problems as possible. We welcome everyone who is interested in becoming part of our community. If you'd like to get involved in any aspect, not just code, first read our Code of Conduct and Contributor guide . Then feel free to reach out to us on GitHub or via the usual social channels.","title":"Get Started"},{"location":"bubbleprof/","text":"Bubbleprof\u662f\u4e00\u79cd\u5168\u65b0\u7684\u65b9\u5f0f\u6765\u53ef\u89c6\u5316\u4f60\u7684Node.js\u8fdb\u7a0b\u7684\u64cd\u4f5c\u3002 \u5b83\u89c2\u5bdf\u5e94\u7528\u7a0b\u5e8f\u7684\u5f02\u6b65\u64cd\u4f5c\uff0c\u5bf9\u5b83\u4eec\u8fdb\u884c\u5206\u7ec4\uff0c\u6d4b\u91cf\u5b83\u4eec\u7684\u5ef6\u8fdf\uff0c\u5e76\u7ed8\u5236\u5e94\u7528\u7a0b\u5e8f\u5f02\u6b65\u6d41\u4e2d\u7684\u5ef6\u8fdf\u56fe \u9605\u8bfb \u4ecb\u7ecd\u6027\u535a\u5ba2\u6587\u7ae0 \u548c \u53d1\u5e03\u516c\u544a \u3002 \u5b83\u662f\u5982\u4f55\u5de5\u4f5c\u7684 \u00b6 \u6bcf\u4e2a\u6c14\u6ce1\u7684\u5927\u5c0f\u8868\u793a\u4e00\u7ec4\u64cd\u4f5c\u4e2d\u7684\u65f6\u95f4\u3002 \u5c06\u8fd9\u4e9b\u6d41\u653e\u5728\u60a8\u81ea\u5df1\u7684\u4ee3\u7801\u3001\u6a21\u5757\u6216\u8282\u70b9\u6838\u5fc3\u4e2d\u7684\u4f4d\u7f6e\u8fdb\u884c\u5206\u7ec4\u3002 \u5c0f\u7684\u76f8\u90bb\u7ec4\u4e5f\u5206\u7ec4\u4ee5\u51cf\u5c11\u6742\u4e71\u3002 \u8fde\u63a5\u6c14\u6ce1\u7684\u7bad\u5934\u7684\u957f\u5ea6\u663e\u793a\u4e86\u6d41\u4ece\u4e00\u4e2a\u7ec4\u79fb\u52a8\u5230\u53e6\u4e00\u4e2a\u7ec4\u65f6\u7684\u5ef6\u8fdf\u3002 \u5185\u90e8\u7684\u5f69\u8272\u7ebf\u8868\u793a\u9020\u6210\u6b64\u5ef6\u8fdf\u7684\u5f02\u6b65\u64cd\u4f5c\u7c7b\u578b\u7684\u6df7\u5408\u3002 \u70b9\u51fb\u67e5\u770b\u3002 \u6c14\u6ce1\u548c\u6570\u5b57\u6807\u7b7e\u4e4b\u95f4\u548c\u5468\u56f4\u7684\u7ebf\u957f\u5ea6\u53cd\u6620\u4e86\u4ee5\u6beb\u79d2(ms)\u4e3a\u5355\u4f4d\u7684\u805a\u5408\u5ef6\u8fdf\u3002 \u6f14\u793a \u00b6 \u4e92\u52a8\u7684\u4f8b\u5b50 \u8bbe\u7f6e \u00b6 1 2 npm install -g clinic clinic bubbleprof --help","title":"Bubbleprof"},{"location":"bubbleprof/#_1","text":"\u6bcf\u4e2a\u6c14\u6ce1\u7684\u5927\u5c0f\u8868\u793a\u4e00\u7ec4\u64cd\u4f5c\u4e2d\u7684\u65f6\u95f4\u3002 \u5c06\u8fd9\u4e9b\u6d41\u653e\u5728\u60a8\u81ea\u5df1\u7684\u4ee3\u7801\u3001\u6a21\u5757\u6216\u8282\u70b9\u6838\u5fc3\u4e2d\u7684\u4f4d\u7f6e\u8fdb\u884c\u5206\u7ec4\u3002 \u5c0f\u7684\u76f8\u90bb\u7ec4\u4e5f\u5206\u7ec4\u4ee5\u51cf\u5c11\u6742\u4e71\u3002 \u8fde\u63a5\u6c14\u6ce1\u7684\u7bad\u5934\u7684\u957f\u5ea6\u663e\u793a\u4e86\u6d41\u4ece\u4e00\u4e2a\u7ec4\u79fb\u52a8\u5230\u53e6\u4e00\u4e2a\u7ec4\u65f6\u7684\u5ef6\u8fdf\u3002 \u5185\u90e8\u7684\u5f69\u8272\u7ebf\u8868\u793a\u9020\u6210\u6b64\u5ef6\u8fdf\u7684\u5f02\u6b65\u64cd\u4f5c\u7c7b\u578b\u7684\u6df7\u5408\u3002 \u70b9\u51fb\u67e5\u770b\u3002 \u6c14\u6ce1\u548c\u6570\u5b57\u6807\u7b7e\u4e4b\u95f4\u548c\u5468\u56f4\u7684\u7ebf\u957f\u5ea6\u53cd\u6620\u4e86\u4ee5\u6beb\u79d2(ms)\u4e3a\u5355\u4f4d\u7684\u805a\u5408\u5ef6\u8fdf\u3002","title":"\u5b83\u662f\u5982\u4f55\u5de5\u4f5c\u7684"},{"location":"bubbleprof/#_2","text":"\u4e92\u52a8\u7684\u4f8b\u5b50","title":"\u6f14\u793a"},{"location":"bubbleprof/#_3","text":"1 2 npm install -g clinic clinic bubbleprof --help","title":"\u8bbe\u7f6e"},{"location":"doctor/","text":"\u533b\u751f\u53ef\u4ee5\u5e2e\u52a9\u8bca\u65ad\u5e94\u7528\u7a0b\u5e8f\u4e2d\u7684\u6027\u80fd\u95ee\u9898\uff0c\u5e76\u6307\u5bfc\u60a8\u4f7f\u7528\u66f4\u4e13\u4e1a\u7684\u5de5\u5177\u6765\u6df1\u5165\u4e86\u89e3\u60a8\u7684\u7279\u5b9a\u95ee\u9898\u3002 \u4f4eCPU\u4f7f\u7528\u7387\u3001\u963b\u585e\u5783\u573e\u6536\u96c6\u3001\u9891\u7e41\u7684\u4e8b\u4ef6\u5faa\u73af\u5ef6\u8fdf\u6216\u6d3b\u52a8\u53e5\u67c4\u6570\u91cf\u6df7\u4e71\u7b49\u75c7\u72b6\u53ef\u80fd\u8868\u660e\u5b58\u5728\u8bb8\u591a\u6f5c\u5728\u95ee\u9898\u3002 \u533b\u751f\u6839\u636e\u8fd9\u4e9b\u75c7\u72b6\u7ed9\u51fa\u5efa\u8bae\uff0c\u5e2e\u52a9\u7f29\u5c0f\u53ef\u80fd\u6027\u3002 I/O\u95ee\u9898\u3001\u672a\u4f18\u5316\u7684\u5783\u573e\u6536\u96c6\u548c\u963b\u585e\u7684\u4e8b\u4ef6\u5faa\u73af\u7b49\u4f8b\u5b50\u975e\u5e38\u5e38\u89c1\u3002 \u533b\u751f\u4f1a\u5e2e\u4f60\u89e3\u51b3\u6240\u6709\u8fd9\u4e9b\u95ee\u9898\u3002 \u4e00\u65e6\u8bca\u65ad\u51fa\u95ee\u9898\uff0c\u533b\u751f\u4f1a\u5e2e\u52a9\u60a8\u627e\u5230\u6b63\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002 \u5b83\u53ef\u80fd\u4f1a\u4e3a\u60a8\u6307\u51fa\u4e00\u4e2a\u7279\u5b9a\u7684\u5206\u6790\u5de5\u5177\u6216\u5efa\u8bae\u89e3\u51b3\u95ee\u9898\u7684\u901a\u7528\u65b9\u6cd5\u3002 \u5bf9\u4e8e\u90a3\u4e9b\u559c\u6b22\u66f4\u591a\u80cc\u666f\u7684\u4eba\u6765\u8bf4\uff0c\u533b\u751f\u8fd8\u63d0\u4f9b\u4e86\u5bf9\u8fd9\u4e2a\u95ee\u9898\u7684\u6df1\u5165\u89e3\u91ca\u3002 \u5728\u60a8\u9884\u611f\u5230\u95ee\u9898\u53ef\u80fd\u4e0e\u5efa\u8bae\u4e0d\u540c\u7684\u60c5\u51b5\u4e0b\uff0c\u533b\u751f\u63d0\u4f9b\u4e86\u5bf9\u6240\u6709\u5176\u4ed6\u95ee\u9898\u7684\u6587\u6863\u7684\u8f7b\u677e\u8bbf\u95ee\u3002 \u6f14\u793a \u00b6 \u4ea4\u4e92\u6837\u4f8b \u914d\u7f6e \u00b6 1 2 npm install -g clinic clinic doctor --help","title":"\u533b\u751f"},{"location":"doctor/#_1","text":"\u4ea4\u4e92\u6837\u4f8b","title":"\u6f14\u793a"},{"location":"doctor/#_2","text":"1 2 npm install -g clinic clinic doctor --help","title":"\u914d\u7f6e"},{"location":"documentation/","text":"Getting started \u00b6 Before you get started with Clinic.js, first let's make sure we install it on our machines and run a couple of tests just to make sure everything is working fine. Let's follow these steps to kick off: 1. Note: You must use a version of Node.js >= 8.11.1 1 npm install -g clinic 2. Confirm that it has installed ok with: 1 clinic --help 3. It should print something similar to the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 Clinic.js - v2.2.1 Getting started As a first step, run the clinic doctor: clinic doctor -- node server.js Then benchmark your server with wrk or autocannon: wrk http://localhost:3000 autocannon http://localhost:3000 Finally shut down your server (Ctrl+C). Once the server process has shutdown clinic doctor will analyse the collected data and detect what type of issue you are having. Based on the issue type, it will provide a recommendation for you. For example, to debug I/O issues, use Clinic.js Bubbleprof: clinic bubbleprof -- node server.js Then benchmark your server again, just like you did with clinic doctor. Report an issue If you encounter any issue, feel free to send us an issue report at: https://github.com/clinicjs/node-clinic/issues Utilities When using clinic a bunch you have fill up your directory with data folders and files. You can clean these easily using clinic clean. More information For information on the clinic sub-commands, use the --help option: clinic doctor --help clinic bubbleprof --help clinic clean --help clinic flame --help clinic heapprofiler --help Flags -h | --help Display Help -v | --version Display Version 4. We have a set of example apps on Github. Let's run through the first one using Clinic.js Doctor and autocannon: 1 2 3 4 git clone git@github.com:nearform/node-clinic-doctor-examples.git cd node-clinic-doctor-examples npm install clinic doctor --autocannon [ / ] -- node ./slow-io This will run autocannon against a simple app with an IO issue and once it's complete it will automatically launch the Doctor tool inside your browser. Up next \u00b6 Normally, when using Clinic.js, we begin by using Clinic.js Doctor to identify what performance problems exist in an application. Doctor will then give us recommendations on what tools and enquires to make next. Get started with Clinic.js Doctor . Alternatively you can dive into an in-depth overview of the CLI. CLI .","title":"\u5165\u95e8"},{"location":"documentation/#getting-started","text":"Before you get started with Clinic.js, first let's make sure we install it on our machines and run a couple of tests just to make sure everything is working fine. Let's follow these steps to kick off: 1. Note: You must use a version of Node.js >= 8.11.1 1 npm install -g clinic 2. Confirm that it has installed ok with: 1 clinic --help 3. It should print something similar to the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 Clinic.js - v2.2.1 Getting started As a first step, run the clinic doctor: clinic doctor -- node server.js Then benchmark your server with wrk or autocannon: wrk http://localhost:3000 autocannon http://localhost:3000 Finally shut down your server (Ctrl+C). Once the server process has shutdown clinic doctor will analyse the collected data and detect what type of issue you are having. Based on the issue type, it will provide a recommendation for you. For example, to debug I/O issues, use Clinic.js Bubbleprof: clinic bubbleprof -- node server.js Then benchmark your server again, just like you did with clinic doctor. Report an issue If you encounter any issue, feel free to send us an issue report at: https://github.com/clinicjs/node-clinic/issues Utilities When using clinic a bunch you have fill up your directory with data folders and files. You can clean these easily using clinic clean. More information For information on the clinic sub-commands, use the --help option: clinic doctor --help clinic bubbleprof --help clinic clean --help clinic flame --help clinic heapprofiler --help Flags -h | --help Display Help -v | --version Display Version 4. We have a set of example apps on Github. Let's run through the first one using Clinic.js Doctor and autocannon: 1 2 3 4 git clone git@github.com:nearform/node-clinic-doctor-examples.git cd node-clinic-doctor-examples npm install clinic doctor --autocannon [ / ] -- node ./slow-io This will run autocannon against a simple app with an IO issue and once it's complete it will automatically launch the Doctor tool inside your browser.","title":"Getting started"},{"location":"documentation/#up-next","text":"Normally, when using Clinic.js, we begin by using Clinic.js Doctor to identify what performance problems exist in an application. Doctor will then give us recommendations on what tools and enquires to make next. Get started with Clinic.js Doctor . Alternatively you can dive into an in-depth overview of the CLI. CLI .","title":"Up next"},{"location":"documentation/bubbleprof/","text":"Bubbleprof \u00b6 \u4f60\u542c\u8bf4\u8fc7\u8fd9\u4e2a\u53ebBubbleprof\u7684\u4ee4\u4eba\u5174\u594b\u7684\u65b0\u5de5\u5177\uff0c\u60f3\u8bd5\u4e00\u8bd5\u5417?\u4e0d\u77e5\u9053\u4ece\u54ea\u91cc\u5f00\u59cb?\u90a3\u4e48\u8fd9\u7bc7\u6587\u7ae0\u5c31\u662f\u7ed9\u4f60\u7684\u3002 \u914d\u7f6e \u51c6\u5907 \u7b2c\u4e00\u5206\u6790 Bubbles \u4fa7\u8fb9\u680f \u627e\u5230\u7b2c\u4e00\u4e2a\u74f6\u9888 \u6539\u5584\u6211\u4eec\u7684\u5ef6\u8fdf \u5e76\u884c\u67e5\u8be2 \u7f13\u5b58\u7ed3\u679c","title":"Bubbleprof"},{"location":"documentation/bubbleprof/#bubbleprof","text":"\u4f60\u542c\u8bf4\u8fc7\u8fd9\u4e2a\u53ebBubbleprof\u7684\u4ee4\u4eba\u5174\u594b\u7684\u65b0\u5de5\u5177\uff0c\u60f3\u8bd5\u4e00\u8bd5\u5417?\u4e0d\u77e5\u9053\u4ece\u54ea\u91cc\u5f00\u59cb?\u90a3\u4e48\u8fd9\u7bc7\u6587\u7ae0\u5c31\u662f\u7ed9\u4f60\u7684\u3002 \u914d\u7f6e \u51c6\u5907 \u7b2c\u4e00\u5206\u6790 Bubbles \u4fa7\u8fb9\u680f \u627e\u5230\u7b2c\u4e00\u4e2a\u74f6\u9888 \u6539\u5584\u6211\u4eec\u7684\u5ef6\u8fdf \u5e76\u884c\u67e5\u8be2 \u7f13\u5b58\u7ed3\u679c","title":"Bubbleprof"},{"location":"documentation/bubbleprof/01-setup/","text":"Setup \u00b6 Bubbleprof is part of the Clinic.js suit of tools. To install Bubbleprof, simply install Clinic.js like so: 1 npm install -g clinic After installing, we can check if Bubbleprof has been installed by running the clinic bubbleprof command with the --help flag. 1 clinic bubbleprof --help It should print something similar to the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 Clinic.js BubbleProf - v1.11.0 clinic bubbleprof helps you find asynchronous bottlenecks and debug event loop blocking. To run clinic bubbleprof clinic bubbleprof -- node server.js If profiling on a server, it can be useful to only do data collection: clinic bubbleprof --collect-only -- node server.js You can then transfer the data and visualize it locally: clinic bubbleprof --visualize-only PID.clinic-bubbleprof-sample Flags -h | --help Display Help -v | --version Display Version --collect-only Do not process data on terminiation --visualize-only datapath Build or rebuild visualization from data Up next \u00b6 Getting ready","title":"Setup"},{"location":"documentation/bubbleprof/01-setup/#setup","text":"Bubbleprof is part of the Clinic.js suit of tools. To install Bubbleprof, simply install Clinic.js like so: 1 npm install -g clinic After installing, we can check if Bubbleprof has been installed by running the clinic bubbleprof command with the --help flag. 1 clinic bubbleprof --help It should print something similar to the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 Clinic.js BubbleProf - v1.11.0 clinic bubbleprof helps you find asynchronous bottlenecks and debug event loop blocking. To run clinic bubbleprof clinic bubbleprof -- node server.js If profiling on a server, it can be useful to only do data collection: clinic bubbleprof --collect-only -- node server.js You can then transfer the data and visualize it locally: clinic bubbleprof --visualize-only PID.clinic-bubbleprof-sample Flags -h | --help Display Help -v | --version Display Version --collect-only Do not process data on terminiation --visualize-only datapath Build or rebuild visualization from data","title":"Setup"},{"location":"documentation/bubbleprof/01-setup/#up-next","text":"Getting ready","title":"Up next"},{"location":"documentation/bubbleprof/02-getting-ready/","text":"Getting ready \u00b6 Great, now we are ready to start profiling. To start profiling we find need a server of some sorts that does some async operations to get started. To keep things simple let's use our \"official\" Bubbleprof example that evolves around optimising a server that queries a mongodb containing npm metadata data. 1 git clone https://github.com/clinicjs/node-clinic-bubbleprof-demo.git If you read the example README, it will contain some instructions on how to get mongodb setup with npm data and more. Up next \u00b6 First analysis","title":"Getting ready"},{"location":"documentation/bubbleprof/02-getting-ready/#getting-ready","text":"Great, now we are ready to start profiling. To start profiling we find need a server of some sorts that does some async operations to get started. To keep things simple let's use our \"official\" Bubbleprof example that evolves around optimising a server that queries a mongodb containing npm metadata data. 1 git clone https://github.com/clinicjs/node-clinic-bubbleprof-demo.git If you read the example README, it will contain some instructions on how to get mongodb setup with npm data and more.","title":"Getting ready"},{"location":"documentation/bubbleprof/02-getting-ready/#up-next","text":"First analysis","title":"Up next"},{"location":"documentation/bubbleprof/03-first-analysis/","text":"First analysis \u00b6 Now we're ready to run an analysis. Let's try with the first server in the repo, 1-server-with-no-index.js . It contains a small server that queries mongodb for the 5 newest and 5 oldest node modules. You can run it by simply doing node 1-server-with-no-index.js and query it by going to localhost:3000 in your browser afterwards. If it returns a JSON response things are working! Let's try and profile the server with Bubbleprof to see if we can find any bottlenecks. To do that we need a tool that can send a ton of http requests against the server fast. If you don't have one, autocannon is easy to use. You can install it from npm. 1 npm install -g autocannon To run the analysis we want to run the server with Bubbleprof and when the server is ready - i.e. starts listening on a port - we want to send a ton of requests to it using autocannon. We can do all that in this one single command: 1 clinic bubbleprof --on-port 'autocannon -c 5 -a 500 localhost:$PORT' -- node 1 -server-with-no-index.js Before running it, let's explain what's happening in there. The part after -- is simply the command to start running your server. The --on-port flag is a script that is executed as soon as your server starts listening on a port. The $PORT variable in that script is set to the port your server started listening on. When the --on-port scripts ends, the Bubbleprof analysis will run on the data collected from the server and open the results in a html page. You may have also noticed -c 5 -a 500 flags. This tells autocannon to send a fixed amount of requests (5 connections making a total of 500 requests). By default autocannon tries to apply pressure for full 10 seconds, then suddenly stops. While very useful for testing load tolerance, this would make it difficult to observe performance improvements in single components, as most async operations would be active for 95% of the profiling time. Now try and run it. It'll take about 15 seconds to run. Afterwards a html page similar to this should open: First thought is probably something similar to \"That's a lot of bubbles!\". Up next \u00b6 Bubbles","title":"First analysis"},{"location":"documentation/bubbleprof/03-first-analysis/#first-analysis","text":"Now we're ready to run an analysis. Let's try with the first server in the repo, 1-server-with-no-index.js . It contains a small server that queries mongodb for the 5 newest and 5 oldest node modules. You can run it by simply doing node 1-server-with-no-index.js and query it by going to localhost:3000 in your browser afterwards. If it returns a JSON response things are working! Let's try and profile the server with Bubbleprof to see if we can find any bottlenecks. To do that we need a tool that can send a ton of http requests against the server fast. If you don't have one, autocannon is easy to use. You can install it from npm. 1 npm install -g autocannon To run the analysis we want to run the server with Bubbleprof and when the server is ready - i.e. starts listening on a port - we want to send a ton of requests to it using autocannon. We can do all that in this one single command: 1 clinic bubbleprof --on-port 'autocannon -c 5 -a 500 localhost:$PORT' -- node 1 -server-with-no-index.js Before running it, let's explain what's happening in there. The part after -- is simply the command to start running your server. The --on-port flag is a script that is executed as soon as your server starts listening on a port. The $PORT variable in that script is set to the port your server started listening on. When the --on-port scripts ends, the Bubbleprof analysis will run on the data collected from the server and open the results in a html page. You may have also noticed -c 5 -a 500 flags. This tells autocannon to send a fixed amount of requests (5 connections making a total of 500 requests). By default autocannon tries to apply pressure for full 10 seconds, then suddenly stops. While very useful for testing load tolerance, this would make it difficult to observe performance improvements in single components, as most async operations would be active for 95% of the profiling time. Now try and run it. It'll take about 15 seconds to run. Afterwards a html page similar to this should open: First thought is probably something similar to \"That's a lot of bubbles!\".","title":"First analysis"},{"location":"documentation/bubbleprof/03-first-analysis/#up-next","text":"Bubbles","title":"Up next"},{"location":"documentation/bubbleprof/04-bubbles/","text":"Bubbles \u00b6 Let's dive into what these mean and how to interpret them. A bubble represents a group of async operations. The more time spent within a group of async operations, the bigger the bubble is. The longer the lines between bubbles, the more latency exists between them. The way bubbles group together async time is based on a couple of heuristics. Profiled code is grouped into 3 areas - userland (the application being profiled), dependencies (3 rd party modules), and node core . Whenever an async operation crosses from one space into another it defines the boundary of a bubble. Clicking on a bubble steps inside it to show the bubbles it's composed of. For example, try clicking on the yellow bubble at the bottom called mongojs. The UI tells us it contains userland code as well as it has a lightblue color. In here we see that there are two smaller bubbles. If we click on the first one it'll show the stack trace for the operation that created this as it has no sub bubbles. This stack trace is actually a combined stack trace by multiple async operations. The userland code is highlighted to help you navigate to your code that triggered this bubble. In this case it tells us that it was from line 10 in 1-server-with-no-index.js . If we look into the source file we'll see that line 10 looks like this: 1 col . find (). sort ({ modified : - 1 }). limit ( 5 , function ( err , newest ) { Which makes sense, as this is a mongodb query. In fact if we look at the other yellow bubble on the page, we'll see that it contains a reference to the next query inside its callback. This link indicates that these bubbles are executed in series. Up next \u00b6 Sidebar","title":"Bubbles"},{"location":"documentation/bubbleprof/04-bubbles/#bubbles","text":"Let's dive into what these mean and how to interpret them. A bubble represents a group of async operations. The more time spent within a group of async operations, the bigger the bubble is. The longer the lines between bubbles, the more latency exists between them. The way bubbles group together async time is based on a couple of heuristics. Profiled code is grouped into 3 areas - userland (the application being profiled), dependencies (3 rd party modules), and node core . Whenever an async operation crosses from one space into another it defines the boundary of a bubble. Clicking on a bubble steps inside it to show the bubbles it's composed of. For example, try clicking on the yellow bubble at the bottom called mongojs. The UI tells us it contains userland code as well as it has a lightblue color. In here we see that there are two smaller bubbles. If we click on the first one it'll show the stack trace for the operation that created this as it has no sub bubbles. This stack trace is actually a combined stack trace by multiple async operations. The userland code is highlighted to help you navigate to your code that triggered this bubble. In this case it tells us that it was from line 10 in 1-server-with-no-index.js . If we look into the source file we'll see that line 10 looks like this: 1 col . find (). sort ({ modified : - 1 }). limit ( 5 , function ( err , newest ) { Which makes sense, as this is a mongodb query. In fact if we look at the other yellow bubble on the page, we'll see that it contains a reference to the next query inside its callback. This link indicates that these bubbles are executed in series.","title":"Bubbles"},{"location":"documentation/bubbleprof/04-bubbles/#up-next","text":"Sidebar","title":"Up next"},{"location":"documentation/bubbleprof/05-sidebar/","text":"The sidebar \u00b6 So the bubbles show an overview on where time is spent. To get an idea on how much time is spent in general you can inspect the sidebar. Try going back to the Main View. You can use backspace on your keyboard to step back, or use the on-screen controls. On the right side of screen you should see a sidebar similar to this one: In the top of the sidebar there are two important sections. A search bar that allows you to search the data to find a specific function call. A timeline that shows the async activity over time. Timeline is very useful at examining throughput and that could give us a clue where the first bottleneck could be found. Up next \u00b6 Finding the first bottleneck","title":"The sidebar"},{"location":"documentation/bubbleprof/05-sidebar/#the-sidebar","text":"So the bubbles show an overview on where time is spent. To get an idea on how much time is spent in general you can inspect the sidebar. Try going back to the Main View. You can use backspace on your keyboard to step back, or use the on-screen controls. On the right side of screen you should see a sidebar similar to this one: In the top of the sidebar there are two important sections. A search bar that allows you to search the data to find a specific function call. A timeline that shows the async activity over time. Timeline is very useful at examining throughput and that could give us a clue where the first bottleneck could be found.","title":"The sidebar"},{"location":"documentation/bubbleprof/05-sidebar/#up-next","text":"Finding the first bottleneck","title":"Up next"},{"location":"documentation/bubbleprof/06-finding-the-first-bottleneck/","text":"Finding the first bottleneck \u00b6 A few things we notice by looking at this first diagram are: A lot of time is spent inside the mongodb bubble on the left. Similar amount of time is spent querying in the query bubble at the bottom. The timeline is sparse indicating low throughput. This seems like a throughput problem, likely related to mongodb. If we investigate the database setup we notice that the server is using a collection that doesn't contain an index. This means the database has to iterate all the data every time to answer our query, which creates a lot of database latency. We can reduce this overhead by adding an index on the properties we use. Up next \u00b6 Improving our latency","title":"Finding the first bottleneck"},{"location":"documentation/bubbleprof/06-finding-the-first-bottleneck/#finding-the-first-bottleneck","text":"A few things we notice by looking at this first diagram are: A lot of time is spent inside the mongodb bubble on the left. Similar amount of time is spent querying in the query bubble at the bottom. The timeline is sparse indicating low throughput. This seems like a throughput problem, likely related to mongodb. If we investigate the database setup we notice that the server is using a collection that doesn't contain an index. This means the database has to iterate all the data every time to answer our query, which creates a lot of database latency. We can reduce this overhead by adding an index on the properties we use.","title":"Finding the first bottleneck"},{"location":"documentation/bubbleprof/06-finding-the-first-bottleneck/#up-next","text":"Improving our latency","title":"Up next"},{"location":"documentation/bubbleprof/07-improving-our-latency/","text":"Improving our latency \u00b6 In our example, we have already added an ascending index based on the modified (datetime) attribute in our data. This indexed collection is used by the second example - 2-server-with-index.js . Let's run our benchmark against this server and see if we can improve further. 1 clinic bubbleprof --on-port 'autocannon -c 5 -a 500 localhost:$PORT' -- node 2 -server-with-index.js Much better. We can immediately see that the time in mongo and fastify bubbles has dropped by about one third. Looking at the timeline we can also notice that the total time to serve those 500 requests went down from 15 seconds to 9 seconds. Also the timeline itself became denser, meaning we spend less time waiting between serving the requests. In other words - it took less time to do the same amount of work. How can we improve this even further? Let's explore the bubbles a bit. The mongodb bubble on the left is based on the mongodb npm module and there is probably not much we can do to improve this 3 rd party dependency. Same goes for the fastify bubble on the top. Let's dive in to our query bubble at the bottom: This bubble clearly shows that our queries are executed in series as one follows the other. However if we think about this a little bit, there is actually no need for that. Both of the queries are independent so we can easily execute them in parallel. Doing that would make this bubble much smaller and hopefully increase performance. Up next \u00b6 Parallel queries","title":"Improving our latency"},{"location":"documentation/bubbleprof/07-improving-our-latency/#improving-our-latency","text":"In our example, we have already added an ascending index based on the modified (datetime) attribute in our data. This indexed collection is used by the second example - 2-server-with-index.js . Let's run our benchmark against this server and see if we can improve further. 1 clinic bubbleprof --on-port 'autocannon -c 5 -a 500 localhost:$PORT' -- node 2 -server-with-index.js Much better. We can immediately see that the time in mongo and fastify bubbles has dropped by about one third. Looking at the timeline we can also notice that the total time to serve those 500 requests went down from 15 seconds to 9 seconds. Also the timeline itself became denser, meaning we spend less time waiting between serving the requests. In other words - it took less time to do the same amount of work. How can we improve this even further? Let's explore the bubbles a bit. The mongodb bubble on the left is based on the mongodb npm module and there is probably not much we can do to improve this 3 rd party dependency. Same goes for the fastify bubble on the top. Let's dive in to our query bubble at the bottom: This bubble clearly shows that our queries are executed in series as one follows the other. However if we think about this a little bit, there is actually no need for that. Both of the queries are independent so we can easily execute them in parallel. Doing that would make this bubble much smaller and hopefully increase performance.","title":"Improving our latency"},{"location":"documentation/bubbleprof/07-improving-our-latency/#up-next","text":"Parallel queries","title":"Up next"},{"location":"documentation/bubbleprof/08-parallel-queries/","text":"Parallel queries \u00b6 The third example, 3-server-with-index-in-parallel.js is similar to second example, but executes the queries in parallel. Let's profile it again. 1 clinic bubbleprof --on-port 'autocannon -c 5 -a 500 localhost:$PORT' -- node 3 -server-with-index-in-parallel.js Now instead of having a simple mongodb query bubble we have two tiny ones each flanking our fastify bubble on the top. This is much better as it means we are doing more async operations in parallel. Once again we can see a drop in the total times by further one third. This is reflected both in the bubbles and the timeline. As it stands, this bubble layout is close to optimal. We have almost no userland code taking up any time anymore, which means most time is spent in 3 rd party modules - which we assume to be pretty optimal for their usecase as well. The main way to improve this now, would be to get rid of the mongodb bubble or fastify bubble entirely. Getting rid of fastify would be hard as our application is a http server and fastify is already really good at doing http stuff. To get rid of the mongodb bubble we would have to do fewer things with the database. Up next \u00b6 Caching the results","title":"Parallel queries"},{"location":"documentation/bubbleprof/08-parallel-queries/#parallel-queries","text":"The third example, 3-server-with-index-in-parallel.js is similar to second example, but executes the queries in parallel. Let's profile it again. 1 clinic bubbleprof --on-port 'autocannon -c 5 -a 500 localhost:$PORT' -- node 3 -server-with-index-in-parallel.js Now instead of having a simple mongodb query bubble we have two tiny ones each flanking our fastify bubble on the top. This is much better as it means we are doing more async operations in parallel. Once again we can see a drop in the total times by further one third. This is reflected both in the bubbles and the timeline. As it stands, this bubble layout is close to optimal. We have almost no userland code taking up any time anymore, which means most time is spent in 3 rd party modules - which we assume to be pretty optimal for their usecase as well. The main way to improve this now, would be to get rid of the mongodb bubble or fastify bubble entirely. Getting rid of fastify would be hard as our application is a http server and fastify is already really good at doing http stuff. To get rid of the mongodb bubble we would have to do fewer things with the database.","title":"Parallel queries"},{"location":"documentation/bubbleprof/08-parallel-queries/#up-next","text":"Caching the results","title":"Up next"},{"location":"documentation/bubbleprof/09-caching-the-results/","text":"Caching the results \u00b6 A way to do that would be to add caching. Our result seldomly changes so caching it in an LRU cache would be a great way to reduce the amount of database queries done in total. This is implemented in 4-server-with-caching.js . Let's run it: 1 clinic bubbleprof --on-port 'autocannon -c 5 -a 500 localhost:$PORT' -- node 4 -server-with-caching.js Our mongodb bubbles have shrunk 7x. And about 20x in comparison with the first example. Also, following the yellow line in the timeline tells us that mongodb activity has been replaced by caching activity after initial query response arrived. Now our mongodb bubbles are tiny - and by far the most time is spent just serving http requests. Hope this helps you understand the Bubbleprof tool better and how to use it to find your bottlenecks.","title":"Caching the results"},{"location":"documentation/bubbleprof/09-caching-the-results/#caching-the-results","text":"A way to do that would be to add caching. Our result seldomly changes so caching it in an LRU cache would be a great way to reduce the amount of database queries done in total. This is implemented in 4-server-with-caching.js . Let's run it: 1 clinic bubbleprof --on-port 'autocannon -c 5 -a 500 localhost:$PORT' -- node 4 -server-with-caching.js Our mongodb bubbles have shrunk 7x. And about 20x in comparison with the first example. Also, following the yellow line in the timeline tells us that mongodb activity has been replaced by caching activity after initial query response arrived. Now our mongodb bubbles are tiny - and by far the most time is spent just serving http requests. Hope this helps you understand the Bubbleprof tool better and how to use it to find your bottlenecks.","title":"Caching the results"},{"location":"documentation/cli/","text":"CLI \u00b6 Clinic.js comes with a number of tools to help us identify performance issues in our Node.js applications. All these tools can be accessed using the top-level clinic command once Clinic.js is installed . The anatomy of a command \u00b6 A Clinic.js command in its simplest form that generates a Doctor sample is as follows: 1 clinic doctor -- node server.js The -- denotes the end of the command and flags , dividing the command into the following two related parts: 1. clinic doctor informs Clinic.js that we wish to use Clinic.js Doctor to get an overview of our application's current performance. 2. node server.js is the command we use in order to run our Node.js application from the command line as normal. If our application needs some environment variables , we can set them beforehand: 1 NODE_ENV=production clinic doctor -- node server.js From here, we can start making the most out of Clinic.js by simulating load . Viewing tool information \u00b6 To see all help information relating to a Clinic.js tool including the version and supported flags we can simply run: 1 clinic doctor --help Or, to quickly see the version of each Clinic.js tool: 1 clinic flame --version If we want to just see information about an installed version of Clinic.js: 1 clinic --help Or, to just see the installed version of Clinic.js: 1 clinic --version For a complete list of options, see the CLI reference . Up next \u00b6 Simulating load","title":"CLI"},{"location":"documentation/cli/#cli","text":"Clinic.js comes with a number of tools to help us identify performance issues in our Node.js applications. All these tools can be accessed using the top-level clinic command once Clinic.js is installed .","title":"CLI"},{"location":"documentation/cli/#the-anatomy-of-a-command","text":"A Clinic.js command in its simplest form that generates a Doctor sample is as follows: 1 clinic doctor -- node server.js The -- denotes the end of the command and flags , dividing the command into the following two related parts: 1. clinic doctor informs Clinic.js that we wish to use Clinic.js Doctor to get an overview of our application's current performance. 2. node server.js is the command we use in order to run our Node.js application from the command line as normal. If our application needs some environment variables , we can set them beforehand: 1 NODE_ENV=production clinic doctor -- node server.js From here, we can start making the most out of Clinic.js by simulating load .","title":"The anatomy of a command"},{"location":"documentation/cli/#viewing-tool-information","text":"To see all help information relating to a Clinic.js tool including the version and supported flags we can simply run: 1 clinic doctor --help Or, to quickly see the version of each Clinic.js tool: 1 clinic flame --version If we want to just see information about an installed version of Clinic.js: 1 clinic --help Or, to just see the installed version of Clinic.js: 1 clinic --version For a complete list of options, see the CLI reference .","title":"Viewing tool information"},{"location":"documentation/cli/#up-next","text":"Simulating load","title":"Up next"},{"location":"documentation/cli/01-simulating-load/","text":"Simulating load \u00b6 If our app handles requests, simply pointing Clinic.js at it won't be particularly insightful when testing its performance, since there will be no load to monitor unless we're manually using the running app. One way around this is to simulate load using a benchmarking tool like autocannon . Using autocannon \u00b6 We can simulate load on our server with autocannon very easily using the --autocannon flag: 1 clinic doctor --autocannon [ 'localhost:$PORT' ] -- node server.js Clinic.js automatically replaces the value of $PORT with the actual port our server is listening to inside server.js . To make things even simpler, if we've set up our server to use the PORT environment variable for the application port, we can produce the same results as the above command like this: 1 clinic doctor --autocannon [ / ] -- node server.js Here / is the same as 'localhost:$PORT' inside the subargs: [ ] . Be careful to ensure there is space after the opening and before the closing square bracket to ensure the subargs are parsed correctly. Common autocannon flags \u00b6 All available flags can be seen in the autocannon README , but some of the more common flags are documented below. Connections \u00b6 By default, autocannon will set the number of concurrent connections to 10. To change this to 100 for example, we can set the -c or --connections flag as a subarg like so: 1 clinic doctor --autocannon [ -c 100 / ] -- node server.js HTTP methods \u00b6 Let's say we have an API with a POST endpoint that we want to test and monitor, this is easily accomplished using the -m or --method flag as a subarg: 1 clinic doctor --autocannon [ -m POST /api/item ] -- node server.js That's probably not that helpful without sending some actual data in the body of the request though, so let's add some using the -b or --body flag as a subarg: 1 clinic doctor --autocannon [ -m POST /api/item -b '{\"my\": \"data\"}' ] -- node server.js This could get a bit hard to manage with JSON strings or any other data type as part of the command, especially if we want to test different endpoints with different data sets or varying lengths. Handily autocannon supports using a local file to provide data for the request body, so with some JSON files we can simplify this load test with the -i flag like this: 1 clinic doctor --autocannon [ -m POST /api/item -i my-data.json ] -- node server.js Neat! Using our own command \u00b6 For more control, we can point any custom command at our server by using the --on-port flag. With autocannon globally installed we can simulate load on our app like so: 1 clinic doctor --on-port 'autocannon localhost:$PORT' -- node server.js Just like with the --autocannon flag, Clinic.js replaces the value of $PORT with the actual port our server is listening to inside server.js . The advantage of the --on-port flag is that it gives us the flexibility to use any command we like, including other benchmarking tools such as wrk . For example, a similar command as above with wrk globally installed would be: 1 clinic doctor --on-port 'wrk http://localhost:$PORT' -- node server.js Clinic.js then simply monitors the performance of the application under simulated load and will generate a sample when the script running --on-port exits. Which script we use is entirely up to us, we might have some complicated test commands already in place as NPM scripts so we could easily call one of those instead: 1 clinic doctor --on-port 'npm run load-test' -- node server.js Up next \u00b6 Controlling the output","title":"Simulating load"},{"location":"documentation/cli/01-simulating-load/#simulating-load","text":"If our app handles requests, simply pointing Clinic.js at it won't be particularly insightful when testing its performance, since there will be no load to monitor unless we're manually using the running app. One way around this is to simulate load using a benchmarking tool like autocannon .","title":"Simulating load"},{"location":"documentation/cli/01-simulating-load/#using-autocannon","text":"We can simulate load on our server with autocannon very easily using the --autocannon flag: 1 clinic doctor --autocannon [ 'localhost:$PORT' ] -- node server.js Clinic.js automatically replaces the value of $PORT with the actual port our server is listening to inside server.js . To make things even simpler, if we've set up our server to use the PORT environment variable for the application port, we can produce the same results as the above command like this: 1 clinic doctor --autocannon [ / ] -- node server.js Here / is the same as 'localhost:$PORT' inside the subargs: [ ] . Be careful to ensure there is space after the opening and before the closing square bracket to ensure the subargs are parsed correctly.","title":"Using autocannon"},{"location":"documentation/cli/01-simulating-load/#common-autocannon-flags","text":"All available flags can be seen in the autocannon README , but some of the more common flags are documented below.","title":"Common autocannon flags"},{"location":"documentation/cli/01-simulating-load/#connections","text":"By default, autocannon will set the number of concurrent connections to 10. To change this to 100 for example, we can set the -c or --connections flag as a subarg like so: 1 clinic doctor --autocannon [ -c 100 / ] -- node server.js","title":"Connections"},{"location":"documentation/cli/01-simulating-load/#http-methods","text":"Let's say we have an API with a POST endpoint that we want to test and monitor, this is easily accomplished using the -m or --method flag as a subarg: 1 clinic doctor --autocannon [ -m POST /api/item ] -- node server.js That's probably not that helpful without sending some actual data in the body of the request though, so let's add some using the -b or --body flag as a subarg: 1 clinic doctor --autocannon [ -m POST /api/item -b '{\"my\": \"data\"}' ] -- node server.js This could get a bit hard to manage with JSON strings or any other data type as part of the command, especially if we want to test different endpoints with different data sets or varying lengths. Handily autocannon supports using a local file to provide data for the request body, so with some JSON files we can simplify this load test with the -i flag like this: 1 clinic doctor --autocannon [ -m POST /api/item -i my-data.json ] -- node server.js Neat!","title":"HTTP methods"},{"location":"documentation/cli/01-simulating-load/#using-our-own-command","text":"For more control, we can point any custom command at our server by using the --on-port flag. With autocannon globally installed we can simulate load on our app like so: 1 clinic doctor --on-port 'autocannon localhost:$PORT' -- node server.js Just like with the --autocannon flag, Clinic.js replaces the value of $PORT with the actual port our server is listening to inside server.js . The advantage of the --on-port flag is that it gives us the flexibility to use any command we like, including other benchmarking tools such as wrk . For example, a similar command as above with wrk globally installed would be: 1 clinic doctor --on-port 'wrk http://localhost:$PORT' -- node server.js Clinic.js then simply monitors the performance of the application under simulated load and will generate a sample when the script running --on-port exits. Which script we use is entirely up to us, we might have some complicated test commands already in place as NPM scripts so we could easily call one of those instead: 1 clinic doctor --on-port 'npm run load-test' -- node server.js","title":"Using our own command"},{"location":"documentation/cli/01-simulating-load/#up-next","text":"Controlling the output","title":"Up next"},{"location":"documentation/cli/02-controlling-the-output/","text":"Controlling the output \u00b6 By default, any of the Clinic.js tool commands will generate a compiled HTML file and directory of data, so the following command: 1 clinic doctor -- node server.js Would generate a file structure like this: 1 2 3 4 .clinic/1234.clinic-doctor/1234.clinic-doctor-processstat .clinic/1234.clinic-doctor/1234.clinic-doctor-systeminfo .clinic/1234.clinic-doctor/1234.clinic-doctor-traceevent .clinic/1234.clinic-doctor.html The exact output will vary between Doctor , Flame and Bubbleprof since each tool generates and requires slightly different data to create a sample. Collecting data only \u00b6 In order to prevent any tool from generating the HTML, we can use the --collect-only flag like so: 1 clinic doctor --collect-only -- node server.js This would generate a directory from which the HTML profile can be generated, with a file structure similar to this: 1 2 3 .clinic/1234.clinic-doctor/1234.clinic-doctor-processstat .clinic/1234.clinic-doctor/1234.clinic-doctor-systeminfo .clinic/1234.clinic-doctor/1234.clinic-doctor-traceevent Visualizing existing data \u00b6 To generate or re-generate the HTML profile from Clinic.js collected data, we can use the --visualize-only flag and pass it the local path to the existing data like so: 1 clinic doctor --visualize-only .clinic/1234.clinic-doctor This would generate the following HTML file in the same directory as its data directory 1234.clinic-doctor : 1 .clinic/1234.clinic-doctor.html Note that since we're generating the sample visualization from an existing sample, we don't need to call any server start script after a -- flag delimiter when passing a directory path to the --visualize-only flag. Changing the output destination \u00b6 By default all tool sample files will be generated within the directory the command is run from ( . ). To change this, we can use the --dest flag to point to a different destination like so: 1 clinic doctor --dest ../some-other-dir -- node server.js This is quite useful when generating lots of Clinic.js files to ensure they don't fall within a project directory and avoid any version control mistakes! Up next \u00b6 Managing samples","title":"Controlling the output"},{"location":"documentation/cli/02-controlling-the-output/#controlling-the-output","text":"By default, any of the Clinic.js tool commands will generate a compiled HTML file and directory of data, so the following command: 1 clinic doctor -- node server.js Would generate a file structure like this: 1 2 3 4 .clinic/1234.clinic-doctor/1234.clinic-doctor-processstat .clinic/1234.clinic-doctor/1234.clinic-doctor-systeminfo .clinic/1234.clinic-doctor/1234.clinic-doctor-traceevent .clinic/1234.clinic-doctor.html The exact output will vary between Doctor , Flame and Bubbleprof since each tool generates and requires slightly different data to create a sample.","title":"Controlling the output"},{"location":"documentation/cli/02-controlling-the-output/#collecting-data-only","text":"In order to prevent any tool from generating the HTML, we can use the --collect-only flag like so: 1 clinic doctor --collect-only -- node server.js This would generate a directory from which the HTML profile can be generated, with a file structure similar to this: 1 2 3 .clinic/1234.clinic-doctor/1234.clinic-doctor-processstat .clinic/1234.clinic-doctor/1234.clinic-doctor-systeminfo .clinic/1234.clinic-doctor/1234.clinic-doctor-traceevent","title":"Collecting data only"},{"location":"documentation/cli/02-controlling-the-output/#visualizing-existing-data","text":"To generate or re-generate the HTML profile from Clinic.js collected data, we can use the --visualize-only flag and pass it the local path to the existing data like so: 1 clinic doctor --visualize-only .clinic/1234.clinic-doctor This would generate the following HTML file in the same directory as its data directory 1234.clinic-doctor : 1 .clinic/1234.clinic-doctor.html Note that since we're generating the sample visualization from an existing sample, we don't need to call any server start script after a -- flag delimiter when passing a directory path to the --visualize-only flag.","title":"Visualizing existing data"},{"location":"documentation/cli/02-controlling-the-output/#changing-the-output-destination","text":"By default all tool sample files will be generated within the directory the command is run from ( . ). To change this, we can use the --dest flag to point to a different destination like so: 1 clinic doctor --dest ../some-other-dir -- node server.js This is quite useful when generating lots of Clinic.js files to ensure they don't fall within a project directory and avoid any version control mistakes!","title":"Changing the output destination"},{"location":"documentation/cli/02-controlling-the-output/#up-next","text":"Managing samples","title":"Up next"},{"location":"documentation/cli/03-managing-samples/","text":"Managing samples \u00b6 After we've used Clinic.js tools to gain insight into our application's performance, addressed some observed bottlenecks and re-tested, we might end up with a directory filled with Clinic.js-generated samples. To remove all of these files and directories we can run the following command from the directory these samples reside in: 1 clinic clean If we've placed our files somewhere else, we can simply add the --path flag to point to that directory instead: 1 clinic clean --path ../some-other-dir After running this command there should be no more samples left in the directory. This can be a handy step if we've finished analysing our app and need to push code changes, but don't want Clinic.js samples in our application's repo. Ignoring files in git \u00b6 With git , we can add this entry to our application's .gitignore file to ensure any samples are ignored in the future: 1 .clinic Alternatively, we can use a global gitignore, so the project does not have to change to accomodate Clinic.js. To configure a global ignore file for git, and add .clinic to it, open a command line and do: 1 2 git config --global core.excludesfile ~/.gitignore_global echo '.clinic' >> ~/.gitignore_global Up next \u00b6 Get up to speed with the ins and outs of Clinic.js with the CLI reference .","title":"Managing samples"},{"location":"documentation/cli/03-managing-samples/#managing-samples","text":"After we've used Clinic.js tools to gain insight into our application's performance, addressed some observed bottlenecks and re-tested, we might end up with a directory filled with Clinic.js-generated samples. To remove all of these files and directories we can run the following command from the directory these samples reside in: 1 clinic clean If we've placed our files somewhere else, we can simply add the --path flag to point to that directory instead: 1 clinic clean --path ../some-other-dir After running this command there should be no more samples left in the directory. This can be a handy step if we've finished analysing our app and need to push code changes, but don't want Clinic.js samples in our application's repo.","title":"Managing samples"},{"location":"documentation/cli/03-managing-samples/#ignoring-files-in-git","text":"With git , we can add this entry to our application's .gitignore file to ensure any samples are ignored in the future: 1 .clinic Alternatively, we can use a global gitignore, so the project does not have to change to accomodate Clinic.js. To configure a global ignore file for git, and add .clinic to it, open a command line and do: 1 2 git config --global core.excludesfile ~/.gitignore_global echo '.clinic' >> ~/.gitignore_global","title":"Ignoring files in git"},{"location":"documentation/cli/03-managing-samples/#up-next","text":"Get up to speed with the ins and outs of Clinic.js with the CLI reference .","title":"Up next"},{"location":"documentation/cli/04-reference/","text":"Reference \u00b6 This page contains a reference of all the commands and flags supported by Clinic.js to help us get the most out of using Clinic.js. Commands \u00b6 clinic doctor \u00b6 Separated by the -- flag delimiter, this command takes a start script for a Node.js application and generates Doctor sample to diagnose performance issues. Example : 1 clinic doctor -- node server.js For more technical information follow the Doctor docs . clinic flame \u00b6 Separated by the -- flag delimiter, this command takes a start script for a Node.js application and generates Flame sample to uncover bottlenecks and hot functions in code with flamegraphs. Example : 1 clinic flame -- node server.js For more technical information follow the Flame docs . clinic bubbleprof \u00b6 Separated by the -- flag delimiter, this command takes a start script for a Node.js application and generates Bubbleprof sample to observe and map async operations. Example : 1 clinic bubbleprof -- node server.js For more technical information follow the Bubbleprof docs . Flags \u00b6 Below is a reference list of useful flags (options) that can be passed to Clinic.js when generating samples with Doctor, Flame and Bubbleprof. --on-port \u00b6 Takes a command string which is executed when the application that's being profiled starts listening to a port. Example : 1 clinic doctor --on-port 'my-script localhost:$PORT' -- node server.js More information about this flag with use cases can be found in the Simulating load docs . --autocannon \u00b6 Takes subargs inside the [ ] which are passed to autocannon. Example : 1 clinic doctor --autocannon [ 'localhost:$PORT' ] -- node server.js More information about this flag with use cases can be found in the Simulating load docs . --collect-only \u00b6 If used, the Clinic.js tool will only generate a directory of sample information with no visualization HTML. Example : 1 clinic doctor --collect-only -- node server.js More information about this flag can be found in the Controlling the output docs . --visualize-only \u00b6 Takes a path to a directory of previously collected Clinic tool sample data and generates visualization HTML to view the output in a browser. Example : 1 clinic doctor --visualize-only .clinic/1234.clinic-doctor More information about this flag can be found in the Controlling the output docs . --dest \u00b6 Takes a path to a local directory which the generated sample output is saved to. Example : 1 clinic doctor --dest ../some-other-dir -- node server.js More information about this flag can be found in the Controlling the output docs . Default : . (current directory) --sample-interval \u00b6 Takes a number and changes the rate at which Doctor samples an application in milliseconds. Example : 1 clinic doctor --sample-interval 100 -- node server.js Default : 10 * This flag is applicable with the clinic doctor command. --version \u00b6 If used will output the current installed version of the Clinic.js or the specific Clinic.js tool. Examples : 1 2 3 4 5 6 7 clinic --version # or clinic -v clinic doctor --version # or clinic doctor -v --help \u00b6 If used will output help text with example commands and supported flags for Clinic.js or the specific Clinic.js tool. Examples : 1 2 3 4 5 6 7 clinic --help # or clinic -h clinic doctor --help # or clinic doctor -h","title":"Reference"},{"location":"documentation/cli/04-reference/#reference","text":"This page contains a reference of all the commands and flags supported by Clinic.js to help us get the most out of using Clinic.js.","title":"Reference"},{"location":"documentation/cli/04-reference/#commands","text":"","title":"Commands"},{"location":"documentation/cli/04-reference/#clinic-doctor","text":"Separated by the -- flag delimiter, this command takes a start script for a Node.js application and generates Doctor sample to diagnose performance issues. Example : 1 clinic doctor -- node server.js For more technical information follow the Doctor docs .","title":"clinic doctor"},{"location":"documentation/cli/04-reference/#clinic-flame","text":"Separated by the -- flag delimiter, this command takes a start script for a Node.js application and generates Flame sample to uncover bottlenecks and hot functions in code with flamegraphs. Example : 1 clinic flame -- node server.js For more technical information follow the Flame docs .","title":"clinic flame"},{"location":"documentation/cli/04-reference/#clinic-bubbleprof","text":"Separated by the -- flag delimiter, this command takes a start script for a Node.js application and generates Bubbleprof sample to observe and map async operations. Example : 1 clinic bubbleprof -- node server.js For more technical information follow the Bubbleprof docs .","title":"clinic bubbleprof"},{"location":"documentation/cli/04-reference/#flags","text":"Below is a reference list of useful flags (options) that can be passed to Clinic.js when generating samples with Doctor, Flame and Bubbleprof.","title":"Flags"},{"location":"documentation/cli/04-reference/#-on-port","text":"Takes a command string which is executed when the application that's being profiled starts listening to a port. Example : 1 clinic doctor --on-port 'my-script localhost:$PORT' -- node server.js More information about this flag with use cases can be found in the Simulating load docs .","title":"--on-port"},{"location":"documentation/cli/04-reference/#-autocannon","text":"Takes subargs inside the [ ] which are passed to autocannon. Example : 1 clinic doctor --autocannon [ 'localhost:$PORT' ] -- node server.js More information about this flag with use cases can be found in the Simulating load docs .","title":"--autocannon"},{"location":"documentation/cli/04-reference/#-collect-only","text":"If used, the Clinic.js tool will only generate a directory of sample information with no visualization HTML. Example : 1 clinic doctor --collect-only -- node server.js More information about this flag can be found in the Controlling the output docs .","title":"--collect-only"},{"location":"documentation/cli/04-reference/#-visualize-only","text":"Takes a path to a directory of previously collected Clinic tool sample data and generates visualization HTML to view the output in a browser. Example : 1 clinic doctor --visualize-only .clinic/1234.clinic-doctor More information about this flag can be found in the Controlling the output docs .","title":"--visualize-only"},{"location":"documentation/cli/04-reference/#-dest","text":"Takes a path to a local directory which the generated sample output is saved to. Example : 1 clinic doctor --dest ../some-other-dir -- node server.js More information about this flag can be found in the Controlling the output docs . Default : . (current directory)","title":"--dest"},{"location":"documentation/cli/04-reference/#-sample-interval","text":"Takes a number and changes the rate at which Doctor samples an application in milliseconds. Example : 1 clinic doctor --sample-interval 100 -- node server.js Default : 10 * This flag is applicable with the clinic doctor command.","title":"--sample-interval"},{"location":"documentation/cli/04-reference/#-version","text":"If used will output the current installed version of the Clinic.js or the specific Clinic.js tool. Examples : 1 2 3 4 5 6 7 clinic --version # or clinic -v clinic doctor --version # or clinic doctor -v","title":"--version"},{"location":"documentation/cli/04-reference/#-help","text":"If used will output help text with example commands and supported flags for Clinic.js or the specific Clinic.js tool. Examples : 1 2 3 4 5 6 7 clinic --help # or clinic -h clinic doctor --help # or clinic doctor -h","title":"--help"},{"location":"documentation/doctor/","text":"Doctor \u00b6 Symptoms such as low CPU usage, blocking garbage collection, frequent event loop delay or a chaotic number of active handles may indicate a number of potential problems. Doctor helps narrow down the possibilities by generating a recommendation based on those symptoms. Examples such as I/O issues, non-optimized garbage collection and blocked event loop are quite common. Doctor will help you with all of these. Setup Getting ready First analysis Reading a profile Fixing an event loop problem Fixing an I/O problem","title":"Doctor"},{"location":"documentation/doctor/#doctor","text":"Symptoms such as low CPU usage, blocking garbage collection, frequent event loop delay or a chaotic number of active handles may indicate a number of potential problems. Doctor helps narrow down the possibilities by generating a recommendation based on those symptoms. Examples such as I/O issues, non-optimized garbage collection and blocked event loop are quite common. Doctor will help you with all of these. Setup Getting ready First analysis Reading a profile Fixing an event loop problem Fixing an I/O problem","title":"Doctor"},{"location":"documentation/doctor/01-setup/","text":"Setup \u00b6 Doctor is part of the Clinic.js suit of tools. To install Doctor, simply install Clinic.js like so: 1 npm install -g clinic After installing, we can check if Doctor has been installed by running the clinic doctor command with the --help flag. 1 clinic doctor --help It should print something similar to the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 Clinic.js Doctor - v3.12.0 clinic doctor is the first step in profiling your application. It will show you what kind of problem you are having and recommend the path forward. To run clinic doctor clinic doctor -- node server.js If profiling on a server, it can be useful to only do data collection: clinic doctor --collect-only -- node server.js You can then transfer the data and visualize it locally: clinic doctor --visualize-only PID.clinic-doctor-sample Flags -h | --help Display Help -v | --version Display Version --collect-only Do not process data on termination --visualize-only datapath Build or rebuild visualization from data --sample-interval interval Sample interval in milliseconds --on-port Run a script when the server starts listening on a port. --dest Destination for the collect data (default .). Up next \u00b6 Getting ready","title":"Setup"},{"location":"documentation/doctor/01-setup/#setup","text":"Doctor is part of the Clinic.js suit of tools. To install Doctor, simply install Clinic.js like so: 1 npm install -g clinic After installing, we can check if Doctor has been installed by running the clinic doctor command with the --help flag. 1 clinic doctor --help It should print something similar to the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 Clinic.js Doctor - v3.12.0 clinic doctor is the first step in profiling your application. It will show you what kind of problem you are having and recommend the path forward. To run clinic doctor clinic doctor -- node server.js If profiling on a server, it can be useful to only do data collection: clinic doctor --collect-only -- node server.js You can then transfer the data and visualize it locally: clinic doctor --visualize-only PID.clinic-doctor-sample Flags -h | --help Display Help -v | --version Display Version --collect-only Do not process data on termination --visualize-only datapath Build or rebuild visualization from data --sample-interval interval Sample interval in milliseconds --on-port Run a script when the server starts listening on a port. --dest Destination for the collect data (default .).","title":"Setup"},{"location":"documentation/doctor/01-setup/#up-next","text":"Getting ready","title":"Up next"},{"location":"documentation/doctor/02-getting-ready/","text":"Getting ready \u00b6 We need some example applications to use Doctor on. The repo node-clinic-doctor-examples contains several for us to explore. Let's run the following to clone the repository and install its dependencies: 1 2 3 git clone git@github.com:nearform/node-clinic-doctor-examples.git cd node-clinic-doctor-examples npm install The README contains some general information, which can be read while the install completes. Up next \u00b6 First analysis","title":"Getting ready"},{"location":"documentation/doctor/02-getting-ready/#getting-ready","text":"We need some example applications to use Doctor on. The repo node-clinic-doctor-examples contains several for us to explore. Let's run the following to clone the repository and install its dependencies: 1 2 3 git clone git@github.com:nearform/node-clinic-doctor-examples.git cd node-clinic-doctor-examples npm install The README contains some general information, which can be read while the install completes.","title":"Getting ready"},{"location":"documentation/doctor/02-getting-ready/#up-next","text":"First analysis","title":"Up next"},{"location":"documentation/doctor/03-first-analysis/","text":"First analysis \u00b6 We're now ready to profile one of the example applications. For the first example, we will use slow-event-loop . First, let's confirm that it is ready and working by running node slow-event-loop from inside the examples directory. Once the process seems to be running we can visit http://localhost:3000/ in a browser to check. We should see some basic output in the browser, like {} . Ctrl-C in the command line to close the slow-event-loop server. This is a server, so we need to apply load. Profiling a server handling just one request doesn't give us much data or indication of how it performs when handling many requests. We recommend the benchmarking tool Autocannon . We will execute autocannon in the example application directories when we call the clinic executable, so let's install it globally, with the following command: 1 npm install -g autocannon To load-test the server, we want to run it with Doctor, and point autocannon at it as soon as it starts listening on a port. This will bombard the server with requests, as soon as it is ready to handle them and Doctor is ready to collect data. Let's do all that with this single command, which automatically assigns the correct ports: 1 clinic doctor --on-port 'autocannon localhost:$PORT' -- node slow-event-loop Let's break this command down: The clinic doctor portion invokes the Doctor command tool. The --on-port flag will execute the supplied script as soon as the server starts listening on a port. The $PORT variable in that script is set to the first port that the server began listening on. Everything after the double-dash ( -- ) is the command which starts the server that we want to profile, in this case node slow-event-loop . This one command runs three executables: the clinic doctor parent executable, the autocannon executable in --on-port and the node executable. Upon running the command, the slow-event-loop server will be hit by requests from 10 concurrent connections for 10 seconds (as per autocannon defaults). then the results be compiled into a single HTML file that should automatically open in the browser. The resulting HTML should look similar to the following: Up next \u00b6 Reading a profile","title":"First analysis"},{"location":"documentation/doctor/03-first-analysis/#first-analysis","text":"We're now ready to profile one of the example applications. For the first example, we will use slow-event-loop . First, let's confirm that it is ready and working by running node slow-event-loop from inside the examples directory. Once the process seems to be running we can visit http://localhost:3000/ in a browser to check. We should see some basic output in the browser, like {} . Ctrl-C in the command line to close the slow-event-loop server. This is a server, so we need to apply load. Profiling a server handling just one request doesn't give us much data or indication of how it performs when handling many requests. We recommend the benchmarking tool Autocannon . We will execute autocannon in the example application directories when we call the clinic executable, so let's install it globally, with the following command: 1 npm install -g autocannon To load-test the server, we want to run it with Doctor, and point autocannon at it as soon as it starts listening on a port. This will bombard the server with requests, as soon as it is ready to handle them and Doctor is ready to collect data. Let's do all that with this single command, which automatically assigns the correct ports: 1 clinic doctor --on-port 'autocannon localhost:$PORT' -- node slow-event-loop Let's break this command down: The clinic doctor portion invokes the Doctor command tool. The --on-port flag will execute the supplied script as soon as the server starts listening on a port. The $PORT variable in that script is set to the first port that the server began listening on. Everything after the double-dash ( -- ) is the command which starts the server that we want to profile, in this case node slow-event-loop . This one command runs three executables: the clinic doctor parent executable, the autocannon executable in --on-port and the node executable. Upon running the command, the slow-event-loop server will be hit by requests from 10 concurrent connections for 10 seconds (as per autocannon defaults). then the results be compiled into a single HTML file that should automatically open in the browser. The resulting HTML should look similar to the following:","title":"First analysis"},{"location":"documentation/doctor/03-first-analysis/#up-next","text":"Reading a profile","title":"Up next"},{"location":"documentation/doctor/04-reading-a-profile/","text":"Reading a profile \u00b6 The Clinic.js Doctor profile has three main sections: Alert Bar : Points us towards the main problem, if there is one. Also contains View Controls . Graphs : Plot the data from which Doctor is drawing its conclusions Recommendations Panel : Detailed explanation of the problem with next steps Alert Bar \u00b6 Click on this to open it out, and we see a one-line summary of the main problem, if there is one. Hover over this summary and it will underline the title of the specific graph that Doctor thinks is most relevant to understanding the problem. Doctor does not generally identify more than one issue, so there will generally be either one problem here, or a note that no problems were found. This is because one performance problem can disrupt the data needed to diagnose another problem. For example, if Doctor is sure there is an event loop problem, it might not be able to take enough readings to judge if there is an I/O problem. A first-time user will mainly use the Alert Bar to see if there is a detected problem or not, before going straight to the description in the Recommendations Panel to understand it better. A more experienced user will recognise common detected problems and then study the appropriate graphs for clues about how this particular example of the problem is manefesting itself. In the above example, Doctor is telling us that it has detected a problem, and the problem is \"a potential Event Loop issue\", which can be seen in the Event Loop Delay graph: View controls \u00b6 To the right of the Alert Bar there are two buttons to change the view: Grid View / List View. By default, Doctor shows all graphs in a grid so they can all be seen on the screen at once. This button switches to a List View where each graph takes the full width of the screen. This can be useful for advanced users who might want to study the detail of the graphs. When in List View, clicking on the problem described in the Alert Bar scrolls the page down to the most relevant graph, if it is not in view. Light Theme / Dark Theme. By default, Doctor uses a theme with a dark background and light text. This is good for reducing glare, but some in situations (or for some individual preferences), a theme with a light background and dark text is better. For example, we will probably want to switch to the Light Theme when taking screenshots that will be printed on paper, or when projecting a Doctor profile in a well-lit room where the dark theme is hard to read. Graphs \u00b6 These plot various variables used in Doctor's analysis over time, from the start time of the profile (left end of the X-axis) to the finish time (right end of the X-axis). All graphs use the same X-axis. We will see that hovering over one shows the values at the same point in time on all the other graphs. CPU Usage % \u00b6 This graph shows what percentage of the machine's available CPU capacity was being used by the Node.js process being profiled at any one point in time. CPU Usage can exceed 100% if the machine has multiple cores. 100% on this graph means 100% of the capacity of a single core. Spikes in this graph indicate high CPU activity. This can be a problem if it is excessive and correlates with event loop blockage (see below), but rapid spikes can be a sign that the server is healthily processing high load quickly. Too little CPU activity can be a sign that the Node.js process is stuck waiting for an I/O operation to complete, like a slow database query or file write. In this profile, the processor is usually fairly busy, which looks healthy. In part 6 of this walkthrough, Fixing an I/O problem , we will see an example of an unhealthy CPU Usage graph. Memory Usage MB \u00b6 This graph has three lines, showing megabytes of memory at each point in time, all on the same scale. The three lines are: RSS (Resident Set Size) : This will always be the highest value, representing all memory allocated as part of the execution of this process. The gap between this line and the Total Heap Allocated line represents non-heap memory, such as storing of the JS code itself, the \"stack\" which contains variable pointers and primitives like boolean and integer states, and a memory pool for Buffer contents. THA (Total Heap Allocated) : This is the amount of space that has been set aside for storing items that have a reference, such as strings, objects and function closures. Unlike the stack, where the reference pointers to these items are stored, a pre-set amount of memory is assigned for the heap, before it is needed. HU (Heap Used) : This is how much heap memory is actually being used at this point. It represents the total size of all strings, objects, closures etc that have been allocated but not garbage collected at a given point in time. This is usually the most interesting line , with RSS and Total Heap Allocated providing context. A constantly increasing Heap Used line suggests a possible memory leak, where references to something are remaining in-scope, meaning it can't ever be garbage collected and therefore causing available memory to eventually run dry. The opposite is perhaps a more common problem: many sharp drops, correlating with high readings in the Event Loop Delay graph, suggests that disruptive garbage collection events are disrupting the process and blocking Node.js from executing code. In this profile, the heap goes up and down fairly gradually, there is always plenty of spare heap allocation, and there is plenty of non-heap memory in the Resident Set as well. This looks healthy. If we encounter an unhealthy Memory graph, the specific line indicating a problem will be marked in red: Event Loop Delay ms \u00b6 This represents points in time in which Node.js was blocked by executing synchronous JavaScript code. It is important that we understand how this graph works, because it also gives us information about the acuity of the other graphs: The Y-axis represents the duration of the event loop delay that ended at the moment in time indicated by the tooltip arrow This is then always followed by a horizontal line representing the same amount of time on the X-axis. For the length of this line, Node.js was blocked , therefore, we don't have any data on any graphs. If we run the cursor along a graph containing substantial event loop delay, the tooltip jumps - this is because no data could be collected between the jumps, for any graphs, because Node.js was stuck executing some slow synchronous code. For example, in the below screenshot, we can see: That this is the (joint) longest event loop delay seen in this profile, because it is the highest point on the Y-axis. That this delay took up a noticable chunk of the duration of the profile, by looking at the horizontal line between this tooltip and the previous one. Moving the cursor along, we can see that four event loop delays account for most of the run time. We can also see that this is causing the other data to be very course - after the first quarter of a second or so, there are noticable jumps between each reading for memory, CPU, etc, because Node.js was too busy executing some slow synchronous code to even take another reading. This is clearly not healthy - and Doctor has flagged it as such, colouring this graph red and pointing to it in the Alert box. Active Handles \u00b6 This graph shows the quantity of I/O handles presently active, awaiting an output. When Node.js delegates asychronously, such as using libuv to delegate a task like a file write or a database query the operating system, it stores a \"handle\". An \"active handle\" is a delegated task that has not reported as being complete. The Active Handles graph therefore gives us an idea of how much asychronous I/O activity the Node.js process is waiting on at any point in time. This should ideally follow an orderly pattern, rising and falling as requests are handled and completed. It can also offer clues when combined with the other graphs - for example, CPU spikes on a server that correlate with increased active handles should usually also correlate with incoming requests. This graph generally provides context for the other graphs. It's hard to say generically what an Active Handles graph \"should\" look like: we generally can't point to an Active Handles graph and say \"That looks unhealthy\" without knowing the application logic. Here, we have a period with very few active handles, and very little other activity, which presumably represents the process getting ready. There is then a steady period of [103] active handles, which presumably represents incoming requests being dealt with. It tells us we can probably ignore that early period with very few active handles as not representing typical server activity. Recommendations Panel \u00b6 Click on the blue bar at the bottom, and it will open a panel telling us more about Doctor's conclusions about what is happening with this application. This is in two parts: a Recommendations Summary, and a Recommendation in Detail article below a 'Read more' button. Recommendations Summary \u00b6 This gives a simple bullet point overview of the identified problem, typically with a suggested next step. There are a few UI controls: The 'x' closes the panel \"Browse undetected issues\" allows us to read descriptions of issues that Doctor can identify but hasn't identified for this profile. Clicking on this expands out some tabs to show descriptions of problems that Doctor has not identified for this profile. For example, We might find this useful, for example: While learning about Node.js performance, to avoid creating new problems while fixing an existing problem. To understand why Doctor might not have identified a known problem. As discussed in the Alert Bar section , Doctor generally does not diagnose more than one problem at a time. In this case, it tells us that \"There may be one or more long running synchronous operations blocking the thread\", and recommends using clinic flame to narrow down. After reading the summary, we recommend clicking \"Read more\" to understand the problem in more depth. Recommendation in Detail \u00b6 Clicking 'Read more' expands the Recommendations Panel to show a detailed article on the performance problem that has been diagnosed. These usually have three sections (clicking on the contents list on the left allows us to skip to a particular section): Understanding the analysis describes the problem in depth. Next Steps details some recommended steps to narrow down on the exact cause of the problem, so we can fix it. Usually this involves using another tool in the Clinic.js suite which can identify individual lines of problematic code. Reference gives links for suggested further reading and credits any sources that were quoted or used in writing this recommendation. Then we're ready to look at fixing the problem. Up next \u00b6 Fixing an event loop problem","title":"Reading a profile"},{"location":"documentation/doctor/04-reading-a-profile/#reading-a-profile","text":"The Clinic.js Doctor profile has three main sections: Alert Bar : Points us towards the main problem, if there is one. Also contains View Controls . Graphs : Plot the data from which Doctor is drawing its conclusions Recommendations Panel : Detailed explanation of the problem with next steps","title":"Reading a profile"},{"location":"documentation/doctor/04-reading-a-profile/#alert-bar","text":"Click on this to open it out, and we see a one-line summary of the main problem, if there is one. Hover over this summary and it will underline the title of the specific graph that Doctor thinks is most relevant to understanding the problem. Doctor does not generally identify more than one issue, so there will generally be either one problem here, or a note that no problems were found. This is because one performance problem can disrupt the data needed to diagnose another problem. For example, if Doctor is sure there is an event loop problem, it might not be able to take enough readings to judge if there is an I/O problem. A first-time user will mainly use the Alert Bar to see if there is a detected problem or not, before going straight to the description in the Recommendations Panel to understand it better. A more experienced user will recognise common detected problems and then study the appropriate graphs for clues about how this particular example of the problem is manefesting itself. In the above example, Doctor is telling us that it has detected a problem, and the problem is \"a potential Event Loop issue\", which can be seen in the Event Loop Delay graph:","title":"Alert Bar"},{"location":"documentation/doctor/04-reading-a-profile/#view-controls","text":"To the right of the Alert Bar there are two buttons to change the view: Grid View / List View. By default, Doctor shows all graphs in a grid so they can all be seen on the screen at once. This button switches to a List View where each graph takes the full width of the screen. This can be useful for advanced users who might want to study the detail of the graphs. When in List View, clicking on the problem described in the Alert Bar scrolls the page down to the most relevant graph, if it is not in view. Light Theme / Dark Theme. By default, Doctor uses a theme with a dark background and light text. This is good for reducing glare, but some in situations (or for some individual preferences), a theme with a light background and dark text is better. For example, we will probably want to switch to the Light Theme when taking screenshots that will be printed on paper, or when projecting a Doctor profile in a well-lit room where the dark theme is hard to read.","title":"View controls"},{"location":"documentation/doctor/04-reading-a-profile/#graphs","text":"These plot various variables used in Doctor's analysis over time, from the start time of the profile (left end of the X-axis) to the finish time (right end of the X-axis). All graphs use the same X-axis. We will see that hovering over one shows the values at the same point in time on all the other graphs.","title":"Graphs"},{"location":"documentation/doctor/04-reading-a-profile/#cpu-usage","text":"This graph shows what percentage of the machine's available CPU capacity was being used by the Node.js process being profiled at any one point in time. CPU Usage can exceed 100% if the machine has multiple cores. 100% on this graph means 100% of the capacity of a single core. Spikes in this graph indicate high CPU activity. This can be a problem if it is excessive and correlates with event loop blockage (see below), but rapid spikes can be a sign that the server is healthily processing high load quickly. Too little CPU activity can be a sign that the Node.js process is stuck waiting for an I/O operation to complete, like a slow database query or file write. In this profile, the processor is usually fairly busy, which looks healthy. In part 6 of this walkthrough, Fixing an I/O problem , we will see an example of an unhealthy CPU Usage graph.","title":"CPU Usage %"},{"location":"documentation/doctor/04-reading-a-profile/#memory-usage-mb","text":"This graph has three lines, showing megabytes of memory at each point in time, all on the same scale. The three lines are: RSS (Resident Set Size) : This will always be the highest value, representing all memory allocated as part of the execution of this process. The gap between this line and the Total Heap Allocated line represents non-heap memory, such as storing of the JS code itself, the \"stack\" which contains variable pointers and primitives like boolean and integer states, and a memory pool for Buffer contents. THA (Total Heap Allocated) : This is the amount of space that has been set aside for storing items that have a reference, such as strings, objects and function closures. Unlike the stack, where the reference pointers to these items are stored, a pre-set amount of memory is assigned for the heap, before it is needed. HU (Heap Used) : This is how much heap memory is actually being used at this point. It represents the total size of all strings, objects, closures etc that have been allocated but not garbage collected at a given point in time. This is usually the most interesting line , with RSS and Total Heap Allocated providing context. A constantly increasing Heap Used line suggests a possible memory leak, where references to something are remaining in-scope, meaning it can't ever be garbage collected and therefore causing available memory to eventually run dry. The opposite is perhaps a more common problem: many sharp drops, correlating with high readings in the Event Loop Delay graph, suggests that disruptive garbage collection events are disrupting the process and blocking Node.js from executing code. In this profile, the heap goes up and down fairly gradually, there is always plenty of spare heap allocation, and there is plenty of non-heap memory in the Resident Set as well. This looks healthy. If we encounter an unhealthy Memory graph, the specific line indicating a problem will be marked in red:","title":"Memory Usage MB"},{"location":"documentation/doctor/04-reading-a-profile/#event-loop-delay-ms","text":"This represents points in time in which Node.js was blocked by executing synchronous JavaScript code. It is important that we understand how this graph works, because it also gives us information about the acuity of the other graphs: The Y-axis represents the duration of the event loop delay that ended at the moment in time indicated by the tooltip arrow This is then always followed by a horizontal line representing the same amount of time on the X-axis. For the length of this line, Node.js was blocked , therefore, we don't have any data on any graphs. If we run the cursor along a graph containing substantial event loop delay, the tooltip jumps - this is because no data could be collected between the jumps, for any graphs, because Node.js was stuck executing some slow synchronous code. For example, in the below screenshot, we can see: That this is the (joint) longest event loop delay seen in this profile, because it is the highest point on the Y-axis. That this delay took up a noticable chunk of the duration of the profile, by looking at the horizontal line between this tooltip and the previous one. Moving the cursor along, we can see that four event loop delays account for most of the run time. We can also see that this is causing the other data to be very course - after the first quarter of a second or so, there are noticable jumps between each reading for memory, CPU, etc, because Node.js was too busy executing some slow synchronous code to even take another reading. This is clearly not healthy - and Doctor has flagged it as such, colouring this graph red and pointing to it in the Alert box.","title":"Event Loop Delay ms"},{"location":"documentation/doctor/04-reading-a-profile/#active-handles","text":"This graph shows the quantity of I/O handles presently active, awaiting an output. When Node.js delegates asychronously, such as using libuv to delegate a task like a file write or a database query the operating system, it stores a \"handle\". An \"active handle\" is a delegated task that has not reported as being complete. The Active Handles graph therefore gives us an idea of how much asychronous I/O activity the Node.js process is waiting on at any point in time. This should ideally follow an orderly pattern, rising and falling as requests are handled and completed. It can also offer clues when combined with the other graphs - for example, CPU spikes on a server that correlate with increased active handles should usually also correlate with incoming requests. This graph generally provides context for the other graphs. It's hard to say generically what an Active Handles graph \"should\" look like: we generally can't point to an Active Handles graph and say \"That looks unhealthy\" without knowing the application logic. Here, we have a period with very few active handles, and very little other activity, which presumably represents the process getting ready. There is then a steady period of [103] active handles, which presumably represents incoming requests being dealt with. It tells us we can probably ignore that early period with very few active handles as not representing typical server activity.","title":"Active Handles"},{"location":"documentation/doctor/04-reading-a-profile/#recommendations-panel","text":"Click on the blue bar at the bottom, and it will open a panel telling us more about Doctor's conclusions about what is happening with this application. This is in two parts: a Recommendations Summary, and a Recommendation in Detail article below a 'Read more' button.","title":"Recommendations Panel"},{"location":"documentation/doctor/04-reading-a-profile/#recommendations-summary","text":"This gives a simple bullet point overview of the identified problem, typically with a suggested next step. There are a few UI controls: The 'x' closes the panel \"Browse undetected issues\" allows us to read descriptions of issues that Doctor can identify but hasn't identified for this profile. Clicking on this expands out some tabs to show descriptions of problems that Doctor has not identified for this profile. For example, We might find this useful, for example: While learning about Node.js performance, to avoid creating new problems while fixing an existing problem. To understand why Doctor might not have identified a known problem. As discussed in the Alert Bar section , Doctor generally does not diagnose more than one problem at a time. In this case, it tells us that \"There may be one or more long running synchronous operations blocking the thread\", and recommends using clinic flame to narrow down. After reading the summary, we recommend clicking \"Read more\" to understand the problem in more depth.","title":"Recommendations Summary"},{"location":"documentation/doctor/04-reading-a-profile/#recommendation-in-detail","text":"Clicking 'Read more' expands the Recommendations Panel to show a detailed article on the performance problem that has been diagnosed. These usually have three sections (clicking on the contents list on the left allows us to skip to a particular section): Understanding the analysis describes the problem in depth. Next Steps details some recommended steps to narrow down on the exact cause of the problem, so we can fix it. Usually this involves using another tool in the Clinic.js suite which can identify individual lines of problematic code. Reference gives links for suggested further reading and credits any sources that were quoted or used in writing this recommendation. Then we're ready to look at fixing the problem.","title":"Recommendation in Detail"},{"location":"documentation/doctor/04-reading-a-profile/#up-next","text":"Fixing an event loop problem","title":"Up next"},{"location":"documentation/doctor/05-fixing-event-loop-problem/","text":"Fixing an event loop problem \u00b6 In Reading A Profile , we saw how to understand the information Doctor is giving us, and why it recommends what it does. Now we'll look at fixing the problems. Consulting the Doctor \u00b6 We've already read the profile we created in First Analysis , from the slow-event-loop example server in node-clinic-doctor-examples . We were told that our problem was slow synchronous code blocking the event loop , and Doctor recommended using clinic flame to identify the problem. Following the prescription \u00b6 We can create a Flame profile with a command that is the same as for Doctor, but swapping flame in for doctor : 1 clinic flame --on-port 'autocannon localhost:$PORT' -- node slow-event-loop Our output looks something like this: Clinic.js Flame by default selects the function call that spends most time blocking the event loop, and has identified that the function spending the most time blocking the event loop is sleep , on line 12 of slow-event-loop/index.js . node-clinic-doctor-examples uses very simple example servers: for this example, we don't need to dig deeper into the diagram or advanced features detailed in the Clinic.js Flame documentation walkthrough . We can immediately open up index.js and look for fixable bottlenecks in the line Flame has picked out: 1 2 3 4 5 server . get ( '/' , function ( req , res , next ) { sleep ( 30 ) res . send ({}) next () }) That sleep(30) call is the one flame picked out. Let's see what this sleep function does: 1 2 3 4 function sleep ( ms ) { const future = Date . now () + ms while ( Date . now () < future ); } It's clear why this is causing an event loop delay. The event loop is single-threaded: only one operation is processed at a time. Each request to this server queues a synchronous function on the event loop containing a while loop which will repeatedly iterate for 30 milliseconds. Each of these blocks the event loop: the single thread is busy iterating the while loop, unable to process any other operations. If the pause was from an asynchronous timeout, like from setTimeout , the event loop would not be blocked. The synchronous code containing setTimeout would continue and complete, and then the callback function passed to setTimeout would be called in a separate, future tick of the event loop. This sleep function, however, is entirely synchronous. The event loop is blocked until it completes. Curing the ailment \u00b6 Let's reduce the duration of the loops, changing the arg passed to sleep to 1 : 1 2 server . get ( '/' , function ( req , res , next ) { sleep ( 1 ) ...save, and recreate the profile: 1 clinic doctor --on-port 'autocannon localhost:$PORT' -- node slow-event-loop The profile now detects no issues. Everything is blue, the graphs look healthy and Doctor's Recommendations Panel is cheerfully telling us that \"Everything looks good!\". This is a very simple example server. In a more complex application, we would normally need to explore the Clinic.js Flame profile further to hone in on the cause or causes of the bottleneck. Detailed examples of how to do this are available in the Clinic.js Flame documentation . Up next \u00b6 Fixing an I/O problem","title":"Fixing an event loop problem"},{"location":"documentation/doctor/05-fixing-event-loop-problem/#fixing-an-event-loop-problem","text":"In Reading A Profile , we saw how to understand the information Doctor is giving us, and why it recommends what it does. Now we'll look at fixing the problems.","title":"Fixing an event loop problem"},{"location":"documentation/doctor/05-fixing-event-loop-problem/#consulting-the-doctor","text":"We've already read the profile we created in First Analysis , from the slow-event-loop example server in node-clinic-doctor-examples . We were told that our problem was slow synchronous code blocking the event loop , and Doctor recommended using clinic flame to identify the problem.","title":"Consulting the Doctor"},{"location":"documentation/doctor/05-fixing-event-loop-problem/#following-the-prescription","text":"We can create a Flame profile with a command that is the same as for Doctor, but swapping flame in for doctor : 1 clinic flame --on-port 'autocannon localhost:$PORT' -- node slow-event-loop Our output looks something like this: Clinic.js Flame by default selects the function call that spends most time blocking the event loop, and has identified that the function spending the most time blocking the event loop is sleep , on line 12 of slow-event-loop/index.js . node-clinic-doctor-examples uses very simple example servers: for this example, we don't need to dig deeper into the diagram or advanced features detailed in the Clinic.js Flame documentation walkthrough . We can immediately open up index.js and look for fixable bottlenecks in the line Flame has picked out: 1 2 3 4 5 server . get ( '/' , function ( req , res , next ) { sleep ( 30 ) res . send ({}) next () }) That sleep(30) call is the one flame picked out. Let's see what this sleep function does: 1 2 3 4 function sleep ( ms ) { const future = Date . now () + ms while ( Date . now () < future ); } It's clear why this is causing an event loop delay. The event loop is single-threaded: only one operation is processed at a time. Each request to this server queues a synchronous function on the event loop containing a while loop which will repeatedly iterate for 30 milliseconds. Each of these blocks the event loop: the single thread is busy iterating the while loop, unable to process any other operations. If the pause was from an asynchronous timeout, like from setTimeout , the event loop would not be blocked. The synchronous code containing setTimeout would continue and complete, and then the callback function passed to setTimeout would be called in a separate, future tick of the event loop. This sleep function, however, is entirely synchronous. The event loop is blocked until it completes.","title":"Following the prescription"},{"location":"documentation/doctor/05-fixing-event-loop-problem/#curing-the-ailment","text":"Let's reduce the duration of the loops, changing the arg passed to sleep to 1 : 1 2 server . get ( '/' , function ( req , res , next ) { sleep ( 1 ) ...save, and recreate the profile: 1 clinic doctor --on-port 'autocannon localhost:$PORT' -- node slow-event-loop The profile now detects no issues. Everything is blue, the graphs look healthy and Doctor's Recommendations Panel is cheerfully telling us that \"Everything looks good!\". This is a very simple example server. In a more complex application, we would normally need to explore the Clinic.js Flame profile further to hone in on the cause or causes of the bottleneck. Detailed examples of how to do this are available in the Clinic.js Flame documentation .","title":"Curing the ailment"},{"location":"documentation/doctor/05-fixing-event-loop-problem/#up-next","text":"Fixing an I/O problem","title":"Up next"},{"location":"documentation/doctor/06-fixing-io-problem/","text":"Fixing an I/O problem \u00b6 In Reading A Profile , we saw that the CPU Usage graph can indicate problems with Node.js I/O (Input/Output) operations delegated to other processes, such as slow database queries or file writes delegated by libuv ). Let's look at that in more detail with an example. Consulting the Doctor \u00b6 There is an example server for this problem in node-clinic-doctor-examples , called slow-io . Assuming we have everything set up as described in Getting Ready and First Analysis , let's create a Clinic.js Doctor profile from that server: 1 clinic doctor --on-port 'autocannon localhost:$PORT' -- node slow-io The output should look something like this: The CPU Usage graph is highlighted in red. It shows several spikes, but is mostly low. There is less CPU activity than we'd expect from a busy server. The Recommendations Panel explains that this is likely caused by slow asynchronous operations: our application is waiting for external I/O to resolve promises or trigger callbacks. This is a very different problem to the one we saw while Fixing an event loop problem . The Recommendations Panel advises that we use another Clinic.js tool, clinic bubbleprof . Following the prescription \u00b6 We can create a Bubbleprof profile with a command that is the same as for Doctor, but swapping bubbleprof in for doctor : 1 clinic bubbleprof --on-port 'autocannon localhost:$PORT' -- node slow-io Our output looks something like this: node-clinic-doctor-examples uses very simple example servers, so for now we'll only need to look at the main diagram, not the more advanced features detailed in the Clinic.js Bubbleprof documentation walkthrough . The main diagram shows a busy http.connection , calling a timeout , which then calls more timeout s in parrallel. That first timeout looks key - the rest of the application branches off from it. It could be our bottleneck. Clicking on it opens it out to show two parts. Clicking on the longer part points us to some code: a function async.series , in our application, file ./index.js , line 9, column 16: If we open node-clinic-doctor-examples/slow-io/index.js and find that line, we see: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 function awaitData ( callback ) { async . series ( [ done1 => setTimeout ( done1 , Math . random () * 1000 ), done1 => async . parallel ( [ done2 => setTimeout ( done2 , Math . random () * 1000 ), done2 => setTimeout ( done2 , Math . random () * 1000 ), done2 => setTimeout ( done2 , Math . random () * 1000 ), done2 => setTimeout ( done2 , Math . random () * 1000 ), done2 => setTimeout ( done2 , Math . random () * 1000 ) ], done1 ) ], callback ) } This is what Node.js is waiting on - chained timeouts. If the delay was an external process like a slow database query, the clues visible to us within Node.js would be the same. We can't see what exactly is happening within the external operation, but we can identify which asynchronous operation Node.js is waiting for. Curing the ailment \u00b6 Let's reduce the duration of the timeouts, changing the second argument passed to setTimeout from 1000 to 1 . This simulates dramatically speeding up the external I/O: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 function awaitData ( callback ) { async . series ( [ done1 => setTimeout ( done1 , 1 ), done1 => async . parallel ( [ done2 => setTimeout ( done2 , 1 ), done2 => setTimeout ( done2 , 1 ), done2 => setTimeout ( done2 , 1 ), done2 => setTimeout ( done2 , 1 ), done2 => setTimeout ( done2 , 1 ) ], done1 ) ], callback ) } We then save, and recreate the profile: 1 clinic doctor --on-port 'autocannon localhost:$PORT' -- node slow-io The profile now detects no issues. Everything is blue, the graphs look healthy, the CPU is active, and Doctor's Recommendations Panel is cheerfully telling us that \"Everything looks good!\". This is a very simple example server. In a more complex application, we would normally need to explore the Clinic.js Bubbleprof profile deeper to hone in on the cause or causes of the bottleneck. Detailed examples of how to do this are available in the Clinic.js Bubbleprof documentation . Up next \u00b6 We're now familiar with how to use Doctor to identify the type of problem. The next step is learning more about those tools we can use to hone in on specific code. We're now ready to move on to the walkthrough documentation for: Clinic.js Flame , for identifying slow synchronous code Clinic.js Bubbleprof , for finding problems in asynchronous code","title":"Fixing an I/O problem"},{"location":"documentation/doctor/06-fixing-io-problem/#fixing-an-io-problem","text":"In Reading A Profile , we saw that the CPU Usage graph can indicate problems with Node.js I/O (Input/Output) operations delegated to other processes, such as slow database queries or file writes delegated by libuv ). Let's look at that in more detail with an example.","title":"Fixing an I/O problem"},{"location":"documentation/doctor/06-fixing-io-problem/#consulting-the-doctor","text":"There is an example server for this problem in node-clinic-doctor-examples , called slow-io . Assuming we have everything set up as described in Getting Ready and First Analysis , let's create a Clinic.js Doctor profile from that server: 1 clinic doctor --on-port 'autocannon localhost:$PORT' -- node slow-io The output should look something like this: The CPU Usage graph is highlighted in red. It shows several spikes, but is mostly low. There is less CPU activity than we'd expect from a busy server. The Recommendations Panel explains that this is likely caused by slow asynchronous operations: our application is waiting for external I/O to resolve promises or trigger callbacks. This is a very different problem to the one we saw while Fixing an event loop problem . The Recommendations Panel advises that we use another Clinic.js tool, clinic bubbleprof .","title":"Consulting the Doctor"},{"location":"documentation/doctor/06-fixing-io-problem/#following-the-prescription","text":"We can create a Bubbleprof profile with a command that is the same as for Doctor, but swapping bubbleprof in for doctor : 1 clinic bubbleprof --on-port 'autocannon localhost:$PORT' -- node slow-io Our output looks something like this: node-clinic-doctor-examples uses very simple example servers, so for now we'll only need to look at the main diagram, not the more advanced features detailed in the Clinic.js Bubbleprof documentation walkthrough . The main diagram shows a busy http.connection , calling a timeout , which then calls more timeout s in parrallel. That first timeout looks key - the rest of the application branches off from it. It could be our bottleneck. Clicking on it opens it out to show two parts. Clicking on the longer part points us to some code: a function async.series , in our application, file ./index.js , line 9, column 16: If we open node-clinic-doctor-examples/slow-io/index.js and find that line, we see: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 function awaitData ( callback ) { async . series ( [ done1 => setTimeout ( done1 , Math . random () * 1000 ), done1 => async . parallel ( [ done2 => setTimeout ( done2 , Math . random () * 1000 ), done2 => setTimeout ( done2 , Math . random () * 1000 ), done2 => setTimeout ( done2 , Math . random () * 1000 ), done2 => setTimeout ( done2 , Math . random () * 1000 ), done2 => setTimeout ( done2 , Math . random () * 1000 ) ], done1 ) ], callback ) } This is what Node.js is waiting on - chained timeouts. If the delay was an external process like a slow database query, the clues visible to us within Node.js would be the same. We can't see what exactly is happening within the external operation, but we can identify which asynchronous operation Node.js is waiting for.","title":"Following the prescription"},{"location":"documentation/doctor/06-fixing-io-problem/#curing-the-ailment","text":"Let's reduce the duration of the timeouts, changing the second argument passed to setTimeout from 1000 to 1 . This simulates dramatically speeding up the external I/O: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 function awaitData ( callback ) { async . series ( [ done1 => setTimeout ( done1 , 1 ), done1 => async . parallel ( [ done2 => setTimeout ( done2 , 1 ), done2 => setTimeout ( done2 , 1 ), done2 => setTimeout ( done2 , 1 ), done2 => setTimeout ( done2 , 1 ), done2 => setTimeout ( done2 , 1 ) ], done1 ) ], callback ) } We then save, and recreate the profile: 1 clinic doctor --on-port 'autocannon localhost:$PORT' -- node slow-io The profile now detects no issues. Everything is blue, the graphs look healthy, the CPU is active, and Doctor's Recommendations Panel is cheerfully telling us that \"Everything looks good!\". This is a very simple example server. In a more complex application, we would normally need to explore the Clinic.js Bubbleprof profile deeper to hone in on the cause or causes of the bottleneck. Detailed examples of how to do this are available in the Clinic.js Bubbleprof documentation .","title":"Curing the ailment"},{"location":"documentation/doctor/06-fixing-io-problem/#up-next","text":"We're now familiar with how to use Doctor to identify the type of problem. The next step is learning more about those tools we can use to hone in on specific code. We're now ready to move on to the walkthrough documentation for: Clinic.js Flame , for identifying slow synchronous code Clinic.js Bubbleprof , for finding problems in asynchronous code","title":"Up next"},{"location":"documentation/flame/","text":"Fl\u706b\u7130\u56feame \u00b6 \u5982\u679c\u4f60\u60f3\u5feb\u901f\u5f00\u59cb\u7406\u89e3\u706b\u7130\u56fe\u5e76\u5b66\u4e60\u5982\u4f55\u4f18\u5316Node.js\u4ee3\u7801\uff0c\u90a3\u4e48\u8fd9\u662f\u4e00\u4e2a\u5f88\u597d\u7684\u5f00\u59cb\u3002 \u8bbe\u7f6e \u51c6\u5907 \u9996\u5148\u5206\u6790 \u706b\u7130\u56fe \u63a7\u5236 \u4f18\u5316\u70ed\u51fd\u6570 \u51cf\u5c0f\u56fe\u7684\u5927\u5c0f \u5148\u8fdb\u7684\u5206\u6790 \u5148\u8fdb\u7684\u63a7\u5236","title":"Fl\u706b\u7130\u56feame"},{"location":"documentation/flame/#flame","text":"\u5982\u679c\u4f60\u60f3\u5feb\u901f\u5f00\u59cb\u7406\u89e3\u706b\u7130\u56fe\u5e76\u5b66\u4e60\u5982\u4f55\u4f18\u5316Node.js\u4ee3\u7801\uff0c\u90a3\u4e48\u8fd9\u662f\u4e00\u4e2a\u5f88\u597d\u7684\u5f00\u59cb\u3002 \u8bbe\u7f6e \u51c6\u5907 \u9996\u5148\u5206\u6790 \u706b\u7130\u56fe \u63a7\u5236 \u4f18\u5316\u70ed\u51fd\u6570 \u51cf\u5c0f\u56fe\u7684\u5927\u5c0f \u5148\u8fdb\u7684\u5206\u6790 \u5148\u8fdb\u7684\u63a7\u5236","title":"Fl\u706b\u7130\u56feame"},{"location":"documentation/flame/01-setup/","text":"Setup \u00b6 Flame is part of the Clinic.js suit of tools. To install Flame, simply install Clinic.js like so: 1 npm install -g clinic After installing, we can check if Flame has been installed with running the clinic flame command with the --help flag. 1 clinic flame --help It should print something similar to the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 Clinic.js Flame - v3.6.0 (0x v4.7.2) clinic flame helps you find synchronous bottlenecks by creating a flamegraph visualization that assists in identifying function calls that may be blocking the event loop. For more information see the 0x readme, https://github.com/davidmarkclements/0x To run clinic flame clinic flame -- node server.js If profiling on a server, it can be useful to only do data collection: clinic flame --collect-only -- node server.js You can then transfer the data and visualize it locally: clinic flame --visualize-only PID.clinic.flame Flags -h | --help Display Help -v | --version Display Version --collect-only Do not process data on termination --visualize-only datapath Build or rebuild visualization from data Up next \u00b6 Getting ready","title":"Setup"},{"location":"documentation/flame/01-setup/#setup","text":"Flame is part of the Clinic.js suit of tools. To install Flame, simply install Clinic.js like so: 1 npm install -g clinic After installing, we can check if Flame has been installed with running the clinic flame command with the --help flag. 1 clinic flame --help It should print something similar to the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 Clinic.js Flame - v3.6.0 (0x v4.7.2) clinic flame helps you find synchronous bottlenecks by creating a flamegraph visualization that assists in identifying function calls that may be blocking the event loop. For more information see the 0x readme, https://github.com/davidmarkclements/0x To run clinic flame clinic flame -- node server.js If profiling on a server, it can be useful to only do data collection: clinic flame --collect-only -- node server.js You can then transfer the data and visualize it locally: clinic flame --visualize-only PID.clinic.flame Flags -h | --help Display Help -v | --version Display Version --collect-only Do not process data on termination --visualize-only datapath Build or rebuild visualization from data","title":"Setup"},{"location":"documentation/flame/01-setup/#up-next","text":"Getting ready","title":"Up next"},{"location":"documentation/flame/02-getting-ready/","text":"Getting ready \u00b6 Once we've installed clinic and verified that clinic flame is functioning we can profile an application. To try this out, let's clone and prepare the official Clinic.js Flame demo: 1 2 3 git clone https://github.com/clinicjs/node-clinic-flame-demo.git cd node-clinic-flame-demo npm install While npm downloads the dependencies, check the Readme , then we are ready to profile! Up next \u00b6 First analysis","title":"Getting ready"},{"location":"documentation/flame/02-getting-ready/#getting-ready","text":"Once we've installed clinic and verified that clinic flame is functioning we can profile an application. To try this out, let's clone and prepare the official Clinic.js Flame demo: 1 2 3 git clone https://github.com/clinicjs/node-clinic-flame-demo.git cd node-clinic-flame-demo npm install While npm downloads the dependencies, check the Readme , then we are ready to profile!","title":"Getting ready"},{"location":"documentation/flame/02-getting-ready/#up-next","text":"First analysis","title":"Up next"},{"location":"documentation/flame/03-first-analysis/","text":"First analysis \u00b6 Now we're ready to profile the application. Let's try with the first server in the repo, 1-server-with-slow-function.js . It contains an HTTP server, built using Express with a root route ( / ) that performs some work before rendering a landing page. The server can be started with node 1-server-with-slow-function.js and then accessed in the browser at http://localhost:3000/ . If the landing page says \"Hello World\" then things are working! Let's try and profile the server with Flame to see if we can find any bottlenecks. To do that we need a tool that can simulate sufficiently intense HTTP load. We suggest autocannon which is supported on Windows, Mac and Linux and is straightforward to use. Let's install it from npm: 1 npm install -g autocannon To run the analysis we want to run the server with Flame and when the server is ready - i.e. starts listening on a port - we want to send a ton of requests to it using autocannon . All that can be performed with a single command, which can be copied and pasted as-is: 1 clinic flame --on-port 'autocannon localhost:$PORT' -- node 1 -server-with-slow-function.js Let's break this command down: The clinic flame portion invokes the flame command tool. The --on-port flag will execute the supplied script as soon as the server starts listening on a port. The $PORT variable in that script is set to the first port that the server began listening on. Everything after the double-dash ( -- ) is the command which starts the server that we want to profile, in this case node 1-server-with-slow-function.js . This one command runs three executables: the clinic flame parent executable, the autocannon executable in --on-port and the node executable. Upon running the command, the process will be load tested for 10 seconds (as per the autocannon default duration), then the results be compiled into a single HTML file that should automatically open in the browser. The resulting HTML should look similar to the following: This is known as a Flamegraph. Up next \u00b6 Flamegraphs","title":"First analysis"},{"location":"documentation/flame/03-first-analysis/#first-analysis","text":"Now we're ready to profile the application. Let's try with the first server in the repo, 1-server-with-slow-function.js . It contains an HTTP server, built using Express with a root route ( / ) that performs some work before rendering a landing page. The server can be started with node 1-server-with-slow-function.js and then accessed in the browser at http://localhost:3000/ . If the landing page says \"Hello World\" then things are working! Let's try and profile the server with Flame to see if we can find any bottlenecks. To do that we need a tool that can simulate sufficiently intense HTTP load. We suggest autocannon which is supported on Windows, Mac and Linux and is straightforward to use. Let's install it from npm: 1 npm install -g autocannon To run the analysis we want to run the server with Flame and when the server is ready - i.e. starts listening on a port - we want to send a ton of requests to it using autocannon . All that can be performed with a single command, which can be copied and pasted as-is: 1 clinic flame --on-port 'autocannon localhost:$PORT' -- node 1 -server-with-slow-function.js Let's break this command down: The clinic flame portion invokes the flame command tool. The --on-port flag will execute the supplied script as soon as the server starts listening on a port. The $PORT variable in that script is set to the first port that the server began listening on. Everything after the double-dash ( -- ) is the command which starts the server that we want to profile, in this case node 1-server-with-slow-function.js . This one command runs three executables: the clinic flame parent executable, the autocannon executable in --on-port and the node executable. Upon running the command, the process will be load tested for 10 seconds (as per the autocannon default duration), then the results be compiled into a single HTML file that should automatically open in the browser. The resulting HTML should look similar to the following: This is known as a Flamegraph.","title":"First analysis"},{"location":"documentation/flame/03-first-analysis/#up-next","text":"Flamegraphs","title":"Up next"},{"location":"documentation/flame/04-flamegraphs/","text":"Flamegraphs \u00b6 Let's take a look again at the flamegraph generated in the prior First analysis step. Ignoring the surrounding controls for the moment, let's focus in on understanding the visualization. When generating a flamegraph we are asking three key questions: During the sampling period, which functions called each other? How much time was each function observed on-CPU? How much time was each function observed at the top of the stack? These three questions are answered visually. Which functions called each other (the stack) \u00b6 Each block represents the invocations of one function, aggregated by the call stack that led to it. When one block sits on top of another, it was called by the block below it, which was called by the block below it, and so on down the stack. In Clinic.js Flame, the text and outline colours of each block help you navigate. White represents code in the application being profiled (i.e. code under your direct control). Blue represents your dependencies in node_modules , while grey represents code in Node.js core itself. How long, in aggregate, was a function on-CPU (block width) \u00b6 The width of a block represents the amount of time it was on the CPU, out of the duration of the profile run. This does not necessarily mean the function was executing its own code: where there are blocks above it, it had called that function and was waiting for it to complete. Clinic.js Flame sorts the functions called by each function so that the widest blocks (the functions spending longest on the CPU) are shown first, on the left. How often was a function at the top of stack (\"heat\") \u00b6 This can be rephrased as: \"For how long was a function blocking the Node.js event loop\". If a function is frequently observed at the top of the stack, it means it is spending more time executing its own code than calling other functions or allowing function callbacks to trigger. In Node.js, only one function can execute at any one time (ignoring possibilities like Worker threads). If a function takes a long time to execute, nothing else can happen, including the triggering of I/O callbacks. This is the essence of the phrase \"blocking the event loop\". The brightness of the bar along the exposed top of a block indicates the percentage of time a function was observed at the top of the stack. In other words, the hotter (or, brighter) a block, the more actual time it was taking to execute its own code, preventing any other code from executing. When a function is blocking the event loop in higher proportion to other functions we call this a \"hot\" function. Looking for these \"hot\" functions is a good place to start looking for places to optimise your code. Clinic.js Flame by default selects the \"hottest\" frame, and gives controls to cycle through the next hottest. Up next \u00b6 Controls","title":"Flamegraphs"},{"location":"documentation/flame/04-flamegraphs/#flamegraphs","text":"Let's take a look again at the flamegraph generated in the prior First analysis step. Ignoring the surrounding controls for the moment, let's focus in on understanding the visualization. When generating a flamegraph we are asking three key questions: During the sampling period, which functions called each other? How much time was each function observed on-CPU? How much time was each function observed at the top of the stack? These three questions are answered visually.","title":"Flamegraphs"},{"location":"documentation/flame/04-flamegraphs/#which-functions-called-each-other-the-stack","text":"Each block represents the invocations of one function, aggregated by the call stack that led to it. When one block sits on top of another, it was called by the block below it, which was called by the block below it, and so on down the stack. In Clinic.js Flame, the text and outline colours of each block help you navigate. White represents code in the application being profiled (i.e. code under your direct control). Blue represents your dependencies in node_modules , while grey represents code in Node.js core itself.","title":"Which functions called each other (the stack)"},{"location":"documentation/flame/04-flamegraphs/#how-long-in-aggregate-was-a-function-on-cpu-block-width","text":"The width of a block represents the amount of time it was on the CPU, out of the duration of the profile run. This does not necessarily mean the function was executing its own code: where there are blocks above it, it had called that function and was waiting for it to complete. Clinic.js Flame sorts the functions called by each function so that the widest blocks (the functions spending longest on the CPU) are shown first, on the left.","title":"How long, in aggregate, was a function on-CPU (block width)"},{"location":"documentation/flame/04-flamegraphs/#how-often-was-a-function-at-the-top-of-stack-heat","text":"This can be rephrased as: \"For how long was a function blocking the Node.js event loop\". If a function is frequently observed at the top of the stack, it means it is spending more time executing its own code than calling other functions or allowing function callbacks to trigger. In Node.js, only one function can execute at any one time (ignoring possibilities like Worker threads). If a function takes a long time to execute, nothing else can happen, including the triggering of I/O callbacks. This is the essence of the phrase \"blocking the event loop\". The brightness of the bar along the exposed top of a block indicates the percentage of time a function was observed at the top of the stack. In other words, the hotter (or, brighter) a block, the more actual time it was taking to execute its own code, preventing any other code from executing. When a function is blocking the event loop in higher proportion to other functions we call this a \"hot\" function. Looking for these \"hot\" functions is a good place to start looking for places to optimise your code. Clinic.js Flame by default selects the \"hottest\" frame, and gives controls to cycle through the next hottest.","title":"How often was a function at the top of stack (\"heat\")"},{"location":"documentation/flame/04-flamegraphs/#up-next","text":"Controls","title":"Up next"},{"location":"documentation/flame/05-controls/","text":"Controls \u00b6 The Clinic.js Flame UI controls have three main sections: Flamegraph controls : Interacting with the flamegraph itself Info Panel : Along the top, containing info about the block currently highlighted Options Menu : More advanced controls expandable from the info panel Flamegraph controls \u00b6 Interacting with a flamegraph block \u00b6 Hovering the mouse over a block will temporarily show info about that block in the info panel at the top of the screen. Clicking will select the block, so that the info panel reverts to that block when you are no longer hovering over anything. If you find an interesting-looking block, it can be useful to click on it so you can hover around exploring its neighbours without losing it. Tooltip buttons \u00b6 After single-clicking on a block, or hovering over it for a moment, a tooltip appears, with one or more buttons: Expand . See below for more about expanding a particular block. Contract . If this is the block that you have already expanded, it will show \"Contract\" instead of \"Expand\", which will take you back to the main view. Copy path . Copies to the clipboard the relative file path to the file containing the code this block represents. This only appears for blocks from the application being profiled (white) or its dependencies (blue). Open in browser . For Node.js core blocks only (grey). In a new browser tab, opens the GitHub page showing the source code for the Node.js function represented by this block. Exploring these can be a good way to learn more about what Node.js is doing \"under the hood\". Expanding \u00b6 When a block is double-clicked, or its tooltip \"Expand\" button is used, it will expand to fill the full width of the flamegraph. Blocks below will also expand and fill the full width, while blocks above the clicked block will increase in ratio to the block they sit on. Expanding a block essentially creates a new flamegraph which represents a particular partition of the main flamegraph. The block that has been expanded is marked with a shadow underneath. Every block below this 'shadow' is probably wider (longer on the CPU) than the block that has expanded to fill the screen. To get back to the main, non-expanded view, you can either click on the background, click \"Return to main view\" at the bottom of the screen, double-click on the expanded frame, or click on its \"Contract\" tooltip button. Info panel \u00b6 There are five main features in the Info Panel: Stack bar : A thin bar showing the \"hottest\" blocks in order Selection controls : Flick to the next hottest, previous, etc Code info : Where the function behind the currently highlighted block comes from Search box : For finding functions by name or path Options Menu : More advanced features. This Options Menu has its own section below Stack Bar \u00b6 We previously explained, in the Flamegraphs page, how it is useful to consider how much time a function was at the top of the stack, meaning the Node.js event loop was blocked as the CPU executes code within that function; and how this is represented by the brightness or \"heat\" of the colour of the exposed part of a block. This bar shows you the heat of those exposed stack tops, of every block in the flamegraph, in order of heat i.e. in order of how long that block's function was blocking the event loop. You can run the cursor along this bar from left to right to see where these \"hot\" functions are on the main flamegraph, with the same interaction as above: hover to see info, click to select and show tooltip, double click to expand. The left-most (hottest) block is selected by default when a Clinic.js Flame profile is first opened. Selection Controls \u00b6 These buttons allow you to easily jump from the currently selected block, to the block that is one to the left or right of it in the hottness-ranking shown by the Stack Bar. A good place to start with a Clinic.js Flame flamegraph, is to cycle through using the \"Next hottest\" button, and for each block it selects, think why that function might be spending so much time active. For example, it might be a slow function needing optimising, or it might be a function you know is fast, but when you look at what is below it in the flamegraph, you might discover that it is being called too many times (for example, it might be in a nested loop). Code info \u00b6 This gives you more complete information about the code behind the block that is currently highlighted. Function name (or equivalent) on the left. Anonymous functions are labelled (anonymous) . File path (or equivalent) in the middle, including line and column number (if applicable). Context. This tells you what category this block is (for example, dependency), and may include additional information if certain advanced controls are used. Search box \u00b6 If there is some particular file or function(s) you want to locate, you can type part of the function name, file path or equivalent here, and any matches will be highlighted, in the same colour used for text and outlines (white for code from the profiled application, blue from a dependencies, grey from Node.js itself). This can be useful if you've done such a good job optimizing an operation, you can no longer find it on the flamegraph! If a function you know exists can't be found anywhere , even using search, it's possible it might have been inlined by V8: try searching again after turning off \"Merge\" in the Options Menu . For more on merging and inlined blocks, see the section \"Merging and Unmerging\" in Advanced Controls If it is possible the function was so fast, or on the CPU for so little time, that it was never on the CPU while a sample was being taken, it might appear if you create a new profile with a longer duration and/or more connections in Autocannon . Options Menu \u00b6 Clicking \"Options\" on the right side of the Info Panel opens a menu with more advanced options. Visibility by code area \u00b6 These toggle buttons show (tick) or hide (untick) blocks based on where the code is in the application or Node.js framework. [Application name] : Code inside the main package being profiled. Visible by default. Dependencies : Code in a dependency in a node_modules directory. Visible by default. Node JS : Code inside Node.js core. Visible by default. V8 : Functions inside the V8 JavaScript engine. Hidden by default, recommended for advanced users only. More info Advanced \u00b6 Init : Allows initialization functions to be shown that Flame hides by default. More info Merge : Allows different stacks to be shown for functions that V8 has optimised. More info . Preferences \u00b6 Presentation mode : Increases text sizes and colour contrasts, which can be useful if Clinic.js Flame is being presented under suboptimal conditions (e.g. on a projector in a brightly lit room). Profiles can be set to show in Presentation Mode by default by setting the PRESENTATION_MODE environment variable to TRUE . Up next \u00b6 Optimizing a hot function","title":"Controls"},{"location":"documentation/flame/05-controls/#controls","text":"The Clinic.js Flame UI controls have three main sections: Flamegraph controls : Interacting with the flamegraph itself Info Panel : Along the top, containing info about the block currently highlighted Options Menu : More advanced controls expandable from the info panel","title":"Controls"},{"location":"documentation/flame/05-controls/#flamegraph-controls","text":"","title":"Flamegraph controls"},{"location":"documentation/flame/05-controls/#interacting-with-a-flamegraph-block","text":"Hovering the mouse over a block will temporarily show info about that block in the info panel at the top of the screen. Clicking will select the block, so that the info panel reverts to that block when you are no longer hovering over anything. If you find an interesting-looking block, it can be useful to click on it so you can hover around exploring its neighbours without losing it.","title":"Interacting with a flamegraph block"},{"location":"documentation/flame/05-controls/#tooltip-buttons","text":"After single-clicking on a block, or hovering over it for a moment, a tooltip appears, with one or more buttons: Expand . See below for more about expanding a particular block. Contract . If this is the block that you have already expanded, it will show \"Contract\" instead of \"Expand\", which will take you back to the main view. Copy path . Copies to the clipboard the relative file path to the file containing the code this block represents. This only appears for blocks from the application being profiled (white) or its dependencies (blue). Open in browser . For Node.js core blocks only (grey). In a new browser tab, opens the GitHub page showing the source code for the Node.js function represented by this block. Exploring these can be a good way to learn more about what Node.js is doing \"under the hood\".","title":"Tooltip buttons"},{"location":"documentation/flame/05-controls/#expanding","text":"When a block is double-clicked, or its tooltip \"Expand\" button is used, it will expand to fill the full width of the flamegraph. Blocks below will also expand and fill the full width, while blocks above the clicked block will increase in ratio to the block they sit on. Expanding a block essentially creates a new flamegraph which represents a particular partition of the main flamegraph. The block that has been expanded is marked with a shadow underneath. Every block below this 'shadow' is probably wider (longer on the CPU) than the block that has expanded to fill the screen. To get back to the main, non-expanded view, you can either click on the background, click \"Return to main view\" at the bottom of the screen, double-click on the expanded frame, or click on its \"Contract\" tooltip button.","title":"Expanding"},{"location":"documentation/flame/05-controls/#info-panel","text":"There are five main features in the Info Panel: Stack bar : A thin bar showing the \"hottest\" blocks in order Selection controls : Flick to the next hottest, previous, etc Code info : Where the function behind the currently highlighted block comes from Search box : For finding functions by name or path Options Menu : More advanced features. This Options Menu has its own section below","title":"Info panel"},{"location":"documentation/flame/05-controls/#stack-bar","text":"We previously explained, in the Flamegraphs page, how it is useful to consider how much time a function was at the top of the stack, meaning the Node.js event loop was blocked as the CPU executes code within that function; and how this is represented by the brightness or \"heat\" of the colour of the exposed part of a block. This bar shows you the heat of those exposed stack tops, of every block in the flamegraph, in order of heat i.e. in order of how long that block's function was blocking the event loop. You can run the cursor along this bar from left to right to see where these \"hot\" functions are on the main flamegraph, with the same interaction as above: hover to see info, click to select and show tooltip, double click to expand. The left-most (hottest) block is selected by default when a Clinic.js Flame profile is first opened.","title":"Stack Bar"},{"location":"documentation/flame/05-controls/#selection-controls","text":"These buttons allow you to easily jump from the currently selected block, to the block that is one to the left or right of it in the hottness-ranking shown by the Stack Bar. A good place to start with a Clinic.js Flame flamegraph, is to cycle through using the \"Next hottest\" button, and for each block it selects, think why that function might be spending so much time active. For example, it might be a slow function needing optimising, or it might be a function you know is fast, but when you look at what is below it in the flamegraph, you might discover that it is being called too many times (for example, it might be in a nested loop).","title":"Selection Controls"},{"location":"documentation/flame/05-controls/#code-info","text":"This gives you more complete information about the code behind the block that is currently highlighted. Function name (or equivalent) on the left. Anonymous functions are labelled (anonymous) . File path (or equivalent) in the middle, including line and column number (if applicable). Context. This tells you what category this block is (for example, dependency), and may include additional information if certain advanced controls are used.","title":"Code info"},{"location":"documentation/flame/05-controls/#search-box","text":"If there is some particular file or function(s) you want to locate, you can type part of the function name, file path or equivalent here, and any matches will be highlighted, in the same colour used for text and outlines (white for code from the profiled application, blue from a dependencies, grey from Node.js itself). This can be useful if you've done such a good job optimizing an operation, you can no longer find it on the flamegraph! If a function you know exists can't be found anywhere , even using search, it's possible it might have been inlined by V8: try searching again after turning off \"Merge\" in the Options Menu . For more on merging and inlined blocks, see the section \"Merging and Unmerging\" in Advanced Controls If it is possible the function was so fast, or on the CPU for so little time, that it was never on the CPU while a sample was being taken, it might appear if you create a new profile with a longer duration and/or more connections in Autocannon .","title":"Search box"},{"location":"documentation/flame/05-controls/#options-menu","text":"Clicking \"Options\" on the right side of the Info Panel opens a menu with more advanced options.","title":"Options Menu"},{"location":"documentation/flame/05-controls/#visibility-by-code-area","text":"These toggle buttons show (tick) or hide (untick) blocks based on where the code is in the application or Node.js framework. [Application name] : Code inside the main package being profiled. Visible by default. Dependencies : Code in a dependency in a node_modules directory. Visible by default. Node JS : Code inside Node.js core. Visible by default. V8 : Functions inside the V8 JavaScript engine. Hidden by default, recommended for advanced users only. More info","title":"Visibility by code area"},{"location":"documentation/flame/05-controls/#advanced","text":"Init : Allows initialization functions to be shown that Flame hides by default. More info Merge : Allows different stacks to be shown for functions that V8 has optimised. More info .","title":"Advanced"},{"location":"documentation/flame/05-controls/#preferences","text":"Presentation mode : Increases text sizes and colour contrasts, which can be useful if Clinic.js Flame is being presented under suboptimal conditions (e.g. on a projector in a brightly lit room). Profiles can be set to show in Presentation Mode by default by setting the PRESENTATION_MODE environment variable to TRUE .","title":"Preferences"},{"location":"documentation/flame/05-controls/#up-next","text":"Optimizing a hot function","title":"Up next"},{"location":"documentation/flame/06-optimizing-a-hot-function/","text":"Optimizing a hot function \u00b6 The Clinic.js Flame UI has pointed us towards the \"hottest\" functions. One clearly stands out: the payload function in 1-server-with-slow-function.js , line 15. We can also see that this hot payload function is called by app.get on line 8, column 14. Let's take a look at the function starting at line 8: 1 2 3 app . get ( '/' , ( req , res ) => { res . send ( payload ()) }) The function is actually a fat arrow function, which is anonymous. In the absence of a name, the Flame sampler names this function app.get because it's passed to app.get . The column number (14) makes it clear that the function in question is the fat arrow function. This is the route handler for / which is called every time a request is made. It's calling the payload function (our bottleneck) and then passing it to res.send . Let's take a look at line 16: 1 return function payload () {} Since there's a return statement there must be an outer function. Let's take a look at the whole thing, including the outer function which creates the payload function: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 function initPayload ( idSize = 20 ) { return function payload () { let chars = '' let n = idSize const date = Date . now () const radix = 36 n *= n * idSize while ( n -- ) { const num = date + n chars += num . toString ( radix ). toUpperCase () } const id = chars . slice ( - idSize ) return { date , id } } } So in the initPayload scope there is a parameter, idSize with a default value of 20 . The payload function is returning an object with a date field and an id field. The date is created with Date.now() so it doesn't look like we can optimize much there. Let's focus on how the id is created. The variable n is set to the idSize , but then later multiplied by itself times the idSize , which is essentially n\u00b3. The n variable is then decreased by one per each iteration of a while loop. The body of the while loop is adding the date timestamp and n together and then calling toString(36) (the radix constant is 36 ) which will convert the number to an alphanumeric string (base36). This base36 string is then upper cased and added to the chars string. Finally the a slice with a length corresponding to idSize is taken from the chars string to form the id . What an odd and over-engineered way to create an ID. Surely no one would really write code like that? (Narrator: \"They do.\") Let's improve the algorithm, but keep the same fundamental qualities of the id: It must be an alphanumeric string It must be configurable by idSize It will never be less than 6 characters long A more optimal version of the initPayload function looks like so: 1 2 3 4 5 6 7 8 9 10 11 12 function initPayload ( idSize = 20 ) { if ( idSize < 6 ) throw Error ( 'idSize must be greater than 5' ) const max = 2147483647 var count = 0 return function payload () { count = ( count + 1 ) % max const date = Date . now () const chars = count . toString ( 36 ). toUpperCase () const id = '0' . repeat ( idSize - chars . length ) + chars return { date , id } } } The max number is the largest 32bit integer (2\u00b3\u00b9 - 1) we use this to cycle count back round to 0 when it reaches 2\u00b3\u00b9. This isn't strictly necessary for speed in our case, but Node's JavaScript engine (V8) is optimized for the numbers in 32bit range (since most numbers in practice tend to be). Additionally, when converted to base36 is (2\u00b3\u00b9 - 1) 6 characters, which means we don't have use a minimum length offset to enforce idSize . Each time the payload function is called, count is increase by one. We turn count into a base36 string, and upper case it. Then we pad the beginning of the string with the necessary amount of zeros (which is valid base36) to create an id with a length corresponding to idSize . Testing the optimized function \u00b6 The optimal payload function is in 2-server-with-optimized-function.js . Let's profile this server with Flame to assess the result: 1 clinic flame --on-port 'autocannon localhost:$PORT' -- node 2 -server-with-optimized-function.js This should generate something like the following: This immediately looks healthier - it's no longer dominated by just one function, and as a result we have a range of shades of yellow and orange. Notice the payload function is nowhere to be seen. This is because the function was inlined into its parent: the anonymous lamda function labelled as app.get . There is more information on inlining and merging on the Advanced Controls page . Using autocannon we can show that our optimization has lead to our server being over 50 times faster (220 req/s vs 11640 req/s). Up next \u00b6 Reducing the graph size","title":"Optimizing a hot function"},{"location":"documentation/flame/06-optimizing-a-hot-function/#optimizing-a-hot-function","text":"The Clinic.js Flame UI has pointed us towards the \"hottest\" functions. One clearly stands out: the payload function in 1-server-with-slow-function.js , line 15. We can also see that this hot payload function is called by app.get on line 8, column 14. Let's take a look at the function starting at line 8: 1 2 3 app . get ( '/' , ( req , res ) => { res . send ( payload ()) }) The function is actually a fat arrow function, which is anonymous. In the absence of a name, the Flame sampler names this function app.get because it's passed to app.get . The column number (14) makes it clear that the function in question is the fat arrow function. This is the route handler for / which is called every time a request is made. It's calling the payload function (our bottleneck) and then passing it to res.send . Let's take a look at line 16: 1 return function payload () {} Since there's a return statement there must be an outer function. Let's take a look at the whole thing, including the outer function which creates the payload function: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 function initPayload ( idSize = 20 ) { return function payload () { let chars = '' let n = idSize const date = Date . now () const radix = 36 n *= n * idSize while ( n -- ) { const num = date + n chars += num . toString ( radix ). toUpperCase () } const id = chars . slice ( - idSize ) return { date , id } } } So in the initPayload scope there is a parameter, idSize with a default value of 20 . The payload function is returning an object with a date field and an id field. The date is created with Date.now() so it doesn't look like we can optimize much there. Let's focus on how the id is created. The variable n is set to the idSize , but then later multiplied by itself times the idSize , which is essentially n\u00b3. The n variable is then decreased by one per each iteration of a while loop. The body of the while loop is adding the date timestamp and n together and then calling toString(36) (the radix constant is 36 ) which will convert the number to an alphanumeric string (base36). This base36 string is then upper cased and added to the chars string. Finally the a slice with a length corresponding to idSize is taken from the chars string to form the id . What an odd and over-engineered way to create an ID. Surely no one would really write code like that? (Narrator: \"They do.\") Let's improve the algorithm, but keep the same fundamental qualities of the id: It must be an alphanumeric string It must be configurable by idSize It will never be less than 6 characters long A more optimal version of the initPayload function looks like so: 1 2 3 4 5 6 7 8 9 10 11 12 function initPayload ( idSize = 20 ) { if ( idSize < 6 ) throw Error ( 'idSize must be greater than 5' ) const max = 2147483647 var count = 0 return function payload () { count = ( count + 1 ) % max const date = Date . now () const chars = count . toString ( 36 ). toUpperCase () const id = '0' . repeat ( idSize - chars . length ) + chars return { date , id } } } The max number is the largest 32bit integer (2\u00b3\u00b9 - 1) we use this to cycle count back round to 0 when it reaches 2\u00b3\u00b9. This isn't strictly necessary for speed in our case, but Node's JavaScript engine (V8) is optimized for the numbers in 32bit range (since most numbers in practice tend to be). Additionally, when converted to base36 is (2\u00b3\u00b9 - 1) 6 characters, which means we don't have use a minimum length offset to enforce idSize . Each time the payload function is called, count is increase by one. We turn count into a base36 string, and upper case it. Then we pad the beginning of the string with the necessary amount of zeros (which is valid base36) to create an id with a length corresponding to idSize .","title":"Optimizing a hot function"},{"location":"documentation/flame/06-optimizing-a-hot-function/#testing-the-optimized-function","text":"The optimal payload function is in 2-server-with-optimized-function.js . Let's profile this server with Flame to assess the result: 1 clinic flame --on-port 'autocannon localhost:$PORT' -- node 2 -server-with-optimized-function.js This should generate something like the following: This immediately looks healthier - it's no longer dominated by just one function, and as a result we have a range of shades of yellow and orange. Notice the payload function is nowhere to be seen. This is because the function was inlined into its parent: the anonymous lamda function labelled as app.get . There is more information on inlining and merging on the Advanced Controls page . Using autocannon we can show that our optimization has lead to our server being over 50 times faster (220 req/s vs 11640 req/s).","title":"Testing the optimized function"},{"location":"documentation/flame/06-optimizing-a-hot-function/#up-next","text":"Reducing the graph size","title":"Up next"},{"location":"documentation/flame/07-reducing-the-graph-size/","text":"Reducing the graph size \u00b6 While Flamegraph's highlight bottlenecks primarily through visualizing the top-of-stack metric, they can also be used to understand application complexity. If a flamegraph has a lot of high rising stacks, this can also be thought of as a distributed bottleneck. If we can find ways to reduce the graph size, perhaps by removing unnecessary layers, this can also improve application performance. One potentially low hanging fruit can be replacing libraries with simpler or smarter alternatives that create and call less functions in the hottest paths. Let's take a look at the flamegraph we already generated for 2-server-with-optimized-function.js : In 3-server-with-reduced-call-graph.js we change the web framework from Express to Fastify . Let's generate a flamegraph for 3-server-with-reduced-call-graph.js : 1 clinic flame --on-port 'autocannon localhost:$PORT' -- node 3 -server-with-reduced-call-graph.js Let's take a look at the resulting flamegraph: This is clearly simpler, and there's much less blue meaning less activity in dependencies. There's a tall blue column on the right, but it's very thin: therefore there is complexity there, but it is fast. There are, however, some hot frames under handleRequest , so there may still be more we can do. First, however, we must confirm that performance really has improved. Reducing function calls and complexity doesn't always result in a faster application. We can measure the difference between 2-server-with-optimized-function.js and 3-server-with-reduced-call-graph.js using autocannon : That's a huge improvement. By swapping to a framework that focuses on reducing the function graph complexity, performance has significantly improved. In fact, we can now serve close to double the amount of requests. Up next \u00b6 Advanced analysis","title":"Reducing the graph size"},{"location":"documentation/flame/07-reducing-the-graph-size/#reducing-the-graph-size","text":"While Flamegraph's highlight bottlenecks primarily through visualizing the top-of-stack metric, they can also be used to understand application complexity. If a flamegraph has a lot of high rising stacks, this can also be thought of as a distributed bottleneck. If we can find ways to reduce the graph size, perhaps by removing unnecessary layers, this can also improve application performance. One potentially low hanging fruit can be replacing libraries with simpler or smarter alternatives that create and call less functions in the hottest paths. Let's take a look at the flamegraph we already generated for 2-server-with-optimized-function.js : In 3-server-with-reduced-call-graph.js we change the web framework from Express to Fastify . Let's generate a flamegraph for 3-server-with-reduced-call-graph.js : 1 clinic flame --on-port 'autocannon localhost:$PORT' -- node 3 -server-with-reduced-call-graph.js Let's take a look at the resulting flamegraph: This is clearly simpler, and there's much less blue meaning less activity in dependencies. There's a tall blue column on the right, but it's very thin: therefore there is complexity there, but it is fast. There are, however, some hot frames under handleRequest , so there may still be more we can do. First, however, we must confirm that performance really has improved. Reducing function calls and complexity doesn't always result in a faster application. We can measure the difference between 2-server-with-optimized-function.js and 3-server-with-reduced-call-graph.js using autocannon : That's a huge improvement. By swapping to a framework that focuses on reducing the function graph complexity, performance has significantly improved. In fact, we can now serve close to double the amount of requests.","title":"Reducing the graph size"},{"location":"documentation/flame/07-reducing-the-graph-size/#up-next","text":"Advanced analysis","title":"Up next"},{"location":"documentation/flame/08-advanced-analysis/","text":"Advanced analysis \u00b6 We still have some hot blocks in our latest flamegraph: None of the hottest blocks refer to any of our code, but let's take a minute to think through what's happening. The hottest is clearBuffer in _stream_writable.js . This is part of Node core. Since the goal here isn't to begin optimizing Node core, let's look at the next hottest block: handleRequest . We can expand the handleRequest block to look something like the following: Looking at the three hottest functions in this sub-view: Node.js's clearBuffer is first and accounts for most of the time spent inside onSendEnd . As discussed, optimizing that will probably be difficult. Second hottest is the Fastify function handleRequest , which has many children so is clearly quite complex. Third is in our own payload function - but we've already optimized that. So, why are we spending a long time inside handleRequest ? If we click copy path to look at the Fastify code, there's nothing obviously wrong, and we know Fastify is quite well optimized for performance. Maybe something is missing? Let's open the Options menu and tick the unticked \"V8\" button, showing operations inside the V8 JavaScript engine that are normally hidden: The gap almost completely disappears, and a new block appears (selected in the above screenshot), starting T v8::internal::Builtin_JsonStringify . This means it refers to a C++ function inside V8, named Builtin_JsonStringify . Clearly, this is related to JSON.stringify() . It's worth knowing that the JavaScript wrappers JSON.stringify() and JSON.parse() that we are familiar with are not sampled directly by V8, which instead skips straight to the underlying C++ implementation. Expand that, and we see V8 needs to do many, many steps when trying to stringify some JSON. This is a case where instead of focussing on one \"hot\" function, we need to focus on an inefficient parent function that is calling many micro-tasks. Each one looks pretty fast - the problem is, they add up to a lot of time. Why is JSON stringification happening here? It's because we send an object, and both Express and Fastify will automatically serialize an object passed to their send method. This bottleneck becomes a lot clearer when we turn off inlining. Let's run the following command: 1 clinic flame --on-port 'autocannon localhost:$PORT' -- node --no-turbo-inlining 3 -server-with-reduced-call-graph.js This will produce a flamegraph similar to the following: We have a new second-hottest function (after the Node core one) - serialize . This was previously hidden due to being inlined by V8. With none of the functions inlining, it becomes a lot more apparent that the serialize function is a bottleneck. Whereas with inlining the hot blocks were seen more on the top of the stack because they represent several other functions that have also been inlined into them. The 4-server-with-manual-serialization.js alters the line 23 in the payload function from return {date, id} to: 1 return `{\"date\": ${ date } , \"id\": \" ${ id } \"}` It should be noted here that this technique may be inappropriate in many cases, for instance where escaping inputs is crucial to security. An alternative to manual serialization which is still faster than using JSON.stringify is schema-based serialization using fast-json-stringify . The Fastify web framework also supports schema-based serialization by default, see Fastify's Serialization Documentation . Let's run Clinic.js Flame to create a flamegraph for 4-server-with-manual-serialization.js : 1 clinic flame --on-port 'autocannon localhost:$PORT' -- node 4 -server-with-manual-serialization.js This should give something like: We can see the the hottest block is a Node core function, Socket._writeGeneric , which is called by clearBuffer . It's same Node core bottleneck as before, it's just that in this sampling period the V8 engine didn't inline Socket._writeGeneric into clearBuffer . Let's use autocannon to determine the effect this has had on server performance: We've achieved roughly another 10% improvement. At this point further optimization of the application becomes increasingly challenging, since functions in Node core have become the primary bottleneck. A few more percent could be squeezed out here and there, especially if we were willing to change the constraints of the id field. However, for the most part, our work here is done. Up next \u00b6 The walkthrough is complete. Congratulations! You should now be able to use Clinic.js Flame to solve common performance problems. You may also choose to continue to read about Flame's more advanced controls .","title":"Advanced analysis"},{"location":"documentation/flame/08-advanced-analysis/#advanced-analysis","text":"We still have some hot blocks in our latest flamegraph: None of the hottest blocks refer to any of our code, but let's take a minute to think through what's happening. The hottest is clearBuffer in _stream_writable.js . This is part of Node core. Since the goal here isn't to begin optimizing Node core, let's look at the next hottest block: handleRequest . We can expand the handleRequest block to look something like the following: Looking at the three hottest functions in this sub-view: Node.js's clearBuffer is first and accounts for most of the time spent inside onSendEnd . As discussed, optimizing that will probably be difficult. Second hottest is the Fastify function handleRequest , which has many children so is clearly quite complex. Third is in our own payload function - but we've already optimized that. So, why are we spending a long time inside handleRequest ? If we click copy path to look at the Fastify code, there's nothing obviously wrong, and we know Fastify is quite well optimized for performance. Maybe something is missing? Let's open the Options menu and tick the unticked \"V8\" button, showing operations inside the V8 JavaScript engine that are normally hidden: The gap almost completely disappears, and a new block appears (selected in the above screenshot), starting T v8::internal::Builtin_JsonStringify . This means it refers to a C++ function inside V8, named Builtin_JsonStringify . Clearly, this is related to JSON.stringify() . It's worth knowing that the JavaScript wrappers JSON.stringify() and JSON.parse() that we are familiar with are not sampled directly by V8, which instead skips straight to the underlying C++ implementation. Expand that, and we see V8 needs to do many, many steps when trying to stringify some JSON. This is a case where instead of focussing on one \"hot\" function, we need to focus on an inefficient parent function that is calling many micro-tasks. Each one looks pretty fast - the problem is, they add up to a lot of time. Why is JSON stringification happening here? It's because we send an object, and both Express and Fastify will automatically serialize an object passed to their send method. This bottleneck becomes a lot clearer when we turn off inlining. Let's run the following command: 1 clinic flame --on-port 'autocannon localhost:$PORT' -- node --no-turbo-inlining 3 -server-with-reduced-call-graph.js This will produce a flamegraph similar to the following: We have a new second-hottest function (after the Node core one) - serialize . This was previously hidden due to being inlined by V8. With none of the functions inlining, it becomes a lot more apparent that the serialize function is a bottleneck. Whereas with inlining the hot blocks were seen more on the top of the stack because they represent several other functions that have also been inlined into them. The 4-server-with-manual-serialization.js alters the line 23 in the payload function from return {date, id} to: 1 return `{\"date\": ${ date } , \"id\": \" ${ id } \"}` It should be noted here that this technique may be inappropriate in many cases, for instance where escaping inputs is crucial to security. An alternative to manual serialization which is still faster than using JSON.stringify is schema-based serialization using fast-json-stringify . The Fastify web framework also supports schema-based serialization by default, see Fastify's Serialization Documentation . Let's run Clinic.js Flame to create a flamegraph for 4-server-with-manual-serialization.js : 1 clinic flame --on-port 'autocannon localhost:$PORT' -- node 4 -server-with-manual-serialization.js This should give something like: We can see the the hottest block is a Node core function, Socket._writeGeneric , which is called by clearBuffer . It's same Node core bottleneck as before, it's just that in this sampling period the V8 engine didn't inline Socket._writeGeneric into clearBuffer . Let's use autocannon to determine the effect this has had on server performance: We've achieved roughly another 10% improvement. At this point further optimization of the application becomes increasingly challenging, since functions in Node core have become the primary bottleneck. A few more percent could be squeezed out here and there, especially if we were willing to change the constraints of the id field. However, for the most part, our work here is done.","title":"Advanced analysis"},{"location":"documentation/flame/08-advanced-analysis/#up-next","text":"The walkthrough is complete. Congratulations! You should now be able to use Clinic.js Flame to solve common performance problems. You may also choose to continue to read about Flame's more advanced controls .","title":"Up next"},{"location":"documentation/flame/09-advanced-controls/","text":"Advanced controls \u00b6 This section provides further reference information on some of Clinic.js Flame's advanced features. Init \u00b6 Clinic.js Flame by default hides various initializaton functions. Filtering out these frames reduces generally redundant initialization noise. They include: Internal module system functions which are repeated frequently as the dependency tree is loaded (such as require() ) Functions relating to Clinic.js Flame's own data capture Other initialization functions An option in the \"Advanced\" section of the Options Menu allows these to be shown. When shown, Init blocks are presented like all other blocks, and will be shown or hidden according to the options selected in \"Visibility by code area\". The only differences is that when shown in the Info Panel , the context section on the right states \"In initialization process\" . Merging and unmerging \u00b6 V8 may apply automatic optimizations to some frequently-run code, creating optimized versions of those functions. At an internal JavaScript engine level, optimized and unoptimized functions are separate entities. Optimized and unoptimized code \u00b6 By default, Clinic.js Flame merges all Optimized and Unoptimized functions, and represents them as single blocks. It also merges all inlinable functions in to the calling functions that they are later inlined into. This creates a simplified graph where stacks only diverge based code logic. Unticking \"Merge\" in the \"Advanced\" section of the Options Menu separates Optimized and Unoptimized functions, showing them as seperate blocks and seperate stacks. This is the unmerged view of the flamegraph we created for Optimizing A Hot Function . Note how app.get (among others) forks into two stacks. One is the original unoptimized function, the other is the optimized version. The Info Panel shows the highlighted block's optimization status, in the context section on the right. For all JavaScript blocks, this will say either \"Unoptimized\" or \"Optimized\" . If the block is inlined by V8, it will also say \"Inlinable\" . Show optimization status \u00b6 With \"Merge\" unticked, there is another way to see which blocks are Optimized or Unoptimized. Another option, \"Show optimization status\", becomes available when \"Merge\" is unticked. If this is ticked, the text and outline colours of blocks are changed, along with the key at the bottom right of the flamegraph, to show: Unoptimized blocks in white Optimized blocks in grey Blocks where optimization is not relevant because they don't represent JavaScript in blue Inlinable functions can be found by typing \"inlinable\" into the search box while \"Merge\" is unticked. For example, in the above flamegraph (the one we generated while Optimizing A Hot Function ), we can see more easily that app.get forks into an optimized and unoptimized branch, and in the optimized branch, payload (selected) is flagged as \"Inlinable\". This is why, when we looked at that flamegraph in the default merged view, it was absent. It was inlined into its parent function, the optimized version of app.get . V8 \u00b6 In addition to showing functions from the Node.js framework, Clinic.js Flame can be set to show functions from within the V8 JavaScript engine, by ticking the \"V8\" checkbox in the \"Visibility by code area\" section of the Options Menu . This often adds a significant amount of complexity to the flamegraph, much of which may not be wanted. Flame therefore allows users to expand the V8 options and filter specific types of V8 function: V8 Native \u00b6 These are native JavaScript functions that are compiled into V8. This would include any native prototype methods ( Array.prototype.join for instance), and any functions that aren't publicly exposed but are used internally by V8 ( InnerArrayJoin for instance). In addition, evaluated functions (either code run with eval or created with Function ) will also appear as native frames, with the file path shown as [eval] . V8 Runtime \u00b6 These are C++ frames pertaining to the runtime operations of V8's implementation of JavaScript. Examples include (depending on V8 version) StringEqual and ObjectSetPrototypeOf . Tags can include [CODE:LoadGlobalIC] , [CODE:Handler] , [CODE:CallIC] , [CODE:LoadIC] , [CODE:StoreIC] , [CODE:Builtin] , [CODE:BytecodeHandler] , [CODE:Builtin] , [CODE:Stub] . V8 C++ \u00b6 These are C++ frames that are called by the V8 layer, not including C++ frames that may be called in Node, Libuv or third party modules. These frames can include the tags [CPP] and [SHARED_LIB] . RegExp \u00b6 RegExp stands for Regular Expressions. These are also captured as \"frames\". In this case the regular expression notation fills in as the \"function name\" portion of the block label. This can be useful in identifying slow regular expressions (in particular exponential time regular expressions ). These will have the tag [CODE:RegExp] .","title":"Advanced controls"},{"location":"documentation/flame/09-advanced-controls/#advanced-controls","text":"This section provides further reference information on some of Clinic.js Flame's advanced features.","title":"Advanced controls"},{"location":"documentation/flame/09-advanced-controls/#init","text":"Clinic.js Flame by default hides various initializaton functions. Filtering out these frames reduces generally redundant initialization noise. They include: Internal module system functions which are repeated frequently as the dependency tree is loaded (such as require() ) Functions relating to Clinic.js Flame's own data capture Other initialization functions An option in the \"Advanced\" section of the Options Menu allows these to be shown. When shown, Init blocks are presented like all other blocks, and will be shown or hidden according to the options selected in \"Visibility by code area\". The only differences is that when shown in the Info Panel , the context section on the right states \"In initialization process\" .","title":"Init"},{"location":"documentation/flame/09-advanced-controls/#merging-and-unmerging","text":"V8 may apply automatic optimizations to some frequently-run code, creating optimized versions of those functions. At an internal JavaScript engine level, optimized and unoptimized functions are separate entities.","title":"Merging and unmerging"},{"location":"documentation/flame/09-advanced-controls/#optimized-and-unoptimized-code","text":"By default, Clinic.js Flame merges all Optimized and Unoptimized functions, and represents them as single blocks. It also merges all inlinable functions in to the calling functions that they are later inlined into. This creates a simplified graph where stacks only diverge based code logic. Unticking \"Merge\" in the \"Advanced\" section of the Options Menu separates Optimized and Unoptimized functions, showing them as seperate blocks and seperate stacks. This is the unmerged view of the flamegraph we created for Optimizing A Hot Function . Note how app.get (among others) forks into two stacks. One is the original unoptimized function, the other is the optimized version. The Info Panel shows the highlighted block's optimization status, in the context section on the right. For all JavaScript blocks, this will say either \"Unoptimized\" or \"Optimized\" . If the block is inlined by V8, it will also say \"Inlinable\" .","title":"Optimized and unoptimized code"},{"location":"documentation/flame/09-advanced-controls/#show-optimization-status","text":"With \"Merge\" unticked, there is another way to see which blocks are Optimized or Unoptimized. Another option, \"Show optimization status\", becomes available when \"Merge\" is unticked. If this is ticked, the text and outline colours of blocks are changed, along with the key at the bottom right of the flamegraph, to show: Unoptimized blocks in white Optimized blocks in grey Blocks where optimization is not relevant because they don't represent JavaScript in blue Inlinable functions can be found by typing \"inlinable\" into the search box while \"Merge\" is unticked. For example, in the above flamegraph (the one we generated while Optimizing A Hot Function ), we can see more easily that app.get forks into an optimized and unoptimized branch, and in the optimized branch, payload (selected) is flagged as \"Inlinable\". This is why, when we looked at that flamegraph in the default merged view, it was absent. It was inlined into its parent function, the optimized version of app.get .","title":"Show optimization status"},{"location":"documentation/flame/09-advanced-controls/#v8","text":"In addition to showing functions from the Node.js framework, Clinic.js Flame can be set to show functions from within the V8 JavaScript engine, by ticking the \"V8\" checkbox in the \"Visibility by code area\" section of the Options Menu . This often adds a significant amount of complexity to the flamegraph, much of which may not be wanted. Flame therefore allows users to expand the V8 options and filter specific types of V8 function:","title":"V8"},{"location":"documentation/flame/09-advanced-controls/#v8-native","text":"These are native JavaScript functions that are compiled into V8. This would include any native prototype methods ( Array.prototype.join for instance), and any functions that aren't publicly exposed but are used internally by V8 ( InnerArrayJoin for instance). In addition, evaluated functions (either code run with eval or created with Function ) will also appear as native frames, with the file path shown as [eval] .","title":"V8 Native"},{"location":"documentation/flame/09-advanced-controls/#v8-runtime","text":"These are C++ frames pertaining to the runtime operations of V8's implementation of JavaScript. Examples include (depending on V8 version) StringEqual and ObjectSetPrototypeOf . Tags can include [CODE:LoadGlobalIC] , [CODE:Handler] , [CODE:CallIC] , [CODE:LoadIC] , [CODE:StoreIC] , [CODE:Builtin] , [CODE:BytecodeHandler] , [CODE:Builtin] , [CODE:Stub] .","title":"V8 Runtime"},{"location":"documentation/flame/09-advanced-controls/#v8-c","text":"These are C++ frames that are called by the V8 layer, not including C++ frames that may be called in Node, Libuv or third party modules. These frames can include the tags [CPP] and [SHARED_LIB] .","title":"V8 C++"},{"location":"documentation/flame/09-advanced-controls/#regexp","text":"RegExp stands for Regular Expressions. These are also captured as \"frames\". In this case the regular expression notation fills in as the \"function name\" portion of the block label. This can be useful in identifying slow regular expressions (in particular exponential time regular expressions ). These will have the tag [CODE:RegExp] .","title":"RegExp"},{"location":"documentation/heapprofiler/","text":"\u5806\u5206\u6790\u5668 \u00b6 \u5982\u679c\u4f60\u6b63\u5728\u5bfb\u627e\u5feb\u901f\u5f00\u59cb\u7406\u89e3\u5185\u5b58\u5206\u6790\u548c\u5b66\u4e60\u5982\u4f55\u4f18\u5316Node.js\u4ee3\u7801\uff0c\u90a3\u4e48\u8fd9\u662f\u4e00\u4e2a\u5f88\u597d\u7684\u5f00\u59cb\u3002 \u914d\u7f6e \u51c6\u5907 \u9996\u5148\u5206\u6790 \u706b\u7130\u56fe \u63a7\u5236\u5668 \u4f18\u5316\u70ed\u51fd\u6570","title":"\u5806\u5206\u6790\u5668"},{"location":"documentation/heapprofiler/#_1","text":"\u5982\u679c\u4f60\u6b63\u5728\u5bfb\u627e\u5feb\u901f\u5f00\u59cb\u7406\u89e3\u5185\u5b58\u5206\u6790\u548c\u5b66\u4e60\u5982\u4f55\u4f18\u5316Node.js\u4ee3\u7801\uff0c\u90a3\u4e48\u8fd9\u662f\u4e00\u4e2a\u5f88\u597d\u7684\u5f00\u59cb\u3002 \u914d\u7f6e \u51c6\u5907 \u9996\u5148\u5206\u6790 \u706b\u7130\u56fe \u63a7\u5236\u5668 \u4f18\u5316\u70ed\u51fd\u6570","title":"\u5806\u5206\u6790\u5668"},{"location":"documentation/heapprofiler/01-setup/","text":"Setup \u00b6 Heap Profiler is part of the Clinic.js suit of tools. To install Heap Profiler, simply install Clinic.js like so: 1 npm install -g clinic After installing, we can check if HeapProfiler has been installed with running the clinic heapprofiler command with the --help flag. 1 clinic heapprofiler --help It should print something similar to the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 Clinic.js Heap Profiler - v3.0.0 clinic heapprofiler helps you find memory leaks by creating a flamegraph visualization that assists in identifying function calls that may be leaking memory. To run clinic heapprofiler clinic heapprofiler -- node server.js Once you exit (Ctrl-C) the process, your report will open in a browser window. You can disable this behavior: clinic heapprofiler --open=false -- node server.js If profiling on a server, it can be useful to only do data collection: clinic heapprofiler --collect-only -- node server.js You can then transfer the data and visualize it locally: clinic heapprofiler --visualize-only PID.clinic.heapprofile You can use the --autocannon flag to simulate load on your server. --autocannon accepts configuration for autocannon using \"subarg\" syntax: clinic heapprofiler --autocannon [ -m POST /api/example ] -- node server.js When configuring --autocannon, the $PORT environment variable contains the port your server is listening on: clinic heapprofiler --autocannon [ -m POST 'http://localhost:$PORT/?\\$page=1' ] -- node server.js Note that dollar signs ($) appearing in the URL must be escaped, else they will be treated as environment variables as well. Flags -h | --help Display Help -v | --version Display Version --collect-only Do not process data on termination --visualize-only datapath Build or rebuild visualization from data --on-port Run a script when the server starts listening on a port. --autocannon Run the autocannon benchmarking tool when the server starts listening on a port. --open Boolean to enable or disable your report opening in your web browser. --dest Destination for the collected data (default .clinic/). Up next \u00b6 Getting ready","title":"Setup"},{"location":"documentation/heapprofiler/01-setup/#setup","text":"Heap Profiler is part of the Clinic.js suit of tools. To install Heap Profiler, simply install Clinic.js like so: 1 npm install -g clinic After installing, we can check if HeapProfiler has been installed with running the clinic heapprofiler command with the --help flag. 1 clinic heapprofiler --help It should print something similar to the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 Clinic.js Heap Profiler - v3.0.0 clinic heapprofiler helps you find memory leaks by creating a flamegraph visualization that assists in identifying function calls that may be leaking memory. To run clinic heapprofiler clinic heapprofiler -- node server.js Once you exit (Ctrl-C) the process, your report will open in a browser window. You can disable this behavior: clinic heapprofiler --open=false -- node server.js If profiling on a server, it can be useful to only do data collection: clinic heapprofiler --collect-only -- node server.js You can then transfer the data and visualize it locally: clinic heapprofiler --visualize-only PID.clinic.heapprofile You can use the --autocannon flag to simulate load on your server. --autocannon accepts configuration for autocannon using \"subarg\" syntax: clinic heapprofiler --autocannon [ -m POST /api/example ] -- node server.js When configuring --autocannon, the $PORT environment variable contains the port your server is listening on: clinic heapprofiler --autocannon [ -m POST 'http://localhost:$PORT/?\\$page=1' ] -- node server.js Note that dollar signs ($) appearing in the URL must be escaped, else they will be treated as environment variables as well. Flags -h | --help Display Help -v | --version Display Version --collect-only Do not process data on termination --visualize-only datapath Build or rebuild visualization from data --on-port Run a script when the server starts listening on a port. --autocannon Run the autocannon benchmarking tool when the server starts listening on a port. --open Boolean to enable or disable your report opening in your web browser. --dest Destination for the collected data (default .clinic/).","title":"Setup"},{"location":"documentation/heapprofiler/01-setup/#up-next","text":"Getting ready","title":"Up next"},{"location":"documentation/heapprofiler/02-getting-ready/","text":"Getting ready \u00b6 Once we've installed clinic and verified that clinic heapprofiler is functioning we can profile an application. To try this out, let's clone and prepare the official Clinic.js Heap Profiler demo: 1 2 3 git clone https://github.com/clinicjs/node-clinic-heap-profiler-demo.git cd node-clinic-heap-profiler-demo npm install While npm downloads the dependencies, check the Readme , then we are ready to profile! Up next \u00b6 First analysis","title":"Getting ready"},{"location":"documentation/heapprofiler/02-getting-ready/#getting-ready","text":"Once we've installed clinic and verified that clinic heapprofiler is functioning we can profile an application. To try this out, let's clone and prepare the official Clinic.js Heap Profiler demo: 1 2 3 git clone https://github.com/clinicjs/node-clinic-heap-profiler-demo.git cd node-clinic-heap-profiler-demo npm install While npm downloads the dependencies, check the Readme , then we are ready to profile!","title":"Getting ready"},{"location":"documentation/heapprofiler/02-getting-ready/#up-next","text":"First analysis","title":"Up next"},{"location":"documentation/heapprofiler/03-first-analysis/","text":"First analysis \u00b6 Now we're ready to profile the application. Let's try with the first server in the repository, 1-server-with-many-allocations.js . It contains an HTTP server, built using Fastify with a root route ( / ) that adds an integer to a global array in each request received. The server can be started with node 1-server-with-many-allocations.js and then accessed in the browser at http://localhost:3000/ . If the landing page says \"Hello World\" then things are working! Let's try and profile the server with Flame to see if we can find any bottlenecks. To do that we need a tool that can simulate sufficiently intense HTTP load. We suggest autocannon which is supported on Windows, Mac and Linux and is straightforward to use. Let's install it from npm: 1 npm install -g autocannon To run the analysis we want to run the server with HeapProfiler and when the server is ready, we want to send a ton of requests to it using autocannon . All that can be performed with a single command, which can be copied and pasted as-is: 1 clinic heapprofiler --autocannon [ / --method GET -d 120 ] -- node 1 -server-with-many-allocations.js Let's break this command down: The clinic heapprofiler portion invokes the heapprofiler command tool. The --autocannon flag will execute the tool as soon as the server starts listening on a port sending tons of requests. Everything after the double-dash ( -- ) is the command which starts the server that we want to profile, in this case node 1-server-with-many-allocations.js . This one command runs three executables: the clinic heapprofiler parent executable, the autocannon executable, and the node executable. Upon running the command, the process will be load tested for 120 seconds (as per the -d 120 parameter), then the results be compiled into a single HTML file that should automatically open in the browser. The resulting HTML should look similar to the following: This is known as a Flamegraph. Up next \u00b6 Flamegraphs","title":"First analysis"},{"location":"documentation/heapprofiler/03-first-analysis/#first-analysis","text":"Now we're ready to profile the application. Let's try with the first server in the repository, 1-server-with-many-allocations.js . It contains an HTTP server, built using Fastify with a root route ( / ) that adds an integer to a global array in each request received. The server can be started with node 1-server-with-many-allocations.js and then accessed in the browser at http://localhost:3000/ . If the landing page says \"Hello World\" then things are working! Let's try and profile the server with Flame to see if we can find any bottlenecks. To do that we need a tool that can simulate sufficiently intense HTTP load. We suggest autocannon which is supported on Windows, Mac and Linux and is straightforward to use. Let's install it from npm: 1 npm install -g autocannon To run the analysis we want to run the server with HeapProfiler and when the server is ready, we want to send a ton of requests to it using autocannon . All that can be performed with a single command, which can be copied and pasted as-is: 1 clinic heapprofiler --autocannon [ / --method GET -d 120 ] -- node 1 -server-with-many-allocations.js Let's break this command down: The clinic heapprofiler portion invokes the heapprofiler command tool. The --autocannon flag will execute the tool as soon as the server starts listening on a port sending tons of requests. Everything after the double-dash ( -- ) is the command which starts the server that we want to profile, in this case node 1-server-with-many-allocations.js . This one command runs three executables: the clinic heapprofiler parent executable, the autocannon executable, and the node executable. Upon running the command, the process will be load tested for 120 seconds (as per the -d 120 parameter), then the results be compiled into a single HTML file that should automatically open in the browser. The resulting HTML should look similar to the following: This is known as a Flamegraph.","title":"First analysis"},{"location":"documentation/heapprofiler/03-first-analysis/#up-next","text":"Flamegraphs","title":"Up next"},{"location":"documentation/heapprofiler/04-flamegraphs/","text":"Flamegraphs \u00b6 Let's take a look again at the flamegraph generated in the prior First analysis step. Ignoring the surrounding controls for the moment, let's focus in on understanding the visualization. When generating a flamegraph we are asking three key questions: During the sampling period, which functions called each other? How much memory was allocated by each function observed? What are the functions that allocate more memory on the heap? These three questions are answered visually. Which functions called each other (the stack) \u00b6 Each block represents the invocations of one function, aggregated by the call stack that led to it. When one block sits on top of another, it was called by the block below it, which was called by the block below it, and so on down the stack. How much memory was allocated by each function observed? \u00b6 The width of a block represents the amount of memory that the function allocated. In the box at top-right contain the allocation info of the selected function. What are the functions that allocate more memory on the heap? \u00b6 The \"hottest\" function in the profile is selected by default. Here, we can see the function name and file location (or equivalent), so we can inspect the underlying code and decide if this function is something we can and should optimize. The example above shows an Node.js function as the most hottest, you can click in \"Next biggest\" until reach an application function. In the next chapter, we will see how to manage the UI to visualize only the meaningful data to the provided demo. Up next \u00b6 Controls","title":"Flamegraphs"},{"location":"documentation/heapprofiler/04-flamegraphs/#flamegraphs","text":"Let's take a look again at the flamegraph generated in the prior First analysis step. Ignoring the surrounding controls for the moment, let's focus in on understanding the visualization. When generating a flamegraph we are asking three key questions: During the sampling period, which functions called each other? How much memory was allocated by each function observed? What are the functions that allocate more memory on the heap? These three questions are answered visually.","title":"Flamegraphs"},{"location":"documentation/heapprofiler/04-flamegraphs/#which-functions-called-each-other-the-stack","text":"Each block represents the invocations of one function, aggregated by the call stack that led to it. When one block sits on top of another, it was called by the block below it, which was called by the block below it, and so on down the stack.","title":"Which functions called each other (the stack)"},{"location":"documentation/heapprofiler/04-flamegraphs/#how-much-memory-was-allocated-by-each-function-observed","text":"The width of a block represents the amount of memory that the function allocated. In the box at top-right contain the allocation info of the selected function.","title":"How much memory was allocated by each function observed?"},{"location":"documentation/heapprofiler/04-flamegraphs/#what-are-the-functions-that-allocate-more-memory-on-the-heap","text":"The \"hottest\" function in the profile is selected by default. Here, we can see the function name and file location (or equivalent), so we can inspect the underlying code and decide if this function is something we can and should optimize. The example above shows an Node.js function as the most hottest, you can click in \"Next biggest\" until reach an application function. In the next chapter, we will see how to manage the UI to visualize only the meaningful data to the provided demo.","title":"What are the functions that allocate more memory on the heap?"},{"location":"documentation/heapprofiler/04-flamegraphs/#up-next","text":"Controls","title":"Up next"},{"location":"documentation/heapprofiler/05-controls/","text":"Controls \u00b6 The Clinic.js HeapProfiler UI controls have three main sections: Flamegraph controls : Interacting with the flamegraph itself Allocation Info : Along the top, containing info about the block currently highlighted Options Menu : More advanced controls expandable from the info panel Flamegraph controls \u00b6 Interacting with a flamegraph block \u00b6 Hovering the mouse over a block will temporarily show info about that block in the info panel at the top of the screen. Clicking will select the block, so that the info panel reverts to that block when you are no longer hovering over anything. If you find an interesting-looking block, it can be useful to click on it so you can hover around exploring its neighbours without losing it. Tooltip buttons \u00b6 After single-clicking on a block, or hovering over it for a moment, a tooltip appears, with one or more buttons: Expand . See below for more about expanding a particular block. Contract . If this is the block that you have already expanded, it will show \"Contract\" instead of \"Expand\", which will take you back to the main view. Expanding \u00b6 When a block is double-clicked, or its tooltip \"Expand\" button is used, it will expand to fill the full width of the flamegraph. Blocks below will also expand and fill the full width, while blocks above the clicked block will increase in ratio to the block they sit on. Expanding a block essentially creates a new flamegraph which represents a particular partition of the main flamegraph. The block that has been expanded is marked with a shadow underneath. Every block below this 'shadow' is probably wider (more memory allocated on the heap) than the block that has expanded to fill the screen. To get back to the main, non-expanded view, you can either click on the background, click \"Return to main view\" at the bottom of the screen, double-click on the expanded frame, or click on its \"Contract\" tooltip button. Allocation Info \u00b6 There are five main features in the Allocation Info panel: Stack bar : A thin bar showing the \"hottest\" blocks in order Selection controls : Flick to the next hottest, previous, etc Code info : Where the function behind the currently highlighted block comes from Search box : For finding functions by name or path Options Menu : More advanced features. This Options Menu has its own section below Stack Bar \u00b6 We previously explained, in the Flamegraphs page, how it is useful to consider how much memory a function allocated at the top of the stack, meaning the V8 allocated memory in the heap from that function; and how this is represented by the brightness or \"heat\" of the colour of the exposed part of a block. This bar shows you the heat of those exposed stack tops, of every block in the flamegraph, in order of heat i.e. in order of how long that block's function was blocking the event loop. You can run the cursor along this bar from left to right to see where these \"hot\" functions are on the main flamegraph, with the same interaction as above: hover to see info, click to select and show tooltip, double click to expand. The left-most (hottest) block is selected by default when a Clinic.js Heap profile is first opened. Selection Controls \u00b6 These buttons allow you to easily jump from the currently selected block, to the block that is one to the left or right of it in the hottness-ranking shown by the Stack Bar. A good place to start with a Clinic.js Heap flamegraph, is to cycle through using the \"Next hottest\" button, and for each block it selects, think why that function are allocating too much memory. For example, it might be a function wasting unnecessary resources, or it might be a function you know that is expensive, but when you look at what is below it in the flamegraph, you might discover that it is being called too many times (for example, it might be in a nested loop). Code info \u00b6 This gives you more complete information about the code behind the block that is currently highlighted. Function name (or equivalent) on the left. Anonymous functions are labelled (anonymous) . File path (or equivalent) in the middle, including line and column number (if applicable). Context. This tells you what category this block is (for example, dependency), and may include additional information if certain function labels are used. Search box \u00b6 If there is some particular file or function(s) you want to locate, you can type part of the function name, file path or equivalent here, and any matches will be highlighted, in the same colour used for text and outlines. This can be useful if you've done such a good job optimizing an operation, you can no longer find it on the flamegraph! If it is possible the function do not allocated too much memory in a given time, it might appear if you create a new profile with a longer duration and/or more connections in Autocannon . Options Menu \u00b6 Clicking \"Options\" on the right side of the Info Panel opens a menu with more advanced options. Visibility by code area \u00b6 These toggle buttons show (tick) or hide (untick) blocks based on where the code is in the application or Node.js framework. [Application name] : Code inside the main package being profiled. Visible by default. Dependencies : Code in a dependency in a node_modules directory. Visible by default. Node JS : Code inside Node.js core. Visible by default. V8 : Functions inside the V8 JavaScript engine. Hidden by default. Preferences \u00b6 Presentation mode : Increases text sizes and colour contrasts, which can be useful if Clinic.js Heap is being presented under suboptimal conditions (e.g. on a projector in a brightly lit room). Profiles can be set to show in Presentation Mode by default by setting the PRESENTATION_MODE environment variable to TRUE .","title":"Controls"},{"location":"documentation/heapprofiler/05-controls/#controls","text":"The Clinic.js HeapProfiler UI controls have three main sections: Flamegraph controls : Interacting with the flamegraph itself Allocation Info : Along the top, containing info about the block currently highlighted Options Menu : More advanced controls expandable from the info panel","title":"Controls"},{"location":"documentation/heapprofiler/05-controls/#flamegraph-controls","text":"","title":"Flamegraph controls"},{"location":"documentation/heapprofiler/05-controls/#interacting-with-a-flamegraph-block","text":"Hovering the mouse over a block will temporarily show info about that block in the info panel at the top of the screen. Clicking will select the block, so that the info panel reverts to that block when you are no longer hovering over anything. If you find an interesting-looking block, it can be useful to click on it so you can hover around exploring its neighbours without losing it.","title":"Interacting with a flamegraph block"},{"location":"documentation/heapprofiler/05-controls/#tooltip-buttons","text":"After single-clicking on a block, or hovering over it for a moment, a tooltip appears, with one or more buttons: Expand . See below for more about expanding a particular block. Contract . If this is the block that you have already expanded, it will show \"Contract\" instead of \"Expand\", which will take you back to the main view.","title":"Tooltip buttons"},{"location":"documentation/heapprofiler/05-controls/#expanding","text":"When a block is double-clicked, or its tooltip \"Expand\" button is used, it will expand to fill the full width of the flamegraph. Blocks below will also expand and fill the full width, while blocks above the clicked block will increase in ratio to the block they sit on. Expanding a block essentially creates a new flamegraph which represents a particular partition of the main flamegraph. The block that has been expanded is marked with a shadow underneath. Every block below this 'shadow' is probably wider (more memory allocated on the heap) than the block that has expanded to fill the screen. To get back to the main, non-expanded view, you can either click on the background, click \"Return to main view\" at the bottom of the screen, double-click on the expanded frame, or click on its \"Contract\" tooltip button.","title":"Expanding"},{"location":"documentation/heapprofiler/05-controls/#allocation-info","text":"There are five main features in the Allocation Info panel: Stack bar : A thin bar showing the \"hottest\" blocks in order Selection controls : Flick to the next hottest, previous, etc Code info : Where the function behind the currently highlighted block comes from Search box : For finding functions by name or path Options Menu : More advanced features. This Options Menu has its own section below","title":"Allocation Info"},{"location":"documentation/heapprofiler/05-controls/#stack-bar","text":"We previously explained, in the Flamegraphs page, how it is useful to consider how much memory a function allocated at the top of the stack, meaning the V8 allocated memory in the heap from that function; and how this is represented by the brightness or \"heat\" of the colour of the exposed part of a block. This bar shows you the heat of those exposed stack tops, of every block in the flamegraph, in order of heat i.e. in order of how long that block's function was blocking the event loop. You can run the cursor along this bar from left to right to see where these \"hot\" functions are on the main flamegraph, with the same interaction as above: hover to see info, click to select and show tooltip, double click to expand. The left-most (hottest) block is selected by default when a Clinic.js Heap profile is first opened.","title":"Stack Bar"},{"location":"documentation/heapprofiler/05-controls/#selection-controls","text":"These buttons allow you to easily jump from the currently selected block, to the block that is one to the left or right of it in the hottness-ranking shown by the Stack Bar. A good place to start with a Clinic.js Heap flamegraph, is to cycle through using the \"Next hottest\" button, and for each block it selects, think why that function are allocating too much memory. For example, it might be a function wasting unnecessary resources, or it might be a function you know that is expensive, but when you look at what is below it in the flamegraph, you might discover that it is being called too many times (for example, it might be in a nested loop).","title":"Selection Controls"},{"location":"documentation/heapprofiler/05-controls/#code-info","text":"This gives you more complete information about the code behind the block that is currently highlighted. Function name (or equivalent) on the left. Anonymous functions are labelled (anonymous) . File path (or equivalent) in the middle, including line and column number (if applicable). Context. This tells you what category this block is (for example, dependency), and may include additional information if certain function labels are used.","title":"Code info"},{"location":"documentation/heapprofiler/05-controls/#search-box","text":"If there is some particular file or function(s) you want to locate, you can type part of the function name, file path or equivalent here, and any matches will be highlighted, in the same colour used for text and outlines. This can be useful if you've done such a good job optimizing an operation, you can no longer find it on the flamegraph! If it is possible the function do not allocated too much memory in a given time, it might appear if you create a new profile with a longer duration and/or more connections in Autocannon .","title":"Search box"},{"location":"documentation/heapprofiler/05-controls/#options-menu","text":"Clicking \"Options\" on the right side of the Info Panel opens a menu with more advanced options.","title":"Options Menu"},{"location":"documentation/heapprofiler/05-controls/#visibility-by-code-area","text":"These toggle buttons show (tick) or hide (untick) blocks based on where the code is in the application or Node.js framework. [Application name] : Code inside the main package being profiled. Visible by default. Dependencies : Code in a dependency in a node_modules directory. Visible by default. Node JS : Code inside Node.js core. Visible by default. V8 : Functions inside the V8 JavaScript engine. Hidden by default.","title":"Visibility by code area"},{"location":"documentation/heapprofiler/05-controls/#preferences","text":"Presentation mode : Increases text sizes and colour contrasts, which can be useful if Clinic.js Heap is being presented under suboptimal conditions (e.g. on a projector in a brightly lit room). Profiles can be set to show in Presentation Mode by default by setting the PRESENTATION_MODE environment variable to TRUE .","title":"Preferences"},{"location":"flame/","text":"\u5f15\u7528\u5176\u53d1\u660e\u8005Brendan Gregg\u7684\u8bdd\uff0c \u706b\u7130\u56fe \u662f\u4e00\u79cd\u5206\u6790\u8f6f\u4ef6\u7684\u53ef\u89c6\u5316\uff0c\u5141\u8bb8\u5feb\u901f\u51c6\u786e\u5730\u8bc6\u522b\u6700\u5e38\u89c1\u7684\u4ee3\u7801\u8def\u5f84\u3002 Flame\u662f\u4e13\u95e8\u4e3aNode.js\u8bbe\u8ba1\u7684\uff0c\u88ab\u5185\u7f6e\u4e8eClinic.js\u4e2d\u3002 \u5b83\u6536\u96c6CPU\u91c7\u6837\u4f7f\u7528\u7684\u6307\u6807\uff0c\u7136\u540e\u8ddf\u8e2a\u5806\u6808\u9876\u90e8\u7684\u9891\u7387\u5e76\u521b\u5efa\u706b\u7130\u56fe\u3002 \u706b\u7130\u56fe\u662f\u968f\u65f6\u95f4\u91c7\u6837\u7684\u5806\u6808\u7684\u805a\u5408\u53ef\u89c6\u5316\u3002 \u5b83\u4e3b\u8981\u53ef\u89c6\u5316\u4e24\u4e2a\u6307\u6807\u3002 \u4e00\u4e2a\u51fd\u6570\u5728CPU\u4e0a\u7684\u65f6\u95f4\uff0c\u4ee5\u53ca\u4e00\u4e2a\u51fd\u6570\u5728\u6808\u9876\u7684\u65f6\u95f4\u3002 \u6700\u540e\u88ab\u8c03\u7528\u7684\u51fd\u6570\u88ab\u79f0\u4e3a\u4f4d\u4e8e\u5806\u6808\u9876\u90e8\u7684\u51fd\u6570\u3002 \u5982\u679c\u89c2\u5bdf\u5230\u4e00\u4e2a\u51fd\u6570\u5728\u5806\u6808\u9876\u90e8\u7684\u6b21\u6570\u6bd4\u5176\u4ed6\u51fd\u6570\u591a\uff0c\u5219\u8be5\u51fd\u6570\u53ef\u80fd\u963b\u585e\u4e86\u4e8b\u4ef6\u5faa\u73af\u3002 \u5982\u679c\u89c2\u5bdf\u5230\u4e00\u4e2a\u51fd\u6570\u4ee5\u8f83\u9ad8\u7684\u6bd4\u7387\u4f4d\u4e8e\u5806\u6808\u7684\u9876\u90e8\uff0c\u5219\u5c06\u5176\u79f0\u4e3a\"hot\"\u3002 \u6f14\u793a \u00b6 \u4ea4\u4e92\u4f8b\u5b50 \u8bbe\u7f6e \u00b6 1 2 npm install clinic -g clinic flame --help","title":"\u706b\u7130\u56fe"},{"location":"flame/#_1","text":"\u4ea4\u4e92\u4f8b\u5b50","title":"\u6f14\u793a"},{"location":"flame/#_2","text":"1 2 npm install clinic -g clinic flame --help","title":"\u8bbe\u7f6e"},{"location":"heapprofiler/","text":"\u706b\u7130\u56fe\u662f\u968f\u65f6\u95f4\u5206\u914d\u7684\u5185\u5b58\u7684\u805a\u5408\u53ef\u89c6\u5316\u3002 \u6bcf\u4e2a\u5757\u8868\u793a\u4e00\u4e2a\u51fd\u6570\u5206\u914d\u7684\u5185\u5b58\u91cf\u3002 \u5757\u8d8a\u5bbd\uff0c\u5206\u914d\u7684\u5185\u5b58\u5c31\u8d8a\u591a\u3002 \u6f14\u793a \u00b6 \u4e92\u52a8\u7684\u4f8b\u5b50 \u914d\u7f6e \u00b6 1 2 npm install -g clinic clinic heapprofiler --help","title":"\u5806\u5206\u6790\u5668"},{"location":"heapprofiler/#_1","text":"\u4e92\u52a8\u7684\u4f8b\u5b50","title":"\u6f14\u793a"},{"location":"heapprofiler/#_2","text":"1 2 npm install -g clinic clinic heapprofiler --help","title":"\u914d\u7f6e"},{"location":"reviews/review1/","tags":["fun","python"],"text":"#fun #python .md-typeset .blogging-tags-grid { display: flex; flex-direction: row; flex-wrap: wrap; gap: 8px; margin-top: 5px; } .md-typeset .blogging-tag { color: var(--md-typeset-color); background-color: var(--md-typeset-code-color); } .md-typeset .blogging-tag code { border-radius: 5px; } Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.","title":"Review 1"},{"location":"stubs/team/alan/","text":"","title":"Alan Slater"},{"location":"stubs/team/conor/","text":"","title":"Conor O'Neill"},{"location":"stubs/team/david/","text":"","title":"David Clements"},{"location":"stubs/team/james/","text":"","title":"James Snell"},{"location":"stubs/team/mathias/","text":"","title":"Mathias Buus"},{"location":"stubs/team/matteo/","text":"","title":"Matteo Collina"},{"location":"stubs/team/renee/","text":"","title":"Ren\u00e9e Kooi"},{"location":"stubs/testimonial/bubbleprof/","text":"","title":"Bubbleprof"},{"location":"stubs/testimonial/clinic/","text":"","title":"Clinic"},{"location":"terms/","text":"Clinic.js Terms & Conditions \u00b6","title":"Clinic.js Terms &amp; Conditions"},{"location":"terms/#clinicjs-terms-conditions","text":"","title":"Clinic.js Terms &amp; Conditions"}]}